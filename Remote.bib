@article{heterogprocessing,
  author    = {Amar Shan},
  title     = {{Heterogeneous Processing: a Strategy for Augmenting Moore's Law}},
  journal   = {Linux Journal},
  year      = {2006},
  url       = {https://www.linuxjournal.com/article/8368},
  timestamp = {2020.11.20}
}

@inproceedings{pmlr-v104-schmidt19a,
  title     = {Load-Balanced Parallel Constraint-Based Causal Structure Learning on Multi-Core Systems for High-Dimensional Data},
  author    = {Schmidt, Christopher and Huegle, Johannes and Bode, Philipp and Uflacker, Matthias},
  pages     = {59--77},
  year      = {2019},
  editor    = {Thuc Duy Le and Jiuyong Li and Kun Zhang and Emre Kıcıman Peng Cui and Aapo Hyvärinen},
  volume    = {104},
  series    = {Proceedings of Machine Learning Research},
  address   = {Anchorage, Alaska, USA},
  month     = {05 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v104/schmidt19a/schmidt19a.pdf},
  url       = {http://proceedings.mlr.press/v104/schmidt19a.html},
  abstract  = {In the context of high-dimensional data state-of-the-art methods for constraint-based causal structure learning, such as the PC algorithm, are limited in their application through their worst case exponential computational complexity. To address the resulting long execution time, several parallel extensions have been developed to exploit modern multi-core systems. These extensions apply a static distribution of tasks to the execution units to achieve paral- lelism, which introduces the problem of load imbalance. In our work, we propose a parallel implementation that follows a dynamic task distribution in order to avoid situations of load imbalance and improve the execution time. On the basis of an experimental evaluation on real-world high dimensional datasets, we show that our implementation has a better load balancing compared to an existing parallel implementation in the context of multivariate normal distributed data. For datasets that introduce load imbalance, our dynamic task distribution approach outperforms existing static approaches by factors up to 2.4. Overall, we increase the speed up from factors of up to 27, for the static approach, to factors of up to 39 for the dynamic approach, when scaling to 80 cores compared to a non-parallel execution.}
}

@article{DBLP:journals/corr/abs-1812-08491,
  author        = {Behrooz Zarebavani and
               Foad Jafarinejad and
               Matin Hashemi and
               Saber Salehkaleybar},
  title         = {cuPC: CUDA-based Parallel {PC} Algorithm for Causal Structure Learning
               on {GPU}},
  journal       = {CoRR},
  volume        = {abs/1812.08491},
  year          = {2018},
  url           = {http://arxiv.org/abs/1812.08491},
  archiveprefix = {arXiv},
  eprint        = {1812.08491},
  timestamp     = {Thu, 23 Jan 2020 09:18:04 +0100},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1812-08491.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/vldb/TomeGRRB18,
  author    = {Diego G. Tom{\'{e}} and
               Tim Gubner and
               Mark Raasveldt and
               Eyal Rozenberg and
               Peter A. Boncz},
  editor    = {Rajesh Bordawekar and
               Tirthankar Lahiri},
  title     = {Optimizing Group-By and Aggregation using {GPU-CPU} Co-Processing},
  booktitle = {International Workshop on Accelerating Analytics and Data Management
               Systems Using Modern Processor and Storage Architectures, ADMS@VLDB
               2018, Rio de Janeiro, Brazil, August 27, 2018},
  pages     = {1--10},
  year      = {2018},
  url       = {http://www.adms-conf.org/2018-camera-ready/tome\_groupby.pdf},
  timestamp = {Thu, 12 Mar 2020 11:33:37 +0100},
  biburl    = {https://dblp.org/rec/conf/vldb/TomeGRRB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{JSSv077i02,
  author   = {Marco Scutari},
  title    = {Bayesian Network Constraint-Based Structure Learning Algorithms: Parallel and Optimized Implementations in the bnlearn R Package},
  journal  = {Journal of Statistical Software, Articles},
  volume   = {77},
  number   = {2},
  year     = {2017},
  keywords = {Bayesian networks; structure learning; parallel programming; R},
  abstract = {It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: Its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios. Efficient implementations of score-based structure learning benefit from past and current research in optimization theory, which can be adapted to the task by using the network score as the objective function to maximize. This is not true for approaches based on conditional independence tests, called constraint-based learning algorithms. The only optimization in widespread use, backtracking, leverages the symmetries implied by the definitions of neighborhood and Markov blanket. In this paper we illustrate how backtracking is implemented in recent versions of the bnlearn R package, and how it degrades the stability of Bayesian network structure learning for little gain in terms of speed. As an alternative, we describe a software architecture and framework that can be used to parallelize constraint-based structure learning algorithms (also implemented in bnlearn) and we demonstrate its performance using four reference networks and two real-world data sets from genetics and systems biology. We show that on modern multi-core or multiprocessor hardware parallel implementations are preferable over backtracking, which was developed when single-processor machines were the norm.},
  issn     = {1548-7660},
  pages    = {1--20},
  doi      = {10.18637/jss.v077.i02},
  url      = {https://www.jstatsoft.org/v077/i02}
}

@inproceedings{10.1007/978-3-030-38991-8_7,
  author    = {Schmidt, Christopher
and Huegle, Johannes
and Horschig, Siegfried
and Uflacker, Matthias},
  editor    = {Wen, Sheng
and Zomaya, Albert
and Yang, Laurence T.},
  title     = {Out-of-Core GPU-Accelerated Causal Structure Learning},
  booktitle = {Algorithms and Architectures for Parallel Processing},
  year      = {2020},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {89--104},
  abstract  = {Learning the causal structures in high-dimensional datasets enables deriving advanced insights from observational data. For example, the construction of gene regulatory networks inferred from gene expression data supports solving biological and biomedical problems, such as, in drug design or diagnostics. With the adoption of Graphics Processing Units (GPUs) the runtime of constraint-based causal structure learning algorithms on multivariate normal distributed data is significantly reduced. For extremely high-dimensional datasets, e.g., provided by The Cancer Genome Atlas (TCGA), state-of-the-art GPU-accelerated algorithms hit the device memory limit of single GPUs and consequently, execution fails. In order to overcome this limitation, we propose an out-of-core algorithm for GPU-accelerated constraint-based causal structure learning on multivariate normal distributed data. We experimentally validate the scalability of our algorithm, beyond GPU device memory capacities and compare our implementation to a baseline using Unified Memory (UM). In recent GPU generations, UM overcomes the device memory limit, by utilizing the GPU page migration engine. On a real-world gene expression dataset from the TCGA, our approach outperforms the baseline by a factor of 95 and is faster than a parallel Central Processing Unit (CPU)-based version by a factor of 236.},
  isbn      = {978-3-030-38991-8}
}

@article{6626595,
  author  = {S. {Momcilovic} and A. {Ilic} and N. {Roma} and L. {Sousa}},
  journal = {IEEE Transactions on Multimedia},
  title   = {Dynamic Load Balancing for Real-Time Video Encoding on Heterogeneous CPU+GPU Systems},
  year    = {2014},
  volume  = {16},
  number  = {1},
  pages   = {108-121},
  doi     = {10.1109/TMM.2013.2284892}
}

@article{10.5555/1248659.1248681,
  author     = {Kalisch, Markus and B\"{u}hlmann, Peter},
  title      = {Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm},
  year       = {2007},
  issue_date = {5/1/2007},
  publisher  = {JMLR.org},
  volume     = {8},
  issn       = {1532-4435},
  abstract   = {We consider the PC-algorithm (Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efficiency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(na) for any 0 < a < ∞. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data.},
  journal    = {J. Mach. Learn. Res.},
  month      = may,
  pages      = {613–636},
  numpages   = {24}
}

@inproceedings{constraintgpu,
  author = {Schmidt, Christopher and Huegle, Johannes and Uflacker, Matthias},
  year   = {2018},
  month  = {07},
  pages  = {1-10},
  title  = {Order-independent constraint-based causal structure learning for gaussian distribution models using GPUs},
  doi    = {10.1145/3221269.3221292}
}

@article{deeplearning,
  author  = {LeCun, Yann and Bengio, Y. and Hinton, Geoffrey},
  year    = {2015},
  month   = {05},
  pages   = {436-44},
  title   = {Deep Learning},
  volume  = {521},
  journal = {Nature},
  doi     = {10.1038/nature14539}
}

@book{10.5555/527029,
  author    = {Foster, Ian},
  title     = {Designing and Building Parallel Programs: Concepts and Tools for Parallel Software Engineering},
  year      = {1995},
  isbn      = {0201575949},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address   = {USA},
  abstract  = {From the Publisher:At last, a practitioner's guide to parallel programming! Students and professionals who use parallel or distributed computer systems will be able to solve real problems with Designing and Building Parallel Programs. This book provides a comprehensive introduction to parallel algorithm design, performance analysis, and program construction. It describes the tools needed to write parallel programs and provides numerous examples. A unique feature is the companion on-line version, accessible via the World Wide Web using browsers such as Mosaic. This provides a convenient hypertext version of the text with pointers to programming tools, example programs, and other resources on parallel and distributed computing.}
}

@article{fastpc,
  author  = {le, Thuc and Hoang, Tao and Li, Jiuyong and Liu, Lin and Liu, Huawen},
  year    = {2015},
  month   = {02},
  pages   = {},
  title   = {A Fast PC Algorithm for High Dimensional Causal Discovery with Multi-Core PCs},
  volume  = {16},
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  doi     = {10.1109/TCBB.2016.2591526}
}

@inproceedings{dedicatedlb,
  author = {Galindo, Ismael and Almeida, Francisco and Badía-Contelles, José},
  year   = {2008},
  month  = {09},
  pages  = {64-74},
  title  = {Dynamic Load Balancing on Dedicated Heterogeneous Systems},
  isbn   = {978-3-540-87474-4},
  doi    = {10.1007/978-3-540-87475-1_50}
}

@inproceedings{dynamiclb,
  author  = {Binotto, Alecio and Pereira, Carlos and Fellner, Dieter},
  year    = {2010},
  month   = {05},
  pages   = {1 - 4},
  title   = {Towards dynamic reconfigurable load-balancing for hybrid desktop platforms},
  journal = {Fraunhofer IGD},
  doi     = {10.1109/IPDPSW.2010.5470804}
}