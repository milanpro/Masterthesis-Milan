
@article{le_fast_2019,
	title = {A {Fast} {PC} {Algorithm} for {High} {Dimensional} {Causal} {Discovery} with {Multi}-{Core} {PCs}},
	volume = {16},
	issn = {1545-5963, 1557-9964, 2374-0043},
	url = {http://arxiv.org/abs/1502.02454},
	doi = {10.1109/TCBB.2016.2591526},
	abstract = {Discovering causal relationships from observational data is a crucial problem and it has applications in many research areas. The PC algorithm is the state-of-the-art constraint based method for causal discovery. However, runtime of the PC algorithm, in the worst-case, is exponential to the number of nodes (variables), and thus it is inefﬁcient when being applied to high dimensional data, e.g. gene expression datasets. On another note, the advancement of computer hardware in the last decade has resulted in the widespread availability of multi-core personal computers. There is a signiﬁcant motivation for designing a parallelised PC algorithm that is suitable for personal computers and does not require end users’ parallel computing knowledge beyond their competency in using the PC algorithm. In this paper, we develop parallel-PC, a fast and memory efﬁcient PC algorithm using the parallel computing technique.},
	language = {en},
	number = {5},
	urldate = {2021-04-22},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Le, Thuc Duy and Hoang, Tao and Li, Jiuyong and Liu, Lin and Liu, Huawen},
	month = sep,
	year = {2019},
	note = {arXiv: 1502.02454},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {1483--1495},
	file = {Le et al. - 2019 - A fast PC algorithm for high dimensional causal di.pdf:/Users/i500763/Zotero/storage/VRWHYNTW/Le et al. - 2019 - A fast PC algorithm for high dimensional causal di.pdf:application/pdf},
}

@inproceedings{tome_optimizing_2018,
	title = {Optimizing {Group}-{By} {And} {Aggregation} using {GPU}-{CPU} {Co}-{Processing}},
	url = {http://www.adms-conf.org/2018-camera-ready/tome_groupby.pdf},
	abstract = {While GPU query processing is a well-studied area, real adoption is limited in practice as typically GPU execution is only signiﬁcantly faster than CPU execution if the data resides in GPU memory, which limits scalability to small data scenarios where performance tends to be less critical. Another problem is that not all query code (e.g. UDFs) will realistically be able to run on GPUs. We therefore investigate CPU-GPU co-processing, where both the CPU and GPU are involved in evaluating the query in scenarios where the data does not ﬁt in the GPU memory.},
	language = {en},
	booktitle = {International {Workshop} on {Accelerating} {Analytics} and {Data} {Management} {Systems} {Using} {Modern} {Processor} and {Storage} {Architectures}, {ADMS}@{VLDB} 2018, {Rio} de {Janeiro}, {Brazil}, {August} 27, 2018},
	author = {Tome, Diego and Gubner, Tim and Raasveldt, Mark},
	year = {2018},
	pages = {1--10},
	file = {Tome et al. - Optimizing Group-By And Aggregation using GPU-CPU .pdf:/Users/i500763/Zotero/storage/MAPIXF23/Tome et al. - Optimizing Group-By And Aggregation using GPU-CPU .pdf:application/pdf},
}

@inproceedings{binotto_towards_2010,
	address = {Atlanta, GA},
	title = {Towards dynamic reconfigurable load-balancing for hybrid desktop platforms},
	isbn = {978-1-4244-6533-0 978-1-4244-6534-7},
	url = {http://ieeexplore.ieee.org/document/5470804/},
	doi = {10.1109/IPDPSW.2010.5470804},
	abstract = {High-performance platforms are required by applications that use massive calculations. Actually, desktop accelerators (like the GPUs) form a powerful heterogeneous platform in conjunction with multi-core CPUs. To improve application performance on these hybrid platforms, loadbalancing plays an important role to distribute workload. However, such scheduling problem faces challenges since the cost of a task at a Processing Unit (PU) is non-deterministic and depends on parameters that cannot be known a priori, like input data, online creation of tasks, scenario changing, etc. Therefore, self-adaptive computing is a potential paradigm as it can provide ﬂexibility to explore computational resources and improve performance on different execution scenarios.},
	language = {en},
	urldate = {2021-04-22},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Parallel} \& {Distributed} {Processing}, {Workshops} and {Phd} {Forum} ({IPDPSW})},
	publisher = {IEEE},
	author = {Binotto, Alecio P. D. and Pereira, Carlos E. and Fellner, Dieter W.},
	month = apr,
	year = {2010},
	pages = {1--4},
	file = {Binotto et al. - 2010 - Towards dynamic reconfigurable load-balancing for .pdf:/Users/i500763/Zotero/storage/TX853LYE/Binotto et al. - 2010 - Towards dynamic reconfigurable load-balancing for .pdf:application/pdf},
}

@article{scutari_bayesian_2017,
	title = {Bayesian {Network} {Constraint}-{Based} {Structure} {Learning} {Algorithms}: {Parallel} and {Optimized} {Implementations} in the bnlearn {R} {Package}},
	volume = {77},
	issn = {1548-7660},
	shorttitle = {Bayesian {Network} {Constraint}-{Based} {Structure} {Learning} {Algorithms}},
	url = {https://www.jstatsoft.org/v077/i02},
	doi = {10.18637/jss.v077.i02},
	abstract = {It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: Its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios. Efficient implementations of score-based structure learning benefit from past and current research in optimization theory, which can be adapted to the task by using the network score as the objective function to maximize. This is not true for approaches based on conditional independence tests, called constraint-based learning algorithms. The only optimization in widespread use, backtracking, leverages the symmetries implied by the definitions of neighborhood and Markov blanket. In this paper we illustrate how backtracking is implemented in recent versions of the bnlearn R package, and how it degrades the stability of Bayesian network structure learning for little gain in terms of speed. As an alternative, we describe a software architecture and framework that can be used to parallelize constraint-based structure learning algorithms (also implemented in bnlearn) and we demonstrate its performance using four reference networks and two real-world data sets from genetics and systems biology. We show that on modern multi-core or multiprocessor hardware parallel implementations are preferable over backtracking, which was developed when single-processor machines were the norm.},
	language = {en},
	number = {2},
	urldate = {2021-04-22},
	journal = {Journal of Statistical Software, Articles},
	author = {Scutari, Marco},
	year = {2017},
	keywords = {Bayesian networks, parallel programming, R, structure learning},
	pages = {1--20},
	file = {Scutari - 2017 - Bayesian Network Constraint-Based Structure Learni.pdf:/Users/i500763/Zotero/storage/QXLG6CR4/Scutari - 2017 - Bayesian Network Constraint-Based Structure Learni.pdf:application/pdf},
}

@article{schmidt_order-independent_2018,
	title = {Order-{Independent} {Constraint}-{Based} {Causal} {Structure} {Learning} for {Gaussian} {Distribution} {Models} using {GPUs}},
	abstract = {Learning the causal structures in high-dimensional datasets allows deriving advanced insights from observational data, thus creating the potential for new applications. One crucial limitation of stateof-the-art methods for learning causal relationships, such as the PC algorithm, is their long execution time. While, in the worst case, the execution time is exponential to the dimension of a given dataset, it is polynomial if the underlying causal structures are sparse. To address the long execution time, parallelized extensions of the algorithm have been developed addressing the Central Processing Unit (CPU) as the primary execution device. While modern multicore CPUs expose a decent level of parallelism, coprocessors, such as Graphics Processing Units (GPUs), are specifically designed to process thousands of data points in parallel, providing superior parallel processing capabilities compared to CPUs. In our work, we leverage the parallel processing power of GPUs to address the drawback of the long execution time of the PC algorithm and develop an efficient GPU-accelerated implementation for Gaussian distribution models. Based on an experimental evaluation of various high-dimensional real-world gene expression datasets, we show that our GPU-accelerated implementation outperforms existing CPU-based versions, by factors up to 700.},
	language = {en},
	author = {Schmidt, Christopher and Huegle, Johannes},
	year = {2018},
	pages = {10},
	file = {Schmidt and Huegle - 2018 - Order-Independent Constraint-Based Causal Structur.pdf:/Users/i500763/Zotero/storage/ANBH9IMQ/Schmidt and Huegle - 2018 - Order-Independent Constraint-Based Causal Structur.pdf:application/pdf},
}

@article{zarebavani_cupc_2018,
	title = {{cuPC}: {CUDA}-based {Parallel} {PC} {Algorithm} for {Causal} {Structure} {Learning} on {GPU}},
	volume = {abs/1812.08491},
	url = {http://arxiv.org/abs/1812.08491},
	abstract = {The main goal in many ﬁelds in the empirical sciences is to discover causal relationships among a set of variables from observational data. PC algorithm is one of the promising solutions to learn underlying causal structure by performing a number of conditional independence tests. In this paper, we propose a novel GPU-based parallel algorithm, called cuPC, to execute an order-independent version of PC. The proposed solution has two variants, cuPC-E and cuPC-S, which parallelize PC in two different ways for multivariate normal distribution. Experimental results show the scalability of the proposed algorithms with respect to the number of variables, the number of samples, and different graph densities. For instance, in one of the most challenging datasets, the runtime is reduced from more than 11 hours to about 4 seconds. On average, cuPC-E and cuPC-S achieve 500 X and 1300 X speedup, respectively, compared to serial implementation on CPU. The source code of cuPC is available online [1].},
	language = {en},
	journal = {IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS},
	author = {Zarebavani, Behrooz and Jafarinejad, Foad and Hashemi, Matin and Salehkaleybar, Saber},
	year = {2018},
	note = {\_eprint: 1812.08491},
	pages = {14},
	file = {Zarebavani et al. - cuPC CUDA-based Parallel PC Algorithm for Causal .pdf:/Users/i500763/Zotero/storage/X56FYKSI/Zarebavani et al. - cuPC CUDA-based Parallel PC Algorithm for Causal .pdf:application/pdf},
}

@incollection{lastovetsky_dynamic_2008,
	address = {Berlin, Heidelberg},
	title = {Dynamic {Load} {Balancing} on {Dedicated} {Heterogeneous} {Systems}},
	volume = {5205},
	isbn = {978-3-540-87474-4 978-3-540-87475-1},
	url = {http://link.springer.com/10.1007/978-3-540-87475-1_14},
	abstract = {Parallel computing in heterogeneous environments is drawing considerable attention due to the growing number of these kind of systems. Adapting existing code and libraries to such systems is a fundamental problem. The performance of this code is aﬀected by the large interdependence between the code and these parallel architectures. We have developed a dynamic load balancing library that allows parallel code to be adapted to heterogeneous systems for a wide variety of problems. The overhead introduced by our system is minimal and the cost to the programmer negligible. The strategy was validated on several problems to conﬁrm the soundness of our proposal.},
	language = {en},
	urldate = {2021-04-22},
	booktitle = {Recent {Advances} in {Parallel} {Virtual} {Machine} and {Message} {Passing} {Interface}},
	publisher = {Springer Berlin Heidelberg},
	author = {Galindo, Ismael and Almeida, Francisco and Badía-Contelles, José Manuel},
	editor = {Lastovetsky, Alexey and Kechadi, Tahar and Dongarra, Jack},
	year = {2008},
	doi = {10.1007/978-3-540-87475-1_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {64--74},
	file = {Galindo et al. - 2008 - Dynamic Load Balancing on Dedicated Heterogeneous .pdf:/Users/i500763/Zotero/storage/5RSY2ZB6/Galindo et al. - 2008 - Dynamic Load Balancing on Dedicated Heterogeneous .pdf:application/pdf},
}

@article{momcilovic_dynamic_2014,
	title = {Dynamic {Load} {Balancing} for {Real}-{Time} {Video} {Encoding} on {Heterogeneous} {CPU}+{GPU} {Systems}},
	volume = {16},
	doi = {10.1109/TMM.2013.2284892},
	abstract = {The high computational demands and overall encoding complexity make the processing of high deﬁnition video sequences hard to be achieved in real-time. In this manuscript, we target an efﬁcient parallelization and RD performance analysis of H.264/AVC inter-loop modules and their collaborative execution in hybrid multi-core CPU and multi-GPU systems. The proposed dynamic load balancing algorithm allows efﬁcient and concurrent video encoding across several heterogeneous devices by relying on realistic run-time performance modeling and module-device execution afﬁnities when distributing the computations. Due to an online adjustment of load balancing decisions, this approach is also self-adaptable to different execution scenarios. Experimental results show the proposed algorithm’s ability to achieve real-time encoding for different resolutions of high-deﬁnition sequences in various heterogeneous platforms. Speed-up values of up to 2.6 were obtained when compared to the video inter-loop encoding on a single GPU device, and up to 8.5 when compared to a highly optimized multi-core CPU execution. Moreover, the proposed algorithm also provides an automatic tuning of the encoding parameters, in order to meet strict encoding constraints.},
	language = {en},
	number = {1},
	journal = {IEEE Transactions on Multimedia},
	author = {Momcilovic, Svetislav and Ilic, Aleksandar and Sousa, Leonel},
	year = {2014},
	pages = {108--121},
	file = {Momcilovic et al. - 2014 - Dynamic Load Balancing for Real-Time Video Encodin.pdf:/Users/i500763/Zotero/storage/9FYWWQ8D/Momcilovic et al. - 2014 - Dynamic Load Balancing for Real-Time Video Encodin.pdf:application/pdf},
}

@article{kalisch_estimating_2007,
	title = {Estimating {High}-{Dimensional} {Directed} {Acyclic} {Graphs} with the {PC}-{Algorithm}},
	volume = {8},
	issn = {1532-4435},
	abstract = {We consider the PC-algorithm (Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efﬁciency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(na) for any 0 {\textless} a {\textless} ∞. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data.},
	language = {en},
	journal = {J. Mach. Learn. Res.},
	author = {Kalisch, Markus},
	month = may,
	year = {2007},
	note = {Publisher: JMLR.org},
	pages = {613--636},
	file = {Kalisch - Estimating High-Dimensional Directed Acyclic Graph.pdf:/Users/i500763/Zotero/storage/9Y2DWMDP/Kalisch - Estimating High-Dimensional Directed Acyclic Graph.pdf:application/pdf},
}

@article{liu_estimation_2016,
	title = {Estimation {Accuracy} on {Execution} {Time} of {Run}-{Time} {Tasks} in a {Heterogeneous} {Distributed} {Environment}},
	volume = {16},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/16/9/1386},
	doi = {10.3390/s16091386},
	abstract = {Distributed Computing has achieved tremendous development since cloud computing was proposed in 2006, and played a vital role promoting rapid growth of data collecting and analysis models, e.g., Internet of things, Cyber-Physical Systems, Big Data Analytics, etc. Hadoop has become a data convergence platform for sensor networks. As one of the core components, MapReduce facilitates allocating, processing and mining of collected large-scale data, where speculative execution strategies help solve straggler problems. However, there is still no efﬁcient solution for accurate estimation on execution time of run-time tasks, which can affect task allocation and distribution in MapReduce. In this paper, task execution data have been collected and employed for the estimation. A two-phase regression (TPR) method is proposed to predict the ﬁnishing time of each task accurately. Detailed data of each task have drawn interests with detailed analysis report being made. According to the results, the prediction accuracy of concurrent tasks’ execution time can be improved, in particular for some regular jobs.},
	language = {en},
	number = {9},
	urldate = {2021-04-22},
	journal = {Sensors},
	author = {Liu, Qi and Cai, Weidong and Jin, Dandan and Shen, Jian and Fu, Zhangjie and Liu, Xiaodong and Linge, Nigel},
	month = aug,
	year = {2016},
	pages = {1386},
	file = {Liu et al. - 2016 - Estimation Accuracy on Execution Time of Run-Time .pdf:/Users/i500763/Zotero/storage/76249HKV/Liu et al. - 2016 - Estimation Accuracy on Execution Time of Run-Time .pdf:application/pdf},
}

@inproceedings{schmidt_load-balanced_2019,
	address = {Anchorage, Alaska, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Load-{Balanced} {Parallel} {Constraint}-{Based} {Causal} {Structure} {Learning} on {Multi}-{Core} {Systems} for {High}-{Dimensional} {Data}},
	volume = {104},
	url = {http://proceedings.mlr.press/v104/schmidt19a.html},
	abstract = {In the context of high-dimensional data state-of-the-art methods for constraint-based causal structure learning, such as the PC algorithm, are limited in their application through their worst case exponential computational complexity. To address the resulting long execution time, several parallel extensions have been developed to exploit modern multi-core systems. These extensions apply a static distribution of tasks to the execution units to achieve parallelism, which introduces the problem of load imbalance. In our work, we propose a parallel implementation that follows a dynamic task distribution in order to avoid situations of load imbalance and improve the execution time. On the basis of an experimental evaluation on real-world high dimensional datasets, we show that our implementation has a better load balancing compared to an existing parallel implementation in the context of multivariate normal distributed data. For datasets that introduce load imbalance, our dynamic task distribution approach outperforms existing static approaches by factors up to 2.4. Overall, we increase the speed up from factors of up to 27, for the static approach, to factors of up to 39 for the dynamic approach, when scaling to 80 cores compared to a non-parallel execution.},
	language = {en},
	publisher = {PMLR},
	author = {Schmidt, Christopher and Huegle, Johannes and Bode, Philipp and Uﬂacker, Matthias},
	month = aug,
	year = {2019},
	pages = {59--77},
	file = {Schmidt et al. - Load-Balanced Parallel Constraint-Based Causal Str.pdf:/Users/i500763/Zotero/storage/TVC7UIAW/Schmidt et al. - Load-Balanced Parallel Constraint-Based Causal Str.pdf:application/pdf},
}

@article{youness_load_nodate,
	title = {Load {Balancing} on {CPU}-{GPU} {Heterogeneous} {System}},
	abstract = {In this paper, a High Performance Computing (HPC) technique with load balancing on Personal Computers (PC) with CPU-GPU Heterogeneous System is presented. The technique is based on a Householder algorithm that used mainly for matrix reduction techniques. The purpose for implementing this algorithm is to perform a benchmark to compare the execution performance among three different implementations CPU, GPU and CPU-GPU with load balancing. Our goal is to achieve the best performance by load balancing between multi-core CPU and many-core massively threaded GPU using a Scale Optimization Technique (SOT) to build a HPC system.},
	language = {en},
	author = {Youness, Hassan and Osama, Muhammed and Tarek, Aiman},
	pages = {7},
	file = {Youness et al. - Load Balancing on CPU-GPU Heterogeneous System.pdf:/Users/i500763/Zotero/storage/F423E5FA/Youness et al. - Load Balancing on CPU-GPU Heterogeneous System.pdf:application/pdf},
}

@article{mittal_survey_2015,
	title = {A {Survey} of {CPU}-{GPU} {Heterogeneous} {Computing} {Techniques}},
	volume = {47},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/2788396},
	doi = {10.1145/2788396},
	abstract = {As both CPUs and GPUs become employed in a wide range of applications, it has been acknowledged that both of these Processing Units (PUs) have their unique features and strengths and hence, CPU-GPU collaboration is inevitable to achieve high-performance computing. This has motivated a significant amount of research on heterogeneous computing techniques, along with the design of CPU-GPU fused chips and petascale heterogeneous supercomputers. In this article, we survey Heterogeneous Computing Techniques (HCTs) such as workload partitioning that enable utilizing both CPUs and GPUs to improve performance and/or energy efficiency. We review heterogeneous computing approaches at runtime, algorithm, programming, compiler, and application levels. Further, we review both discrete and fused CPU-GPU systems and discuss benchmark suites designed for evaluating Heterogeneous Computing Systems (HCSs). We believe that this article will provide insights into the workings and scope of applications of HCTs to researchers and motivate them to further harness the computational powers of CPUs and GPUs to achieve the goal of exascale performance.},
	language = {en},
	number = {4},
	urldate = {2021-04-22},
	journal = {ACM Computing Surveys},
	author = {Mittal, Sparsh and Vetter, Jeffrey S.},
	month = jul,
	year = {2015},
	pages = {1--35},
	file = {Mittal and Vetter - 2015 - A Survey of CPU-GPU Heterogeneous Computing Techni.pdf:/Users/i500763/Zotero/storage/UEXNAGC8/Mittal and Vetter - 2015 - A Survey of CPU-GPU Heterogeneous Computing Techni.pdf:application/pdf},
}

@article{lecun_deep_2015,
	title = {Deep {Learning}},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2021-04-22},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/Users/i500763/Zotero/storage/WKHHW7M6/LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@incollection{wen_out--core_2020,
	address = {Cham},
	title = {Out-of-{Core} {GPU}-{Accelerated} {Causal} {Structure} {Learning}},
	volume = {11944},
	isbn = {978-3-030-38990-1 978-3-030-38991-8},
	url = {http://link.springer.com/10.1007/978-3-030-38991-8_7},
	abstract = {Learning the causal structures in high-dimensional datasets enables deriving advanced insights from observational data. For example, the construction of gene regulatory networks inferred from gene expression data supports solving biological and biomedical problems, such as, in drug design or diagnostics. With the adoption of Graphics Processing Units (GPUs) the runtime of constraint-based causal structure learning algorithms on multivariate normal distributed data is signiﬁcantly reduced. For extremely high-dimensional datasets, e.g., provided by The Cancer Genome Atlas (TCGA), state-of-the-art GPU-accelerated algorithms hit the device memory limit of single GPUs and consequently, execution fails. In order to overcome this limitation, we propose an outof-core algorithm for GPU-accelerated constraint-based causal structure learning on multivariate normal distributed data. We experimentally validate the scalability of our algorithm, beyond GPU device memory capacities and compare our implementation to a baseline using Uniﬁed Memory (UM). In recent GPU generations, UM overcomes the device memory limit, by utilizing the GPU page migration engine. On a real-world gene expression dataset from the TCGA, our approach outperforms the baseline by a factor of 95 and is faster than a parallel Central Processing Unit (CPU)-based version by a factor of 236.},
	language = {en},
	urldate = {2021-04-22},
	booktitle = {Algorithms and {Architectures} for {Parallel} {Processing}},
	publisher = {Springer International Publishing},
	author = {Schmidt, Christopher and Huegle, Johannes and Horschig, Siegfried and Uflacker, Matthias},
	editor = {Wen, Sheng and Zomaya, Albert and Yang, Laurence T.},
	year = {2020},
	doi = {10.1007/978-3-030-38991-8_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {89--104},
	file = {Schmidt et al. - 2020 - Out-of-Core GPU-Accelerated Causal Structure Learn.pdf:/Users/i500763/Zotero/storage/2S9RV98K/Schmidt et al. - 2020 - Out-of-Core GPU-Accelerated Causal Structure Learn.pdf:application/pdf},
}

@article{le_parallelpc_2015,
	title = {{ParallelPC}: an {R} package for efficient constraint based causal exploration},
	shorttitle = {{ParallelPC}},
	url = {http://arxiv.org/abs/1510.03042},
	abstract = {Discovering causal relationships from data is the ultimate goal of many research areas. Constraint based causal exploration algorithms, such as PC, FCI, RFCI, PC-simple, IDA and Joint-IDA have achieved signiﬁcant progress and have many applications. A common problem with these methods is the high computational complexity, which hinders their applications in real world high dimensional datasets, e.g gene expression datasets. In this paper, we present an R package, ParallelPC, that includes the parallelised versions of these causal exploration algorithms. The parallelised algorithms help speed up the procedure of experimenting big datasets and reduce the memory used when running the algorithms. The package is not only suitable for super-computers or clusters, but also convenient for researchers using personal computers with multi core CPUs. Our experiment results on real world datasets show that using the parallelised algorithms it is now practical to explore causal relationships in high dimensional datasets with thousands of variables in a single multicore computer. ParallelPC is available in CRAN repository at https://cran.rproject.org/web/packages/ParallelPC/index.html.},
	language = {en},
	urldate = {2021-04-22},
	journal = {arXiv:1510.03042 [cs, stat]},
	author = {Le, Thuc Duy and Hoang, Tao and Li, Jiuyong and Liu, Lin and Hu, Shu},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.03042},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Le et al. - 2015 - ParallelPC an R package for efficient constraint .pdf:/Users/i500763/Zotero/storage/A6LC8BQL/Le et al. - 2015 - ParallelPC an R package for efficient constraint .pdf:application/pdf},
}

@article{feichtinger_flexible_2011,
	title = {A flexible {Patch}-based lattice {Boltzmann} parallelization approach for heterogeneous {GPU}–{CPU} clusters},
	volume = {37},
	issn = {01678191},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167819111000342},
	doi = {10.1016/j.parco.2011.03.005},
	abstract = {Sustaining a large fraction of single GPU performance in parallel computations is considered to be the major problem of GPU-based clusters. We address this issue in the context of a lattice Boltzmann ﬂow solver that is integrated in the WaLBerla software framework. Our multi-GPU implementation uses a block-structured MPI parallelization and is suitable for load balancing and heterogeneous computations on CPUs and GPUs. The overhead required for multi-GPU simulations is discussed in detail. It is demonstrated that a large fraction of the kernel performance can be sustained for weak scaling on InﬁniBand clusters, leading to excellent parallel efﬁciency. However, in strong scaling scenarios using multiple GPUs is much less efﬁcient than running CPU-only simulations on IBM BG/P and x86-based clusters. Hence, a cost analysis must determine the best course of action for a particular simulation task and hardware conﬁguration. Finally we present weak scaling results of heterogeneous simulations conducted on CPUs and GPUs simultaneously, using clusters equipped with varying node conﬁgurations.},
	language = {en},
	number = {9},
	urldate = {2021-04-22},
	journal = {Parallel Computing},
	author = {Feichtinger, Christian and Habich, Johannes and Köstler, Harald and Hager, Georg and Rüde, Ulrich and Wellein, Gerhard},
	month = sep,
	year = {2011},
	pages = {536--549},
	file = {Feichtinger et al. - 2011 - A flexible Patch-based lattice Boltzmann paralleli.pdf:/Users/i500763/Zotero/storage/XVJTZ3XJ/Feichtinger et al. - 2011 - A flexible Patch-based lattice Boltzmann paralleli.pdf:application/pdf},
}

@article{sosutha_heterogeneous_2015,
	title = {Heterogeneous {Parallel} {Computing} {Using} {Cuda} for {Chemical} {Process}},
	abstract = {CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. Using CUDA, the GPUs can be used for general purpose processing which involves parallel computation. CUDA has been used to accelerate non-graphical applications in computational biology, cryptography and other fields by an order of magnitude or more. Chemical processes need validation of their experimental data. It was found that Chemical process could become one such application where CUDA can be efficiently used. These validations of Chemical processes normally involve calculation of many coefficients. The chemical process that has been chosen for parallelizing is Heat Transfer process. This process involves calculation of coefficients for multiple iterations. As each of these iterations is independent of one another, CUDA was used to parallelize the calculation process. The execution time analysis shows that though CPU outperforms GPU when the numbers of iterations are less, when the number of iterations increase the GPU outperforms CPU greatly.},
	language = {en},
	journal = {Procedia Computer Science},
	author = {Sosutha, S},
	year = {2015},
	pages = {10},
	file = {Sosutha - 2015 - Heterogeneous Parallel Computing Using Cuda for Ch.pdf:/Users/i500763/Zotero/storage/QRV46AFU/Sosutha - 2015 - Heterogeneous Parallel Computing Using Cuda for Ch.pdf:application/pdf},
}

@article{zarges_evaluation_nodate,
	title = {Evaluation of {On}-{Node} {GPU} {Interconnects} for {Training} {Deep} {Neural} {Networks}},
	language = {en},
	author = {Zarges, Nane-Maiken},
	pages = {103},
	file = {Zarges - Evaluation of On-Node GPU Interconnects for Traini.pdf:/Users/i500763/Zotero/storage/WSDN3C3M/Zarges - Evaluation of On-Node GPU Interconnects for Traini.pdf:application/pdf},
}

@article{al-rahayfeh_novel_2019,
	title = {Novel {Approach} to {Task} {Scheduling} and {Load} {Balancing} {Using} the {Dominant} {Sequence} {Clustering} and {Mean} {Shift} {Clustering} {Algorithms}},
	volume = {11},
	issn = {1999-5903},
	url = {https://www.mdpi.com/1999-5903/11/5/109},
	doi = {10.3390/fi11050109},
	abstract = {Cloud computing (CC) is fast-growing and frequently adopted in information technology (IT) environments due to the beneﬁts it oﬀers. Task scheduling and load balancing are amongst the hot topics in the realm of CC. To overcome the shortcomings of the existing task scheduling and load balancing approaches, we propose a novel approach that uses dominant sequence clustering (DSC) for task scheduling and a weighted least connection (WLC) algorithm for load balancing. First, users’ tasks are clustered using the DSC algorithm, which represents user tasks as graph of one or more clusters. After task clustering, each task is ranked using Modiﬁed Heterogeneous Earliest Finish Time (MHEFT) algorithm. where the highest priority task is scheduled ﬁrst. Afterwards, virtual machines (VM) are clustered using a mean shift clustering (MSC) algorithm using kernel functions. Load balancing is subsequently performed using a WLC algorithm, which distributes the load based on server weight and capacity as well as client connectivity to server. A highly weighted or least connected server is selected for task allocation, which in turn increases the response time. Finally, we evaluate the proposed architecture using metrics such as response time, makespan, resource utilization, and service reliability.},
	language = {en},
	number = {5},
	urldate = {2021-04-22},
	journal = {Future Internet},
	author = {Al-Rahayfeh, Amer and Atiewi, Saleh and Abuhussein, Abdullah and Almiani, Muder},
	month = may,
	year = {2019},
	pages = {109},
	file = {Al-Rahayfeh et al. - 2019 - Novel Approach to Task Scheduling and Load Balanci.pdf:/Users/i500763/Zotero/storage/NFSSN2PM/Al-Rahayfeh et al. - 2019 - Novel Approach to Task Scheduling and Load Balanci.pdf:application/pdf},
}

@article{gayatri_comparing_nodate,
	title = {Comparing {Managed} {Memory} and {UVM} with and without {Prefetching} on {NVIDIA} {Volta} {GPUs}},
	abstract = {One of the major differences in many-core versus multicore architectures is the presence of two different memory spaces: a host space and a device space. In the case of NVIDIA GPUs, the device is supplied with data from the host via one of the multiple memory management API calls provided by the CUDA framework, such as CudaMallocManaged and CudaMemCpy. Modern systems, such as the Summit supercomputer, have the capability to avoid the use of CUDA calls for memory management and access the same data on GPU and CPU. This is done via the Address Translation Services (ATS) technology that gives a uniﬁed virtual address space for data allocated with malloc and new if there is an NVLink connection between the two memory spaces. In this paper, we perform a deep analysis of the performance achieved when using two types of uniﬁed virtual memory addressing: UVM and managed memory.},
	language = {en},
	author = {Gayatri, Rahulkumar and Gott, Kevin and Deslippe, Jack},
	pages = {6},
	file = {Gayatri et al. - Comparing Managed Memory and UVM with and without .pdf:/Users/i500763/Zotero/storage/ITJ46QRD/Gayatri et al. - Comparing Managed Memory and UVM with and without .pdf:application/pdf},
}

@article{shan_heterogeneous_2006,
	title = {Heterogeneous {Processing}: a {Strategy} for {Augmenting} {Moore}'s {Law}},
	url = {https://www.linuxjournal.com/article/8368},
	journal = {Linux Journal},
	author = {Shan, Amar},
	year = {2006},
}

@book{foster_designing_1995,
	address = {USA},
	title = {Designing and {Building} {Parallel} {Programs}: {Concepts} and {Tools} for {Parallel} {Software} {Engineering}},
	isbn = {0-201-57594-9},
	abstract = {From the Publisher:At last, a practitioner's guide to parallel programming! Students and professionals who use parallel or distributed computer systems will be able to solve real problems with Designing and Building Parallel Programs. This book provides a comprehensive introduction to parallel algorithm design, performance analysis, and program construction. It describes the tools needed to write parallel programs and provides numerous examples. A unique feature is the companion on-line version, accessible via the World Wide Web using browsers such as Mosaic. This provides a convenient hypertext version of the text with pointers to programming tools, example programs, and other resources on parallel and distributed computing.},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Foster, Ian},
	year = {1995},
}

@article{andersson_characterization_1997,
	title = {A {Characterization} of {Markov} {Equivalence} {Classes} for {Acyclic} {Digraphs}},
	volume = {25},
	issn = {00905364},
	url = {http://www.jstor.org/stable/2242556},
	abstract = {Undirected graphs and acyclic digraphs (ADG's), as well as their mutual extension to chain graphs, are widely used to describe dependencies among variables in multivariate distributions. In particular, the likelihood functions of ADG models admit convenient recursive factorizations that often allow explicit maximum likelihood estimates and that are well suited to building Bayesian networks for expert systems. Whereas the undirected graph associated with a dependence model is uniquely determined, there may be many ADG's that determine the same dependence (i.e., Markov) model. Thus, the family of all ADG's with a given set of vertices is naturally partitioned into Markov-equivalence classes, each class being associated with a unique statistical model. Statistical procedures, such as model selection or model averaging, that fail to take into account these equivalence classes may incur substantial computational or other inefficiencies. Here it is shown that each Markov-equivalence class is uniquely determined by a single chain graph, the essential graph, that is itself simultaneously Markov equivalent to all ADG's in the equivalence class. Essential graphs are characterized, a polynomial-time algorithm for their construction is given, and their applications to model selection and other statistical questions are described.},
	number = {2},
	journal = {The Annals of Statistics},
	author = {Andersson, Steen A. and Madigan, David and Perlman, Michael D.},
	year = {1997},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {505--541},
}

@article{chickering_optimal_2003,
	title = {Optimal {Structure} {Identification} with {Greedy} {Search}},
	volume = {3},
	issn = {1532-4435},
	url = {https://doi.org/10.1162/153244303321897717},
	doi = {10.1162/153244303321897717},
	abstract = {In this paper we prove the so-called "Meek Conjecture". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that—when applied to a particular sparsely-connected search space—provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Chickering, David Maxwell},
	month = mar,
	year = {2003},
	note = {Publisher: JMLR.org},
	pages = {507--554},
}

@article{pearl_introduction_2010,
	title = {An {Introduction} to {Causal} {Inference}},
	volume = {6},
	url = {https://doi.org/10.2202/1557-4679.1203},
	doi = {doi:10.2202/1557-4679.1203},
	number = {2},
	journal = {The International Journal of Biostatistics},
	author = {Pearl, Judea},
	year = {2010},
}

@article{yu_efficient_nodate,
	title = {Efficient {Matrix} {Factorization} on {Heterogeneous} {CPU}-{GPU} {Systems}},
	abstract = {Matrix Factorization (MF) has been widely applied in machine learning and data mining. Due to the large computational cost of MF, we aim to improve the efﬁciency of SGD-based MF computation by utilizing the massive parallel processing power of heterogeneous multiprocessors. The main challenge in parallel SGD algorithms on heterogeneous CPUGPU systems lies in the strategy to assign tasks. We design a novel strategy to divide the matrix into a set of blocks by considering two aspects. First, we observe that the matrix should be divided nonuniformly, and relatively large blocks should be assigned to GPUs to saturate the computing power of GPUs. In addition to exploiting the characteristics of hardware, the workloads assigned to two types of hardware should be balanced. We design a cost model tailored for our problem to accurately estimate the performance of hardware on different data sizes. Extensive experiments show that our proposed algorithm achieves high efﬁciency with a high quality of training quality.},
	language = {en},
	author = {Yu, Yuanhang and University, Zhejiang Gongshang},
	pages = {6},
	file = {Yu and University - Efficient Matrix Factorization on Heterogeneous CP.pdf:/Users/i500763/Zotero/storage/SDKDY797/Yu and University - Efficient Matrix Factorization on Heterogeneous CP.pdf:application/pdf},
}

@inproceedings{srivastava_parallel_2020,
	address = {Atlanta, GA, USA},
	title = {A {Parallel} {Framework} for {Constraint}-{Based} {Bayesian} {Network} {Learning} via {Markov} {Blanket} {Discovery}},
	isbn = {978-1-72819-998-6},
	url = {https://ieeexplore.ieee.org/document/9355320/},
	doi = {10.1109/SC41405.2020.00011},
	abstract = {Bayesian networks (BNs) are a widely used graphical model in machine learning. As learning the structure of BNs is NP-hard, high-performance computing methods are necessary for constructing large-scale networks. In this paper, we present a parallel framework to scale BN structure learning algorithms to tens of thousands of variables. Our framework is applicable to learning algorithms that rely on the discovery of Markov blankets (MBs) as an intermediate step. We demonstrate the applicability of our framework by parallelizing three different algorithms: Grow-Shrink (GS), Incremental Association MB (IAMB), and Interleaved IAMB (Inter-IAMB). Our implementations are able to construct BNs from real data sets with tens of thousands of variables and thousands of observations in less than a minute on 1024 cores, with a speedup of up to 845X and 82.5\% efﬁciency. Furthermore, we demonstrate using simulated data sets that our proposed parallel framework can scale to BNs of even higher dimensionality.},
	language = {en},
	urldate = {2021-04-22},
	booktitle = {{SC20}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Srivastava, Ankit and Chockalingam, Sriram P. and Aluru, Srinivas},
	month = nov,
	year = {2020},
	pages = {1--15},
	file = {Srivastava et al. - 2020 - A Parallel Framework for Constraint-Based Bayesian.pdf:/Users/i500763/Zotero/storage/SAAUJ3GS/Srivastava et al. - 2020 - A Parallel Framework for Constraint-Based Bayesian.pdf:application/pdf},
}

@article{kumaigorodski_alexander_fast_2021,
	title = {Fast {CSV} {Loading} {Using} {GPUs} and {RDMA} for {In}-{Memory} {Data} {Processing}},
	issn = {1617-5468},
	url = {http://dl.gi.de/handle/20.500.12116/35792},
	doi = {10.18420/BTW2021-01},
	abstract = {Comma-separated values (CSV) is a widely-used format for data exchange. Due to the format’s prevalence, virtually all industrial-strength database systems and stream processing frameworks support importing CSV input.},
	language = {en},
	urldate = {2021-04-22},
	author = {Kumaigorodski, Alexander and Lutz, Clemens and Markl, Volker},
	year = {2021},
	note = {ISBN: 9783885797050
Publisher: Gesellschaft für Informatik, Bonn},
	keywords = {CSV, CUDA, GPU, InfiniBand, Parsing, RDMA},
	file = {Kumaigorodski, Alexander et al. - 2021 - Fast CSV Loading Using GPUs and RDMA for In-Memory.pdf:/Users/i500763/Zotero/storage/ZEWJAZSZ/Kumaigorodski, Alexander et al. - 2021 - Fast CSV Loading Using GPUs and RDMA for In-Memory.pdf:application/pdf},
}

@article{kalisch_causal_2014,
	title = {Causal {Structure} {Learning} and {Inference}: {A} {Selective} {Review}},
	volume = {11},
	issn = {1684-3703},
	shorttitle = {Causal {Structure} {Learning} and {Inference}},
	url = {http://www.tandfonline.com/doi/full/10.1080/16843703.2014.11673322},
	doi = {10.1080/16843703.2014.11673322},
	language = {en},
	number = {1},
	urldate = {2021-04-22},
	journal = {Quality Technology \& Quantitative Management},
	author = {Kalisch, Markus and Bühlmann, Peter},
	month = jan,
	year = {2014},
	pages = {3--21},
	file = {Kalisch and Bühlmann - 2014 - Causal Structure Learning and Inference A Selecti.pdf:/Users/i500763/Zotero/storage/NUX8AFL3/Kalisch and Bühlmann - 2014 - Causal Structure Learning and Inference A Selecti.pdf:application/pdf},
}

@book{spirtes_causation_1993,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Causation, {Prediction}, and {Search}},
	volume = {81},
	isbn = {978-1-4612-7650-0 978-1-4612-2748-9},
	url = {http://link.springer.com/10.1007/978-1-4612-2748-9},
	language = {en},
	urldate = {2021-04-22},
	publisher = {Springer New York},
	author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	editor = {Berger, J. and Fienberg, S. and Gani, J. and Krickeberg, K. and Olkin, I. and Singer, B.},
	year = {1993},
	doi = {10.1007/978-1-4612-2748-9},
	file = {Spirtes et al. - 1993 - Causation, Prediction, and Search.pdf:/Users/i500763/Zotero/storage/HF7T6BYB/Spirtes et al. - 1993 - Causation, Prediction, and Search.pdf:application/pdf},
}

@article{kim_batch-aware_nodate,
	title = {Batch-{Aware} {Unified} {Memory} {Management} in {GPUs} for {Irregular} {Workloads}},
	abstract = {While unified virtual memory and demand paging in modern GPUs provide convenient abstractions to programmers for working with large-scale applications, they come at a significant performance cost. We provide the first comprehensive analysis of major inefficiencies that arise in page fault handling mechanisms employed in modern GPUs. To amortize the high costs in fault handling, the GPU runtime processes a large number of GPU page faults together. We observe that this batched processing of page faults introduces large-scale serialization that greatly hurts the GPU’s execution throughput. We show real machine measurements that corroborate our findings. Our goal is to mitigate these inefficiencies and enable efficient demand paging for GPUs. To this end, we propose a GPU runtime software and hardware solution that (1) increases the batch size (i.e., the number of page faults handled together), thereby amortizing the GPU runtime fault handling time, and reduces the number of batches by supporting CPU-like thread block context switching, and (2) takes page eviction off the critical path with no hardware changes by overlapping evictions with CPU-to-GPU page migrations. Our evaluation demonstrates that the proposed solution provides an average speedup of 2x over the state-of-the-art page prefetching. We show that our solution increases the batch size by 2.27x and reduces the total number of batches by 51\% on average. We also show that the average batch processing time is reduced by 27\%.},
	language = {en},
	author = {Kim, Hyojong and Sim, Jaewoong and Gera, Prasun and Hadidi, Ramyad and Kim, Hyesoon},
	pages = {14},
	file = {Kim et al. - Batch-Aware Unified Memory Management in GPUs for .pdf:/Users/i500763/Zotero/storage/7J6JC7E3/Kim et al. - Batch-Aware Unified Memory Management in GPUs for .pdf:application/pdf},
}

@article{abdelkader_dynamic_2012,
	title = {Dynamic task scheduling algorithm with load balancing for heterogeneous computing system},
	volume = {13},
	issn = {1110-8665},
	url = {https://www.sciencedirect.com/science/article/pii/S1110866512000175},
	doi = {10.1016/j.eij.2012.04.001},
	abstract = {In parallel computation, the scheduling and mapping tasks is considered the most critical problem which needs High Performance Computing (HPC) to solve it by breaking the problem into subtasks and working on those subtasks at the same time. The application sub tasks are assigned to underline machines and ordered for execution according to its proceeding to grantee efficient use of available resources such as minimize execution time and satisfy load balance between processors of the underline machine. The underline infrastructure may be homogeneous or heterogeneous. Homogeneous infrastructure could use the same machines power and performance. While heterogeneous infrastructure include machines differ in its performance, speed, and interconnection. According to work in this paper a new dynamic task scheduling algorithm for Heterogeneous called a Clustering Based HEFT with Duplication (CBHD) have been developed. The CBHD algorithm is considered an amalgamation between the most two important task scheduling in Heterogeneous machine, The Heterogeneous Earliest Finish Time (HEFT) and the Triplet Clustering algorithms. In the CBHD algorithm the duplication is required to improve the performance of algorithm. A comparative study among the developed CBHD, the HEFT, and the Triplet Cluster algorithms has been done. According to the comparative results, it is found that the developed CBHD algorithm satisfies better execution time than both HEFT algorithm and Triplet Cluster algorithm, and in the same time, it achieves the load balancing which considered one of the main performance factors in the dynamic environment.},
	language = {en},
	number = {2},
	urldate = {2021-04-22},
	journal = {Egyptian Informatics Journal},
	author = {Abdelkader, Doaa M. and Omara, Fatma},
	month = jul,
	year = {2012},
	keywords = {Heterogeneous system, Load balance, Makespane, Sleek time},
	pages = {135--145},
	file = {ScienceDirect Full Text PDF:/Users/i500763/Zotero/storage/U4Y8HF6D/Abdelkader and Omara - 2012 - Dynamic task scheduling algorithm with load balanc.pdf:application/pdf;ScienceDirect Snapshot:/Users/i500763/Zotero/storage/3UMICTDX/S1110866512000175.html:text/html},
}

@misc{sakharnykh_everything_nodate,
	title = {{EVERYTHING} {YOU} {NEED} {TO} {KNOW} {ABOUT} {UNIFIED} {MEMORY}},
	copyright = {NVIDIA},
	language = {en},
	author = {Sakharnykh, Nikolay},
	file = {Sakharnykh - EVERYTHING YOU NEED TO KNOW ABOUT UNIFIED MEMORY.pdf:/Users/i500763/Zotero/storage/Z7AQK26E/Sakharnykh - EVERYTHING YOU NEED TO KNOW ABOUT UNIFIED MEMORY.pdf:application/pdf},
}

@article{caldeira_ibm_nodate,
	title = {{IBM} {Power} {System} {AC922} {Introduction} and {Technical} {Overview}},
	language = {en},
	author = {Caldeira, Alexandre Bicas},
	pages = {74},
	file = {Caldeira - IBM Power System AC922 Introduction and Technical .pdf:/Users/i500763/Zotero/storage/IGCVNSZ5/Caldeira - IBM Power System AC922 Introduction and Technical .pdf:application/pdf},
}

@misc{noauthor_unified_nodate,
	title = {{UNIFIED} {MEMORY} {ON} {P9}+{V100}},
	copyright = {NVIDIA},
	language = {en},
	file = {ORNL_workshop_mar2018.pdf:/Users/i500763/Zotero/storage/J8VIK8NZ/ORNL_workshop_mar2018.pdf:application/pdf},
}

@article{glymour_review_2019,
	title = {Review of {Causal} {Discovery} {Methods} {Based} on {Graphical} {Models}},
	volume = {10},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full},
	doi = {10.3389/fgene.2019.00524},
	abstract = {A fundamental task in various disciplines of science, including biology, is to find underlying causal relations and make use of them. Causal relations can be seen if interventions are properly applied; however, in many cases they are difficult or even impossible to conduct. It is then necessary to discover causal relations by analyzing statistical properties of purely observational data, which is known as causal discovery or causal structure search. This paper aims to give a introduction to and a brief review of the computational methods for causal discovery that were developed in the past three decades, including constraint-based and score-based methods and those based on functional causal models, supplemented by some illustrations and applications.},
	language = {English},
	urldate = {2021-04-22},
	journal = {Frontiers in Genetics},
	author = {Glymour, Clark and Zhang, Kun and Spirtes, Peter},
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {causal discovery, Conditional independence, Directed graphical causal models, non-Gaussian distribution, Non-linear models, Statistical independence, Structural Equation Models},
	file = {Full Text PDF:/Users/i500763/Zotero/storage/WABWRFU6/Glymour et al. - 2019 - Review of Causal Discovery Methods Based on Graphi.pdf:application/pdf},
}

@book{noauthor_practical_2015,
	address = {New York, NY},
	edition = {1st edition},
	title = {Practical approaches to causal relationship exploration},
	isbn = {978-3-319-14432-0},
	publisher = {Springer Science+Business Media},
	year = {2015},
	file = {Samples.pdf:/Users/i500763/Zotero/storage/5SU64RV2/Samples.pdf:application/pdf},
}

@inproceedings{carabano_exploration_2013,
	address = {Darmstadt, Germany},
	title = {An exploration of heterogeneous systems},
	isbn = {978-1-4673-6180-4},
	url = {http://ieeexplore.ieee.org/document/6581542/},
	doi = {10.1109/ReCoSoC.2013.6581542},
	abstract = {Heterogeneous computing represents a trendy way to achieve further scalability in the high-performance computing area. It aims to join different processing units in a networkedbased system such that each task is preferably executed by the unit which is able to efficiently perform that task. Memory hierarchy, instruction set, control logic, and other properties may differ in processing units so as to be specialized for different variety of problems. However, it will be more time-consuming for computer engineers to understand, design, and program on these systems. On the other hand, proper problems running on wellchosen heterogeneous systems present higher performance and superior energy efficiency. Such balance of attributes seldom makes a heterogeneous system useful for other fields than embedded computing or high-performance computing. Among them, embedded computing is more area and energy efficient while high-performance computing obtains more performance.},
	language = {en},
	urldate = {2021-04-22},
	booktitle = {2013 8th {International} {Workshop} on {Reconfigurable} and {Communication}-{Centric} {Systems}-on-{Chip} ({ReCoSoC})},
	publisher = {IEEE},
	author = {Carabano, Jesus and Dios, Francisco and Daneshtalab, Masoud and Ebrahimi, Masoumeh},
	month = jul,
	year = {2013},
	pages = {1--7},
	file = {Carabano et al. - 2013 - An exploration of heterogeneous systems.pdf:/Users/i500763/Zotero/storage/LU65KCRW/Carabano et al. - 2013 - An exploration of heterogeneous systems.pdf:application/pdf},
}

@article{colombo_order-independent_nodate,
	title = {Order-{Independent} {Constraint}-{Based} {Causal} {Structure} {Learning}},
	abstract = {We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The ﬁrst step of all these algorithms consists of the adjacency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modiﬁcations of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modiﬁcations are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modiﬁcations in simulation studies and on a yeast gene expression data set. We show that our modiﬁcations yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.},
	language = {en},
	author = {Colombo, Diego and Maathuis, Marloes H},
	pages = {42},
	file = {Colombo and Maathuis - Order-Independent Constraint-Based Causal Structur.pdf:/Users/i500763/Zotero/storage/72VWENC8/Colombo and Maathuis - Order-Independent Constraint-Based Causal Structur.pdf:application/pdf},
}

@article{kalisch_understanding_2010,
	title = {Understanding human functioning using graphical models},
	volume = {10},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/1471-2288-10-14},
	doi = {10.1186/1471-2288-10-14},
	abstract = {Functioning and disability are universal human experiences. However, our current understanding of functioning from a comprehensive perspective is limited. The development of the International Classification of Functioning, Disability and Health (ICF) on the one hand and recent developments in graphical modeling on the other hand might be combined and open the door to a more comprehensive understanding of human functioning. The objective of our paper therefore is to explore how graphical models can be used in the study of ICF data for a range of applications.},
	number = {1},
	urldate = {2021-04-28},
	journal = {BMC Medical Research Methodology},
	author = {Kalisch, Markus and Fellinghauer, Bernd AG and Grill, Eva and Maathuis, Marloes H. and Mansmann, Ulrich and Bühlmann, Peter and Stucki, Gerold},
	month = feb,
	year = {2010},
	keywords = {Causal Effect, Dependence Structure, Dimension Reduction, Directed Acyclic Graph, Graphical Model},
	pages = {14},
	file = {Full Text PDF:/Users/i500763/Zotero/storage/YG52YM3K/Kalisch et al. - 2010 - Understanding human functioning using graphical mo.pdf:application/pdf;Snapshot:/Users/i500763/Zotero/storage/KKSJ4WPF/1471-2288-10-14.html:text/html},
}
