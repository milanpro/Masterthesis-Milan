
@article{abdelkaderDynamicTaskScheduling2012,
  title = {Dynamic Task Scheduling Algorithm with Load Balancing for Heterogeneous Computing System},
  author = {Abdelkader, Doaa M. and Omara, Fatma},
  year = {2012},
  month = jul,
  volume = {13},
  pages = {135--145},
  issn = {1110-8665},
  doi = {10.1016/j.eij.2012.04.001},
  abstract = {In parallel computation, the scheduling and mapping tasks is considered the most critical problem which needs High Performance Computing (HPC) to solve it by breaking the problem into subtasks and working on those subtasks at the same time. The application sub tasks are assigned to underline machines and ordered for execution according to its proceeding to grantee efficient use of available resources such as minimize execution time and satisfy load balance between processors of the underline machine. The underline infrastructure may be homogeneous or heterogeneous. Homogeneous infrastructure could use the same machines power and performance. While heterogeneous infrastructure include machines differ in its performance, speed, and interconnection. According to work in this paper a new dynamic task scheduling algorithm for Heterogeneous called a Clustering Based HEFT with Duplication (CBHD) have been developed. The CBHD algorithm is considered an amalgamation between the most two important task scheduling in Heterogeneous machine, The Heterogeneous Earliest Finish Time (HEFT) and the Triplet Clustering algorithms. In the CBHD algorithm the duplication is required to improve the performance of algorithm. A comparative study among the developed CBHD, the HEFT, and the Triplet Cluster algorithms has been done. According to the comparative results, it is found that the developed CBHD algorithm satisfies better execution time than both HEFT algorithm and Triplet Cluster algorithm, and in the same time, it achieves the load balancing which considered one of the main performance factors in the dynamic environment.},
  file = {/Users/i500763/Zotero/storage/U4Y8HF6D/Abdelkader and Omara - 2012 - Dynamic task scheduling algorithm with load balanc.pdf;/Users/i500763/Zotero/storage/3UMICTDX/S1110866512000175.html},
  journal = {Egyptian Informatics Journal},
  keywords = {Heterogeneous system,Load balance,Makespane,Sleek time},
  language = {en},
  number = {2}
}

@article{al-rahayfehNovelApproachTask2019,
  title = {Novel {{Approach}} to {{Task Scheduling}} and {{Load Balancing Using}} the {{Dominant Sequence Clustering}} and {{Mean Shift Clustering Algorithms}}},
  author = {{Al-Rahayfeh}, Amer and Atiewi, Saleh and Abuhussein, Abdullah and Almiani, Muder},
  year = {2019},
  month = may,
  volume = {11},
  pages = {109},
  issn = {1999-5903},
  doi = {10.3390/fi11050109},
  abstract = {Cloud computing (CC) is fast-growing and frequently adopted in information technology (IT) environments due to the benefits it offers. Task scheduling and load balancing are amongst the hot topics in the realm of CC. To overcome the shortcomings of the existing task scheduling and load balancing approaches, we propose a novel approach that uses dominant sequence clustering (DSC) for task scheduling and a weighted least connection (WLC) algorithm for load balancing. First, users' tasks are clustered using the DSC algorithm, which represents user tasks as graph of one or more clusters. After task clustering, each task is ranked using Modified Heterogeneous Earliest Finish Time (MHEFT) algorithm. where the highest priority task is scheduled first. Afterwards, virtual machines (VM) are clustered using a mean shift clustering (MSC) algorithm using kernel functions. Load balancing is subsequently performed using a WLC algorithm, which distributes the load based on server weight and capacity as well as client connectivity to server. A highly weighted or least connected server is selected for task allocation, which in turn increases the response time. Finally, we evaluate the proposed architecture using metrics such as response time, makespan, resource utilization, and service reliability.},
  file = {/Users/i500763/Zotero/storage/NFSSN2PM/Al-Rahayfeh et al. - 2019 - Novel Approach to Task Scheduling and Load Balanci.pdf},
  journal = {Future Internet},
  language = {en},
  number = {5}
}

@inproceedings{amdahlValiditySingleProcessor1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, Spring Joint Computer Conference on - {{AFIPS}} '67 ({{Spring}})},
  author = {Amdahl, Gene M.},
  year = {1967},
  pages = {483},
  publisher = {{ACM Press}},
  address = {{Atlantic City, New Jersey}},
  doi = {10.1145/1465482.1465560},
  file = {/Users/i500763/Zotero/storage/EPHFYR5P/Amdahl - 1967 - Validity of the single processor approach to achie.pdf},
  language = {en}
}

@misc{AnalysisX86Vs,
  title = {Analysis: X86 {{Vs PPC}} \textendash{} {{OSnews}}},
  file = {/Users/i500763/Zotero/storage/4UY8SWMW/analysis-x86-vs-ppc.html},
  howpublished = {https://www.osnews.com/story/3997/analysis-x86-vs-ppc/}
}

@article{anderssonCharacterizationMarkovEquivalence1997,
  title = {A {{Characterization}} of {{Markov Equivalence Classes}} for {{Acyclic Digraphs}}},
  author = {Andersson, Steen A. and Madigan, David and Perlman, Michael D.},
  year = {1997},
  volume = {25},
  pages = {505--541},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {00905364},
  abstract = {Undirected graphs and acyclic digraphs (ADG's), as well as their mutual extension to chain graphs, are widely used to describe dependencies among variables in multivariate distributions. In particular, the likelihood functions of ADG models admit convenient recursive factorizations that often allow explicit maximum likelihood estimates and that are well suited to building Bayesian networks for expert systems. Whereas the undirected graph associated with a dependence model is uniquely determined, there may be many ADG's that determine the same dependence (i.e., Markov) model. Thus, the family of all ADG's with a given set of vertices is naturally partitioned into Markov-equivalence classes, each class being associated with a unique statistical model. Statistical procedures, such as model selection or model averaging, that fail to take into account these equivalence classes may incur substantial computational or other inefficiencies. Here it is shown that each Markov-equivalence class is uniquely determined by a single chain graph, the essential graph, that is itself simultaneously Markov equivalent to all ADG's in the equivalence class. Essential graphs are characterized, a polynomial-time algorithm for their construction is given, and their applications to model selection and other statistical questions are described.},
  journal = {The Annals of Statistics},
  number = {2}
}

@article{barnesILLIACIVComputer1968,
  title = {The {{ILLIAC IV Computer}}},
  author = {Barnes, G.H. and Brown, R.M. and Kato, M. and Kuck, D.J. and Slotnick, D.L. and Stokes, R.A.},
  year = {1968},
  month = aug,
  volume = {C-17},
  pages = {746--757},
  issn = {1557-9956},
  doi = {10.1109/TC.1968.229158},
  abstract = {Abstract\textemdash The structure of ILLIAC IV, a parallel-array computer containing 256 processing elements, is described. Special features include multiarray processing, multiprecision arithmetic, and fast data-routing interconnections. Individual processing elements execute 4\texttimes 106 instructions per second to yield an effective rate of 109 operations per second.},
  file = {/Users/i500763/Zotero/storage/9JJBS3L9/1687448.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Array signal processing,Arrays,Computers,Index Termsâ€”Array; computer structure; look-ahead; machine language; parallel processing; speed; thin-film memory.,Parallel processing,Process control,Program processors,Routing},
  number = {8}
}

@inproceedings{binottoDynamicReconfigurableLoadbalancing2010,
  title = {Towards Dynamic Reconfigurable Load-Balancing for Hybrid Desktop Platforms},
  booktitle = {2010 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}, {{Workshops}} and {{Phd Forum}} ({{IPDPSW}})},
  author = {Binotto, Alecio P. D. and Pereira, Carlos E. and Fellner, Dieter W.},
  year = {2010},
  month = apr,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Atlanta, GA}},
  doi = {10.1109/IPDPSW.2010.5470804},
  abstract = {High-performance platforms are required by applications that use massive calculations. Actually, desktop accelerators (like the GPUs) form a powerful heterogeneous platform in conjunction with multi-core CPUs. To improve application performance on these hybrid platforms, loadbalancing plays an important role to distribute workload. However, such scheduling problem faces challenges since the cost of a task at a Processing Unit (PU) is non-deterministic and depends on parameters that cannot be known a priori, like input data, online creation of tasks, scenario changing, etc. Therefore, self-adaptive computing is a potential paradigm as it can provide flexibility to explore computational resources and improve performance on different execution scenarios.},
  file = {/Users/i500763/Zotero/storage/TX853LYE/Binotto et al. - 2010 - Towards dynamic reconfigurable load-balancing for .pdf},
  isbn = {978-1-4244-6533-0 978-1-4244-6534-7},
  language = {en}
}

@article{blumofeSchedulingMultithreadedComputations1999,
  title = {Scheduling Multithreaded Computations by Work Stealing},
  author = {Blumofe, Robert D. and Leiserson, Charles E.},
  year = {1999},
  month = sep,
  volume = {46},
  pages = {720--748},
  issn = {0004-5411},
  doi = {10.1145/324133.324234},
  abstract = {This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is ``work stealing,'' in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies. Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T {$\infty$} , where T1 is the minimum serial execution time of the multithreaded computation and (T {$\infty$} is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT {$\infty$}( 1 + nd)Smax), where Smax is the size of the largest activation record of any thread and nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.},
  file = {/Users/i500763/Zotero/storage/LQPWRE67/Blumofe and Leiserson - 1999 - Scheduling multithreaded computations by work stea.pdf},
  journal = {Journal of the ACM},
  keywords = {critical-path length,multiprocessor,multithreading,randomized algorithm,thread scheduling,work stealing},
  number = {5}
}

@book{breshearsArtConcurrencyThread2009,
  title = {The {{Art}} of {{Concurrency}}: {{A Thread Monkey}}'s {{Guide}} to {{Writing Parallel Applications}}},
  shorttitle = {The {{Art}} of {{Concurrency}}},
  author = {Breshears, Clay},
  year = {2009},
  publisher = {{O'Reilly Media, Inc.}},
  abstract = {If you're looking to take full advantage of multi-core processors with concurrent programming, this practical book provides the knowledge and hands-on experience you need. The Art of Concurrency is one of the few resources to focus on implementing algorithms in the shared-memory model of multi-core processors, rather than just theoretical models or distributed-memory architectures. The book provides detailed explanations and usable samples to help you transform algorithms from serial to parallel code, along with advice and analysis for avoiding mistakes that programmers typically make when first attempting these computations. Written by an Intel engineer with over two decades of parallel and concurrent programming experience, this book will help you: Understand parallelism and concurrency Explore differences between programming for shared-memory and distributed-memory Learn guidelines for designing multithreaded applications, including testing and tuning Discover how to make best use of different threading libraries, including Windows threads, POSIX threads, OpenMP, and Intel Threading Building Blocks Explore how to implement concurrent algorithms that involve sorting, searching, graphs, and other practical computations The Art of Concurrency shows you how to keep algorithms scalable to take advantage of new processors with even more cores. For developing parallel code algorithms for concurrent programming, this book is a must.},
  isbn = {978-0-596-52153-0}
}

@article{caldeiraIBMPowerSystem,
  title = {{{IBM Power System AC922 Introduction}} and {{Technical Overview}}},
  author = {Caldeira, Alexandre Bicas},
  pages = {74},
  file = {/Users/i500763/Zotero/storage/IGCVNSZ5/Caldeira - IBM Power System AC922 Introduction and Technical .pdf},
  language = {en}
}

@inproceedings{carabanoExplorationHeterogeneousSystems2013,
  title = {An Exploration of Heterogeneous Systems},
  booktitle = {2013 8th {{International Workshop}} on {{Reconfigurable}} and {{Communication}}-{{Centric Systems}}-on-{{Chip}} ({{ReCoSoC}})},
  author = {Carabano, Jesus and Dios, Francisco and Daneshtalab, Masoud and Ebrahimi, Masoumeh},
  year = {2013},
  month = jul,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Darmstadt, Germany}},
  doi = {10.1109/ReCoSoC.2013.6581542},
  abstract = {Heterogeneous computing represents a trendy way to achieve further scalability in the high-performance computing area. It aims to join different processing units in a networkedbased system such that each task is preferably executed by the unit which is able to efficiently perform that task. Memory hierarchy, instruction set, control logic, and other properties may differ in processing units so as to be specialized for different variety of problems. However, it will be more time-consuming for computer engineers to understand, design, and program on these systems. On the other hand, proper problems running on wellchosen heterogeneous systems present higher performance and superior energy efficiency. Such balance of attributes seldom makes a heterogeneous system useful for other fields than embedded computing or high-performance computing. Among them, embedded computing is more area and energy efficient while high-performance computing obtains more performance.},
  file = {/Users/i500763/Zotero/storage/LU65KCRW/Carabano et al. - 2013 - An exploration of heterogeneous systems.pdf},
  isbn = {978-1-4673-6180-4},
  language = {en}
}

@article{chickeringOptimalStructureIdentification2003,
  title = {Optimal {{Structure Identification}} with {{Greedy Search}}},
  author = {Chickering, David Maxwell},
  year = {2003},
  month = mar,
  volume = {3},
  pages = {507--554},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  doi = {10.1162/153244303321897717},
  abstract = {In this paper we prove the so-called "Meek Conjecture". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that\textemdash when applied to a particular sparsely-connected search space\textemdash provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.},
  journal = {J. Mach. Learn. Res.},
  number = {null}
}

@inproceedings{cirouTripletClusteringScheduling2001,
  title = {Triplet: {{A}} Clustering Scheduling Algorithm for Heterogeneous Systems},
  shorttitle = {Triplet},
  booktitle = {Proceedings {{International Conference}} on {{Parallel Processing Workshops}}},
  author = {Cirou, B. and Jeannot, E.},
  year = {2001},
  pages = {231--236},
  publisher = {{IEEE Comput. Soc}},
  address = {{Valencia, Spain}},
  doi = {10.1109/ICPPW.2001.951956},
  abstract = {The goal of the OURAGAN project is to provide access of meta-computing resources to Scilab users. We present here an approach that consists, given a Scilab script, in scheduling and executing this script on an heterogeneous cluster of machines. One of the most effective scheduling technique is called clustering which consists in grouping tasks on virtual processors (clusters) and then mapping clusters onto real processors. In this paper, we study and apply the clustering technique for heterogeneous systems. We present a clustering algorithm called triplet, study its performance and compare it to the HEFT algorithm. We show that triplet has good characteristics and outperforms HEFT in most of the cases.},
  file = {/Users/i500763/Zotero/storage/8B3G8QM2/Cirou and Jeannot - 2001 - Triplet A clustering scheduling algorithm for het.pdf},
  isbn = {978-0-7695-1260-0},
  language = {en}
}

@article{cohenSolvingComputationalProblems2009,
  title = {Solving {{Computational Problems}} with {{GPU Computing}}},
  author = {Cohen, Jonathan and Garland, Michael},
  year = {2009},
  month = sep,
  volume = {11},
  pages = {58--63},
  publisher = {{IEEE Computer Society}},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2009.144},
  abstract = {Modern GPUs are massively parallel microprocessors that can deliver very high performance for the parallel computations common in science and engineering.},
  file = {/Users/i500763/Zotero/storage/TSIZ2MYI/Cohen and Garland - 2009 - Solving Computational Problems with GPU Computing.pdf;/Users/i500763/Zotero/storage/SBLGENUF/13rRUyft7yu.html},
  journal = {Computing in Science \& Engineering},
  language = {English},
  number = {05}
}

@article{colomboOrderIndependentConstraintBasedCausal,
  title = {Order-{{Independent Constraint}}-{{Based Causal Structure Learning}}},
  author = {Colombo, Diego and Maathuis, Marloes H},
  pages = {42},
  abstract = {We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The first step of all these algorithms consists of the adjacency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modifications of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modifications in simulation studies and on a yeast gene expression data set. We show that our modifications yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.},
  file = {/Users/i500763/Zotero/storage/72VWENC8/Colombo and Maathuis - Order-Independent Constraint-Based Causal Structur.pdf},
  language = {en}
}

@misc{dellGPUExpanseHPC2020,
  title = {The {{GPU}} ``{{Expanse}}'': {{HPC Acceleration}} for the {{Masses}}},
  shorttitle = {The {{GPU}} ``{{Expanse}}''},
  author = {from Dell, Sponsored Content},
  year = {2020},
  month = apr,
  abstract = {In the early days of GPU-accelerated supercomputers, accelerators were installed with the mission of delivering ultra-high performance for a few select},
  file = {/Users/i500763/Zotero/storage/U9LMHLD3/the-gpu-expanse-hpc-acceleration-for-the-masses.html},
  howpublished = {https://www.nextplatform.com/2020/04/21/the-gpu-expanse-hpc-acceleration-for-the-masses/},
  journal = {The Next Platform},
  language = {en-US}
}

@article{diefendorffAltiVecExtensionPowerPC2000,
  title = {{{AltiVec}} Extension to {{PowerPC}} Accelerates Media Processing},
  author = {Diefendorff, K. and Dubey, P.K. and Hochsprung, R. and Scale, H.},
  year = {March-April/2000},
  volume = {20},
  pages = {85--95},
  issn = {02721732},
  doi = {10.1109/40.848475},
  file = {/Users/i500763/Zotero/storage/I5TH8BBH/Diefendorff et al. - 2000 - AltiVec extension to PowerPC accelerates media pro.pdf},
  journal = {IEEE Micro},
  language = {en},
  number = {2}
}

@incollection{dijkstraCooperatingSequentialProcesses2002,
  title = {Cooperating {{Sequential Processes}}},
  booktitle = {The {{Origin}} of {{Concurrent Programming}}: {{From Semaphores}} to {{Remote Procedure Calls}}},
  author = {Dijkstra, Edsger W.},
  editor = {Hansen, Per Brinch},
  year = {2002},
  pages = {65--138},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4757-3472-0_2},
  abstract = {This chapter is intended for all those who expect that in their future activities they will become seriously involved in the problems that arise in either the design or the more advanced applications of digital information processing equipment; they are further intended for all those who are just interested in information processing.},
  file = {/Users/i500763/Zotero/storage/RV5Y8EBD/Dijkstra - 2002 - Cooperating Sequential Processes.pdf},
  isbn = {978-1-4757-3472-0},
  keywords = {Communication Facility,Critical Section,Mutual Exclusion,Sequential Process,Transmission Variable},
  language = {en}
}

@misc{EasyIntroductionCUDA2012,
  title = {An {{Easy Introduction}} to {{CUDA C}} and {{C}}++},
  year = {2012},
  month = oct,
  abstract = {This first post in a series on CUDA C and C++ covers the basic concepts of parallel programming on the CUDA platform with C/C++.},
  file = {/Users/i500763/Zotero/storage/XRNZQFHU/easy-introduction-cuda-c-and-c.html},
  howpublished = {https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/},
  journal = {NVIDIA Developer Blog},
  language = {en-US}
}

@inproceedings{esmaeilzadehDarkSiliconEnd2011,
  title = {Dark Silicon and the End of Multicore Scaling},
  booktitle = {2011 38th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Esmaeilzadeh, Hadi and Blem, Emily and Amant, Ren{\'e}e St. and Sankaralingam, Karthikeyan and Burger, Doug},
  year = {2011},
  month = jun,
  pages = {365--376},
  issn = {1063-6897},
  abstract = {Since 2005, processor designers have increased core counts to exploit Moore's Law scaling, rather than focusing on single-core performance. The failure of Dennard scaling, to which the shift to multicore parts is partially a response, may soon limit multicore scaling just as single-core scaling has been curtailed. This paper models multicore scaling limits by combining device scaling, single-core scaling, and multicore scaling to measure the speedup potential for a set of parallel workloads for the next five technology generations. For device scaling, we use both the ITRS projections and a set of more conservative device scaling parameters. To model single-core scaling, we combine measurements from over 150 processors to derive Pareto-optimal frontiers for area/performance and power/performance. Finally, to model multicore scaling, we build a detailed performance model of upper-bound performance and lower-bound core power. The multicore designs we study include single-threaded CPU-like and massively threaded GPU-like multicore chip organizations with symmetric, asymmetric, dynamic, and composed topologies. The study shows that regardless of chip organization and topology, multicore scaling is power limited to a degree not widely appreciated by the computing community. Even at 22 nm (just one year from now), 21\% of a fixed-size chip must be powered off, and at 8 nm, this number grows to more than 50\%. Through 2024, only 7.9\texttimes{} average speedup is possible across commonly used parallel workloads, leaving a nearly 24-fold gap from a target of doubled performance per generation.},
  file = {/Users/i500763/Zotero/storage/QBGFYLJB/Esmaeilzadeh et al. - Dark Silicon and the End of Multicore Scaling.pdf;/Users/i500763/Zotero/storage/JK7S2PBH/6307773.html},
  keywords = {Dark Silicon,Instruction sets,Microarchitecture,Modeling,Multicore,Multicore processing,Organizations,Performance evaluation,Power,Technology Scaling,Topology,Transistors}
}

@article{falsafiPrimerHardwarePrefetching2014,
  title = {A {{Primer}} on {{Hardware Prefetching}}},
  author = {Falsafi, Babak and Wenisch, Thomas F.},
  year = {2014},
  month = may,
  volume = {9},
  pages = {1--67},
  publisher = {{Morgan \& Claypool Publishers}},
  issn = {1935-3235},
  doi = {10.2200/S00581ED1V01Y201405CAC028},
  file = {/Users/i500763/Zotero/storage/8BI6QNEA/Falsafi and Wenisch - 2014 - A Primer on Hardware Prefetching.pdf;/Users/i500763/Zotero/storage/TMZR5WAV/S00581ED1V01Y201405CAC028.html},
  journal = {Synthesis Lectures on Computer Architecture},
  number = {1}
}

@article{feichtingerFlexiblePatchbasedLattice2011,
  title = {A Flexible {{Patch}}-Based Lattice {{Boltzmann}} Parallelization Approach for Heterogeneous {{GPU}}\textendash{{CPU}} Clusters},
  author = {Feichtinger, Christian and Habich, Johannes and K{\"o}stler, Harald and Hager, Georg and R{\"u}de, Ulrich and Wellein, Gerhard},
  year = {2011},
  month = sep,
  volume = {37},
  pages = {536--549},
  issn = {01678191},
  doi = {10.1016/j.parco.2011.03.005},
  abstract = {Sustaining a large fraction of single GPU performance in parallel computations is considered to be the major problem of GPU-based clusters. We address this issue in the context of a lattice Boltzmann flow solver that is integrated in the WaLBerla software framework. Our multi-GPU implementation uses a block-structured MPI parallelization and is suitable for load balancing and heterogeneous computations on CPUs and GPUs. The overhead required for multi-GPU simulations is discussed in detail. It is demonstrated that a large fraction of the kernel performance can be sustained for weak scaling on InfiniBand clusters, leading to excellent parallel efficiency. However, in strong scaling scenarios using multiple GPUs is much less efficient than running CPU-only simulations on IBM BG/P and x86-based clusters. Hence, a cost analysis must determine the best course of action for a particular simulation task and hardware configuration. Finally we present weak scaling results of heterogeneous simulations conducted on CPUs and GPUs simultaneously, using clusters equipped with varying node configurations.},
  file = {/Users/i500763/Zotero/storage/XVJTZ3XJ/Feichtinger et al. - 2011 - A flexible Patch-based lattice Boltzmann paralleli.pdf},
  journal = {Parallel Computing},
  language = {en},
  number = {9}
}

@book{fosterDesigningBuildingParallel1995,
  title = {Designing and {{Building Parallel Programs}}: {{Concepts}} and {{Tools}} for {{Parallel Software Engineering}}},
  author = {Foster, Ian},
  year = {1995},
  publisher = {{Addison-Wesley Longman Publishing Co., Inc.}},
  address = {{USA}},
  abstract = {From the Publisher:At last, a practitioner's guide to parallel programming! Students and professionals who use parallel or distributed computer systems will be able to solve real problems with Designing and Building Parallel Programs. This book provides a comprehensive introduction to parallel algorithm design, performance analysis, and program construction. It describes the tools needed to write parallel programs and provides numerous examples. A unique feature is the companion on-line version, accessible via the World Wide Web using browsers such as Mosaic. This provides a convenient hypertext version of the text with pointers to programming tools, example programs, and other resources on parallel and distributed computing.},
  isbn = {0-201-57594-9}
}

@incollection{galindoDynamicLoadBalancing2008,
  title = {Dynamic {{Load Balancing}} on {{Dedicated Heterogeneous Systems}}},
  booktitle = {Recent {{Advances}} in {{Parallel Virtual Machine}} and {{Message Passing Interface}}},
  author = {Galindo, Ismael and Almeida, Francisco and {Bad{\'i}a-Contelles}, Jos{\'e} Manuel},
  editor = {Lastovetsky, Alexey and Kechadi, Tahar and Dongarra, Jack},
  year = {2008},
  volume = {5205},
  pages = {64--74},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87475-1_14},
  abstract = {Parallel computing in heterogeneous environments is drawing considerable attention due to the growing number of these kind of systems. Adapting existing code and libraries to such systems is a fundamental problem. The performance of this code is affected by the large interdependence between the code and these parallel architectures. We have developed a dynamic load balancing library that allows parallel code to be adapted to heterogeneous systems for a wide variety of problems. The overhead introduced by our system is minimal and the cost to the programmer negligible. The strategy was validated on several problems to confirm the soundness of our proposal.},
  file = {/Users/i500763/Zotero/storage/5RSY2ZB6/Galindo et al. - 2008 - Dynamic Load Balancing on Dedicated Heterogeneous .pdf},
  isbn = {978-3-540-87474-4 978-3-540-87475-1},
  language = {en}
}

@misc{ganesannarayanasamyPowerAIDeepDive,
  title = {{{PowerAI Deep}} Dive},
  author = {Ganesan Narayanasamy},
  howpublished = {https://www.slideshare.net/ganesannarayanasamy/powerai-deep-dive}
}

@inproceedings{gangulyAdaptivePageMigration2020,
  title = {Adaptive {{Page Migration}} for {{Irregular Data}}-Intensive {{Applications}} under {{GPU Memory Oversubscription}}},
  booktitle = {2020 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Ganguly, Debashis and Zhang, Ziyu and Yang, Jun and Melhem, Rami},
  year = {2020},
  month = may,
  pages = {451--461},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/IPDPS47924.2020.00054},
  abstract = {Unified Memory in heterogeneous systems serves a wide range of applications. However, limited capacity of the device memory becomes a first order performance bottleneck for data-intensive general-purpose applications with increasing working sets. The performance overhead under memory oversubscription depends on the memory access pattern of the corresponding workload. While a regular application with sequential, dense memory access suffers from long latency writebacks, performance of a irregular application with sparse, seldom access to large data-sets degrades due to page thrashing. Although smart spatio-temporal prefetching and large page eviction yield good performance in general, remote zero-copy access to host-pinned memory proves to be beneficial for irregular, data-intensive applications. Further, new generation GPUs introduced hardware access counters to delay page migration and reduce memory thrashing. However, the responsibility of deciding what strategy is the best fit for a given application relies heavily on the programmer based on thorough understanding of the memory access pattern through intrusive profiling. In this work, we propose a programmer-agnostic runtime that leverages the hardware access counters to automatically categorize memory allocations based on the access pattern and frequency. The proposed heuristic adaptively navigates between remote zero-copy access to host-pinned memory and first-touch page migration based on the trade-off between low latency remote access and high-bandwidth local access. We show that although designed to address memory oversubscription, our scheme has no impact on performance when working sets fit in the device-local memory. Experimental results show that our scheme provides performance improvement of 22\% to 78\% for irregular applications under 125\% memory oversubscription compared to the state of the art. At the same time, regular applications are not impacted by the framework.},
  file = {/Users/i500763/Zotero/storage/98DLKI94/Ganguly et al. - 2020 - Adaptive Page Migration for Irregular Data-intensi.pdf},
  isbn = {978-1-72816-876-0},
  language = {en}
}

@article{gayatriComparingManagedMemory,
  title = {Comparing {{Managed Memory}} and {{UVM}} with and without {{Prefetching}} on {{NVIDIA Volta GPUs}}},
  author = {Gayatri, Rahulkumar and Gott, Kevin and Deslippe, Jack},
  pages = {6},
  abstract = {One of the major differences in many-core versus multicore architectures is the presence of two different memory spaces: a host space and a device space. In the case of NVIDIA GPUs, the device is supplied with data from the host via one of the multiple memory management API calls provided by the CUDA framework, such as CudaMallocManaged and CudaMemCpy. Modern systems, such as the Summit supercomputer, have the capability to avoid the use of CUDA calls for memory management and access the same data on GPU and CPU. This is done via the Address Translation Services (ATS) technology that gives a unified virtual address space for data allocated with malloc and new if there is an NVLink connection between the two memory spaces. In this paper, we perform a deep analysis of the performance achieved when using two types of unified virtual memory addressing: UVM and managed memory.},
  file = {/Users/i500763/Zotero/storage/ITJ46QRD/Gayatri et al. - Comparing Managed Memory and UVM with and without .pdf},
  language = {en}
}

@article{glymourReviewCausalDiscovery2019,
  title = {Review of {{Causal Discovery Methods Based}} on {{Graphical Models}}},
  author = {Glymour, Clark and Zhang, Kun and Spirtes, Peter},
  year = {2019},
  volume = {10},
  publisher = {{Frontiers}},
  issn = {1664-8021},
  doi = {10.3389/fgene.2019.00524},
  abstract = {A fundamental task in various disciplines of science, including biology, is to find underlying causal relations and make use of them. Causal relations can be seen if interventions are properly applied; however, in many cases they are difficult or even impossible to conduct. It is then necessary to discover causal relations by analyzing statistical properties of purely observational data, which is known as causal discovery or causal structure search. This paper aims to give a introduction to and a brief review of the computational methods for causal discovery that were developed in the past three decades, including constraint-based and score-based methods and those based on functional causal models, supplemented by some illustrations and applications.},
  file = {/Users/i500763/Zotero/storage/WABWRFU6/Glymour et al. - 2019 - Review of Causal Discovery Methods Based on Graphi.pdf},
  journal = {Frontiers in Genetics},
  keywords = {causal discovery,Conditional independence,Directed graphical causal models,non-Gaussian distribution,Non-linear models,Statistical independence,Structural Equation Models},
  language = {English}
}

@article{harrisPCAlgorithmNonparanormal,
  title = {{{PC Algorithm}} for {{Nonparanormal Graphical Models}}},
  author = {Harris, Naftali and Drton, Mathias},
  pages = {19},
  abstract = {The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the `Rank PC' algorithm works as well as the `Pearson PC' algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations.},
  file = {/Users/i500763/Zotero/storage/S4KQKNJS/Harris and Drton - PC Algorithm for Nonparanormal Graphical Models.pdf},
  language = {en}
}

@article{hazarikaSurveyMemoryManagement2019,
  title = {Survey on Memory Management Techniques in Heterogeneous Computing Systems},
  author = {Hazarika, Anakhi and Poddar, Soumyajit and Rahaman, Hafizur},
  year = {2019},
  month = dec,
  volume = {14},
  pages = {47--60},
  publisher = {{IET Digital Library}},
  issn = {1751-861X},
  doi = {10.1049/iet-cdt.2019.0092},
  abstract = {A major issue faced by data scientists today is how to scale up their processing infrastructure to meet the challenge of big data and high-performance computing (HPC) workloads. With today\&apos;s HPC domain, it is required to connect multiple graphics processing units (GPUs) to accomplish large-scale parallel computing along with CPUs. Data movement between the processor and on-chip or off-chip memory creates a major bottleneck in overall system performance. The CPU/GPU processes all the data on a computer\&apos;s memory and hence the speed of the data movement to/from memory and the size of the memory affect computer speed. During memory access by any processing element, the memory management unit (MMU) controls the data flow of the computer\&apos;s main memory and impacts the system performance and power. Change in dynamic random access memory (DRAM) architecture, integration of memory-centric hardware accelerator in the heterogeneous system and Processing-in-Memory (PIM) are the techniques adopted from all the available shared resource management techniques to maximise the system throughput. This survey study presents an analysis of various DRAM designs and their performances. The authors also focus on the architecture, functionality, and performance of different hardware accelerators and PIM systems to reduce memory access time. Some insights and potential directions toward enhancements to existing techniques are also discussed. The requirement of fast, reconfigurable, self-adaptive memory management schemes in the high-speed processing scenario motivates us to track the trend. An effective MMU handles memory protection, cache control and bus arbitration associated with the processors.},
  file = {/Users/i500763/Zotero/storage/MEF7YWVC/iet-cdt.2019.html},
  journal = {IET Computers \&amp; Digital Techniques},
  language = {en},
  number = {2}
}

@article{ibmpower9nputeamFunctionalityPerformanceNVLink2018,
  title = {Functionality and Performance of {{NVLink}} with {{IBM POWER9}} Processors},
  author = {{IBM POWER9 NPU team}},
  year = {2018},
  month = jul,
  volume = {62},
  pages = {9:1--9:10},
  issn = {0018-8646},
  doi = {10.1147/JRD.2018.2846978},
  abstract = {Heterogeneous computer systems with multiple types of processing elements (PEs) are becoming a popular design to optimize performance and efficiency for a wide variety of applications. Each part of an application can be executed on the PE for which it is best suited. In heterogeneous systems, communication, efficient data movement, and memory sharing across PEs are critical to execute an application across the different PEs while incurring minimal overhead for communication and synchronization. The IBM POWER9 processor supports the NVIDIA NVLink interface, a high-performance interconnect with many such capabilities. In the IBM Power System AC922, IBM POWER9 processors directly connect to multiple NVIDIA GPUs using NVLink. In this paper, we highlight the important functional and performance capabilities of NVLink with the POWER9 processor. These include high bandwidth, hardware cache coherence, fine-grained data movement, and hardware support for atomic operations across all PEs of a compute node. We also present an analysis of how these performance and functional capabilities of POWER9 processors and NVLink are expected to have significant impacts on performance and programmability across a variety of important applications, such as machine learning and domains within high-performance computing.},
  file = {/Users/i500763/Zotero/storage/2HWLQ8HC/IBM POWER9 NPU team - 2018 - Functionality and performance of NVLink with IBM P.pdf},
  journal = {IBM Journal of Research and Development},
  number = {4-5}
}

@misc{IntelIntrinsicsGuide,
  title = {{{Intel}}\textregistered{} {{Intrinsics Guide}}},
  file = {/Users/i500763/Zotero/storage/YGUK6ZLI/IntrinsicsGuide.html},
  howpublished = {https://software.intel.com/sites/landingpage/IntrinsicsGuide/}
}

@misc{IntelXeonGold,
  title = {{{Intel}}\textregistered{} {{Xeon}}\textregistered{} {{Gold}} 6148 {{Processor}} (27.{{5M Cache}}, 2.40 {{GHz}}) - {{Product Specifications}}},
  abstract = {Intel\textregistered{} Xeon\textregistered{} Gold 6148 Processor (27.5M Cache, 2.40 GHz) quick reference with specifications, features, and technologies.},
  file = {/Users/i500763/Zotero/storage/VGC2SWHJ/specifications.html},
  howpublished = {https://www.intel.com/content/www/us/en/products/sku/120489/intel-xeon-gold-6148-processor-27-5m-cache-2-40-ghz/specifications.html},
  journal = {Intel},
  language = {en}
}

@article{johnsonSuperScalarProcessorDesign,
  title = {Super-{{Scalar Processor Design}}},
  author = {Johnson, William M},
  pages = {146},
  abstract = {A super-scalar processor is one that is capable of sustaining an instruction-execution rate of more than one instruction per clock cycle. Maintaining this execution rate is primarily a problem of scheduling processor resources (such as functional units) for high utilrzation. A number of scheduling algorithms have been published, with wide-ranging claims of performance over the single-instruction issue of a scalar processor. However, a number of these claims are based on idealizations or on special-purpose applications.},
  file = {/Users/i500763/Zotero/storage/T6BB25T5/Johnson - Super-Scalar Processor Design.pdf},
  language = {en}
}

@article{kalischCausalStructureLearning2014,
  title = {Causal {{Structure Learning}} and {{Inference}}: {{A Selective Review}}},
  shorttitle = {Causal {{Structure Learning}} and {{Inference}}},
  author = {Kalisch, Markus and B{\"u}hlmann, Peter},
  year = {2014},
  month = jan,
  volume = {11},
  pages = {3--21},
  issn = {1684-3703},
  doi = {10.1080/16843703.2014.11673322},
  file = {/Users/i500763/Zotero/storage/NUX8AFL3/Kalisch and BÃ¼hlmann - 2014 - Causal Structure Learning and Inference A Selecti.pdf},
  journal = {Quality Technology \& Quantitative Management},
  language = {en},
  number = {1}
}

@article{kalischEstimatingHighDimensionalDirected2007,
  title = {Estimating {{High}}-{{Dimensional Directed Acyclic Graphs}} with the {{PC}}-{{Algorithm}}},
  author = {Kalisch, Markus},
  year = {2007},
  month = may,
  volume = {8},
  pages = {613--636},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  abstract = {We consider the PC-algorithm (Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efficiency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(na) for any 0 {$<$} a {$<$} {$\infty$}. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data.},
  file = {/Users/i500763/Zotero/storage/9Y2DWMDP/Kalisch - Estimating High-Dimensional Directed Acyclic Graph.pdf},
  journal = {J. Mach. Learn. Res.},
  language = {en}
}

@article{kalischUnderstandingHumanFunctioning2010,
  title = {Understanding Human Functioning Using Graphical Models},
  author = {Kalisch, Markus and Fellinghauer, Bernd AG and Grill, Eva and Maathuis, Marloes H. and Mansmann, Ulrich and B{\"u}hlmann, Peter and Stucki, Gerold},
  year = {2010},
  month = feb,
  volume = {10},
  pages = {14},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-10-14},
  abstract = {Functioning and disability are universal human experiences. However, our current understanding of functioning from a comprehensive perspective is limited. The development of the International Classification of Functioning, Disability and Health (ICF) on the one hand and recent developments in graphical modeling on the other hand might be combined and open the door to a more comprehensive understanding of human functioning. The objective of our paper therefore is to explore how graphical models can be used in the study of ICF data for a range of applications.},
  file = {/Users/i500763/Zotero/storage/YG52YM3K/Kalisch et al. - 2010 - Understanding human functioning using graphical mo.pdf;/Users/i500763/Zotero/storage/KKSJ4WPF/1471-2288-10-14.html},
  journal = {BMC Medical Research Methodology},
  keywords = {Causal Effect,Dependence Structure,Dimension Reduction,Directed Acyclic Graph,Graphical Model},
  number = {1}
}

@article{khokharHeterogeneousComputingChallenges1993,
  title = {Heterogeneous Computing: Challenges and Opportunities},
  shorttitle = {Heterogeneous Computing},
  author = {Khokhar, A.A. and Prasanna, V.K. and Shaaban, M.E. and Wang, C.-L.},
  year = {1993},
  month = jun,
  volume = {26},
  pages = {18--27},
  issn = {0018-9162},
  doi = {10.1109/2.214439},
  file = {/Users/i500763/Zotero/storage/E9WE8JWV/Khokhar et al. - 1993 - Heterogeneous computing challenges and opportunit.pdf},
  journal = {Computer},
  language = {en},
  number = {6}
}

@article{kimBatchAwareUnifiedMemory,
  title = {Batch-{{Aware Unified Memory Management}} in {{GPUs}} for {{Irregular Workloads}}},
  author = {Kim, Hyojong and Sim, Jaewoong and Gera, Prasun and Hadidi, Ramyad and Kim, Hyesoon},
  pages = {14},
  abstract = {While unified virtual memory and demand paging in modern GPUs provide convenient abstractions to programmers for working with large-scale applications, they come at a significant performance cost. We provide the first comprehensive analysis of major inefficiencies that arise in page fault handling mechanisms employed in modern GPUs. To amortize the high costs in fault handling, the GPU runtime processes a large number of GPU page faults together. We observe that this batched processing of page faults introduces large-scale serialization that greatly hurts the GPU's execution throughput. We show real machine measurements that corroborate our findings. Our goal is to mitigate these inefficiencies and enable efficient demand paging for GPUs. To this end, we propose a GPU runtime software and hardware solution that (1) increases the batch size (i.e., the number of page faults handled together), thereby amortizing the GPU runtime fault handling time, and reduces the number of batches by supporting CPU-like thread block context switching, and (2) takes page eviction off the critical path with no hardware changes by overlapping evictions with CPU-to-GPU page migrations. Our evaluation demonstrates that the proposed solution provides an average speedup of 2x over the state-of-the-art page prefetching. We show that our solution increases the batch size by 2.27x and reduces the total number of batches by 51\% on average. We also show that the average batch processing time is reduced by 27\%.},
  file = {/Users/i500763/Zotero/storage/7J6JC7E3/Kim et al. - Batch-Aware Unified Memory Management in GPUs for .pdf},
  language = {en}
}

@incollection{kopetzRealTimeScheduling1997,
  title = {Real-{{Time Scheduling}}},
  booktitle = {Real-{{Time Systems}}: {{Design Principles}} for {{Distributed Embedded Applications}}},
  editor = {Kopetz, Hermann},
  year = {1997},
  pages = {227--243},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/0-306-47055-1_11},
  abstract = {OverviewMany thousands of research papers have been written about how to schedule a set of tasks in a system with a limited amount of resources such that all tasks will meet their deadlines. This chapter tries to summarize some important results that are relevant to the designer of real-time systems. The chapter starts by introducing the notion of a schedulability test to determine whether a given task set is schedulable or not. It distinguishes between a sufficient, an exact, and a necessary schedulability test, A scheduling algorithm is effective if it will find a schedule whenever there is a solution. The adversary argument shows that in the general case it is not possible to design an effective on-line scheduling algorithm.Section 11.3 covers the topic of dynamic scheduling. It starts with looking at the problem of scheduling a set of independent tasks by the rate-monotonic algorithm. Next, the problem of scheduling a set of dependent tasks is investigated. After the kernelized monitor, the priority-ceiling protocol is discussed and a schedulability test for the priority ceiling protocol is presented. Finally, the scheduling problem in distributed systems is touched.The final section elaborates on static scheduling. The concept of the schedule period is introduced and an example of a simple search tree that covers a schedule period is given. A heuristic algorithm has to examine the search tree to find a feasible schedule. If it finds one, the solution can be considered a constructive schedulability test. The flexibility of static schedules can be increased by introducing a periodic server task to service sporadic requests. Finally, the topic of mode changes to adapt the temporal control structure even further is discussed.},
  isbn = {978-0-306-47055-4},
  keywords = {Critical Section,Periodic Task,Schedule Problem,Sporadic Task,Static Schedule},
  language = {en},
  series = {The {{International Series}} in {{Engineering}} and {{Computer Science}}}
}

@article{kumaigorodskialexanderFastCSVLoading2021,
  title = {Fast {{CSV Loading Using GPUs}} and {{RDMA}} for {{In}}-{{Memory Data Processing}}},
  author = {Kumaigorodski, Alexander and Lutz, Clemens and Markl, Volker},
  year = {2021},
  publisher = {{Gesellschaft f\"ur Informatik, Bonn}},
  issn = {1617-5468},
  doi = {10.18420/BTW2021-01},
  abstract = {Comma-separated values (CSV) is a widely-used format for data exchange. Due to the format's prevalence, virtually all industrial-strength database systems and stream processing frameworks support importing CSV input.},
  file = {/Users/i500763/Zotero/storage/ZEWJAZSZ/Kumaigorodski, Alexander et al. - 2021 - Fast CSV Loading Using GPUs and RDMA for In-Memory.pdf},
  isbn = {9783885797050},
  keywords = {CSV,CUDA,GPU,InfiniBand,Parsing,RDMA},
  language = {en}
}

@article{kwokStaticSchedulingAlgorithms1999,
  title = {Static Scheduling Algorithms for Allocating Directed Task Graphs to Multiprocessors},
  author = {Kwok, Yu-Kwong and Ahmad, Ishfaq},
  year = {1999},
  month = dec,
  volume = {31},
  pages = {406--471},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/344588.344618},
  abstract = {Static scheduling of a program represented by a directed task graph on a multiprocessor system to minimize the program completion time is a well-known problem in parallel processing. Since finding an optimal schedule is an NP-complete problem in general, researchers have resorted to devising efficient heuristics. A plethora of heuristics have been proposed based on a wide spectrum of techniques, including branch-and-bound, integer-programming, searching, graph-theory, randomization, genetic algorithms, and evolutionary methods. The objective of this survey is to describe various scheduling algorithms and their functionalities in a contrasting fashion as well as examine their relative merits in terms of performance and time-complexity. Since these algorithms are based on diverse   assumptions, they differ in their functionalities, and hence are difficult to describe in a unified context. We propose a taxonomy that classifies these algorithms into different categories. We consider 27 scheduling algorithms, with each algorithm explained through an easy-to-understand description followed by an illustrative example to demonstrate its operation. We also outline some of the novel and promising optimization approaches and current research trends in the area. Finally, we give an overview of the software tools that provide scheduling/mapping functionalities.},
  file = {/Users/i500763/Zotero/storage/PG74KN96/Kwok and Ahmad - 1999 - Static scheduling algorithms for allocating direct.pdf},
  journal = {ACM Computing Surveys},
  language = {en},
  number = {4}
}

@article{lawsonBasicLinearAlgebra1979,
  title = {Basic {{Linear Algebra Subprograms}} for {{Fortran Usage}}},
  author = {Lawson, C. L. and Hanson, R. J. and Kincaid, D. R. and Krogh, F. T.},
  year = {1979},
  month = sep,
  volume = {5},
  pages = {308--323},
  issn = {0098-3500, 1557-7295},
  doi = {10.1145/355841.355847},
  file = {/Users/i500763/Zotero/storage/XY6PQM4T/Lawson et al. - 1979 - Basic Linear Algebra Subprograms for Fortran Usage.pdf},
  journal = {ACM Transactions on Mathematical Software},
  language = {en},
  number = {3}
}

@article{lecunDeepLearning2015,
  title = {Deep {{Learning}}},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  volume = {521},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  file = {/Users/i500763/Zotero/storage/WKHHW7M6/LeCun et al. - 2015 - Deep learning.pdf},
  journal = {Nature},
  language = {en},
  number = {7553}
}

@article{leFastPCAlgorithm2019,
  title = {A {{Fast PC Algorithm}} for {{High Dimensional Causal Discovery}} with {{Multi}}-{{Core PCs}}},
  author = {Le, Thuc Duy and Hoang, Tao and Li, Jiuyong and Liu, Lin and Liu, Huawen},
  year = {2019},
  month = sep,
  volume = {16},
  pages = {1483--1495},
  issn = {1545-5963, 1557-9964, 2374-0043},
  doi = {10.1109/TCBB.2016.2591526},
  abstract = {Discovering causal relationships from observational data is a crucial problem and it has applications in many research areas. The PC algorithm is the state-of-the-art constraint based method for causal discovery. However, runtime of the PC algorithm, in the worst-case, is exponential to the number of nodes (variables), and thus it is inefficient when being applied to high dimensional data, e.g. gene expression datasets. On another note, the advancement of computer hardware in the last decade has resulted in the widespread availability of multi-core personal computers. There is a significant motivation for designing a parallelised PC algorithm that is suitable for personal computers and does not require end users' parallel computing knowledge beyond their competency in using the PC algorithm. In this paper, we develop parallel-PC, a fast and memory efficient PC algorithm using the parallel computing technique.},
  archiveprefix = {arXiv},
  eprint = {1502.02454},
  eprinttype = {arxiv},
  file = {/Users/i500763/Zotero/storage/VRWHYNTW/Le et al. - 2019 - A fast PC algorithm for high dimensional causal di.pdf},
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  number = {5}
}

@article{leParallelPCPackageEfficient2015,
  title = {{{ParallelPC}}: An {{R}} Package for Efficient Constraint Based Causal Exploration},
  shorttitle = {{{ParallelPC}}},
  author = {Le, Thuc Duy and Hoang, Tao and Li, Jiuyong and Liu, Lin and Hu, Shu},
  year = {2015},
  month = oct,
  abstract = {Discovering causal relationships from data is the ultimate goal of many research areas. Constraint based causal exploration algorithms, such as PC, FCI, RFCI, PC-simple, IDA and Joint-IDA have achieved significant progress and have many applications. A common problem with these methods is the high computational complexity, which hinders their applications in real world high dimensional datasets, e.g gene expression datasets. In this paper, we present an R package, ParallelPC, that includes the parallelised versions of these causal exploration algorithms. The parallelised algorithms help speed up the procedure of experimenting big datasets and reduce the memory used when running the algorithms. The package is not only suitable for super-computers or clusters, but also convenient for researchers using personal computers with multi core CPUs. Our experiment results on real world datasets show that using the parallelised algorithms it is now practical to explore causal relationships in high dimensional datasets with thousands of variables in a single multicore computer. ParallelPC is available in CRAN repository at https://cran.rproject.org/web/packages/ParallelPC/index.html.},
  archiveprefix = {arXiv},
  eprint = {1510.03042},
  eprinttype = {arxiv},
  file = {/Users/i500763/Zotero/storage/A6LC8BQL/Le et al. - 2015 - ParallelPC an R package for efficient constraint .pdf},
  journal = {arXiv:1510.03042 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@inproceedings{leParallelPCPackageEfficient2018,
  title = {{{ParallelPC}}: {{An R Package}} for {{Efficient Causal Exploration}} in {{Genomic Data}}},
  shorttitle = {{{ParallelPC}}},
  booktitle = {Trends and {{Applications}} in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Le, Thuc Duy and Xu, Taosheng and Liu, Lin and Shu, Hu and Hoang, Tao and Li, Jiuyong},
  editor = {Ganji, Mohadeseh and Rashidi, Lida and Fung, Benjamin C. M. and Wang, Can},
  year = {2018},
  pages = {207--218},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-04503-6_22},
  abstract = {Discovering causal relationships from genomic data is the ultimate goal in gene regulation research. Constraint based causal exploration algorithms, such as PC, FCI, RFCI, PC-simple, IDA and Joint-IDA have achieved significant progress and have many applications. However, their applications in bioinformatics are still limited due to their high computational complexity. In this paper, we present an R package, ParallelPC, that includes the parallelised versions of these causal exploration algorithms and 12 different conditional independence tests for each. The parallelised algorithms help speed up the procedure of experimenting large biological datasets and reduce the memory used when running the algorithms. Our experiment results on a real gene expression dataset show that using the parallelised algorithms it is now practical to explore causal relationships in high dimensional datasets with thousands of variables in a personal multicore computer. We present some typical applications in bioinformatics using different algorithms in ParallelPC. ParallelPC is available in CRAN repository at https://cran.r-project.org/web/packages/ParallelPC/index.html.},
  file = {/Users/i500763/Zotero/storage/VWAARB7N/Le et al. - 2018 - ParallelPC An R Package for Efficient Causal Expl.pdf},
  isbn = {978-3-030-04503-6},
  keywords = {Bayesian networks,Causality discovery,Constraint-based methods,Parallel computing},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{letzWorkStealingScheduler2010,
  title = {Work {{Stealing Scheduler}} for {{Automatic Parallelization}} in {{Faust}}},
  booktitle = {Linux {{Audio Conference}}},
  author = {Letz, St{\'e}phane and Orlarey, Yann and Fober, Dominique},
  editor = {{LAC}},
  year = {2010},
  address = {{Utrecht, Netherlands}},
  abstract = {Faust 0.9.10 introduces an alternative to OpenMP based parallel code generation using a Work Steal- ing Scheduler and explicit management of worker threads. This paper explains the new option and presents some benchmarks.},
  file = {/Users/i500763/Zotero/storage/MJFZKHZ6/Letz et al. - 2010 - Work Stealing Scheduler for Automatic Parallelizat.pdf},
  keywords = {compiler,dataflow,functional,processing,programming,real-time,signal}
}

@article{liEvaluatingModernGPU2020,
  title = {Evaluating {{Modern GPU Interconnect}}: {{PCIe}}, {{NVLink}}, {{NV}}-{{SLI}}, {{NVSwitch}} and {{GPUDirect}}},
  shorttitle = {Evaluating {{Modern GPU Interconnect}}},
  author = {Li, Ang and Song, Shuaiwen Leon and Chen, Jieyang and Li, Jiajia and Liu, Xu and Tallent, Nathan and Barker, Kevin},
  year = {2020},
  month = jan,
  volume = {31},
  pages = {94--110},
  issn = {1045-9219, 1558-2183, 2161-9883},
  doi = {10.1109/TPDS.2019.2928289},
  abstract = {High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.},
  archiveprefix = {arXiv},
  eprint = {1903.04611},
  eprinttype = {arxiv},
  file = {/Users/i500763/Zotero/storage/GBGYBK3R/Li et al. - 2020 - Evaluating Modern GPU Interconnect PCIe, NVLink, .pdf;/Users/i500763/Zotero/storage/QILL3XEQ/1903.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Hardware Architecture,Computer Science - Networking and Internet Architecture,Computer Science - Performance},
  number = {1}
}

@inproceedings{liLocalityLoopScheduling1993,
  title = {Locality {{And Loop Scheduling On Numa Multiprocessors}}},
  booktitle = {In {{Proceedings}} of the 1993 {{International Conference}} on {{Parallel Processing}}},
  author = {Li, Hui and Sudarsan, Hui Li and Stumm, Michael and Sevcik, Kenneth C.},
  year = {1993},
  pages = {140--147},
  publisher = {{CRC Press, Inc}},
  abstract = {An important issue in the parallel execution of loops is how to partition and schedule the loops onto the available processors. While most existing dynamic scheduling algorithms manage load imbalances well, they fail to take locality into account and therefore perform poorly on parallel systems with non-uniform memory access times. In this paper, we propose a new loop scheduling algorithm, Locality-based Dynamic Scheduling (LDS), that exploits locality, and dynamically balances the load.  Key Words: Locality, Loop Scheduling, NUMA Multiprocessors, Data Partitioning, Locality-based Dynamic Scheduling.  1 Introduction  Loops are a major source of parallelism for todays parallelizing compilers. An important issue in the parallel execution of loops is how to partition and schedule the loops onto the available processors. A number of algorithms have been proposed for this purpose. For example, static scheduling algorithms such as block, cyclic, and block-cyclic scheduling, partition the loo...},
  file = {/Users/i500763/Zotero/storage/WIDTVP7L/Li et al. - 1993 - Locality And Loop Scheduling On Numa Multiprocesso.pdf;/Users/i500763/Zotero/storage/GM6MMFYB/download.html}
}

@misc{LinuxIBMPower,
  title = {Linux on {{IBM Power Systems}} Application Porting and Tuning Guide},
  abstract = {This is a general tuning and optimization guide for Power users to enable their applications on Power. The guide provides resources and techniques necessary for supporting and creating accelerated solutions on Power.},
  file = {/Users/i500763/Zotero/storage/QALXJJPU/migrate-app-on-lop.html},
  howpublished = {https://developer.ibm.com/technologies/linux/tutorials/migrate-app-on-lop/},
  journal = {IBM Developer},
  language = {en-US}
}

@article{liuEstimationAccuracyExecution2016,
  title = {Estimation {{Accuracy}} on {{Execution Time}} of {{Run}}-{{Time Tasks}} in a {{Heterogeneous Distributed Environment}}},
  author = {Liu, Qi and Cai, Weidong and Jin, Dandan and Shen, Jian and Fu, Zhangjie and Liu, Xiaodong and Linge, Nigel},
  year = {2016},
  month = aug,
  volume = {16},
  pages = {1386},
  issn = {1424-8220},
  doi = {10.3390/s16091386},
  abstract = {Distributed Computing has achieved tremendous development since cloud computing was proposed in 2006, and played a vital role promoting rapid growth of data collecting and analysis models, e.g., Internet of things, Cyber-Physical Systems, Big Data Analytics, etc. Hadoop has become a data convergence platform for sensor networks. As one of the core components, MapReduce facilitates allocating, processing and mining of collected large-scale data, where speculative execution strategies help solve straggler problems. However, there is still no efficient solution for accurate estimation on execution time of run-time tasks, which can affect task allocation and distribution in MapReduce. In this paper, task execution data have been collected and employed for the estimation. A two-phase regression (TPR) method is proposed to predict the finishing time of each task accurately. Detailed data of each task have drawn interests with detailed analysis report being made. According to the results, the prediction accuracy of concurrent tasks' execution time can be improved, in particular for some regular jobs.},
  file = {/Users/i500763/Zotero/storage/76249HKV/Liu et al. - 2016 - Estimation Accuracy on Execution Time of Run-Time .pdf},
  journal = {Sensors},
  language = {en},
  number = {9}
}

@article{madsenParallelAlgorithmBayesian2017,
  title = {A Parallel Algorithm for {{Bayesian}} Network Structure Learning from Large Data Sets},
  author = {Madsen, Anders L. and Jensen, Frank and Salmer{\'o}n, Antonio and Langseth, Helge and Nielsen, Thomas D.},
  year = {2017},
  month = feb,
  volume = {117},
  pages = {46--55},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2016.07.031},
  abstract = {This paper considers a parallel algorithm for Bayesian network structure learning from large data sets. The parallel algorithm is a variant of the well known PC algorithm. The PC algorithm is a constraint-based algorithm consisting of five steps where the first step is to perform a set of (conditional) independence tests while the remaining four steps relate to identifying the structure of the Bayesian network using the results of the (conditional) independence tests. In this paper, we describe a new approach to parallelization of the (conditional) independence testing as experiments illustrate that this is by far the most time consuming step. The proposed parallel PC algorithm is evaluated on data sets generated at random from five different real-world Bayesian networks. The algorithm is also compared empirically with a process-based approach where each process manages a subset of the data over all the variables on the Bayesian network. The results demonstrate that significant time performance improvements are possible using both approaches.},
  file = {/Users/i500763/Zotero/storage/4ARLSUQS/Madsen et al. - 2017 - A parallel algorithm for Bayesian network structur.pdf},
  journal = {Knowledge-Based Systems},
  keywords = {Bayesian network,Parallelization,PC algorithm},
  number = {C}
}

@inproceedings{madsenParallelisationPCAlgorithm2015,
  title = {Parallelisation of the {{PC Algorithm}}},
  booktitle = {Advances in {{Artificial Intelligence}}},
  author = {Madsen, Anders L. and Jensen, Frank and Salmer{\'o}n, Antonio and Langseth, Helge and Nielsen, Thomas D.},
  editor = {Puerta, Jos{\'e} M. and G{\'a}mez, Jos{\'e} A. and Dorronsoro, Bernabe and Barrenechea, Edurne and Troncoso, Alicia and Baruque, Bruno and Galar, Mikel},
  year = {2015},
  pages = {14--24},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24598-0_2},
  abstract = {This paper describes a parallel version of the PC algorithm for learning the structure of a Bayesian network from data. The PC algorithm is a constraint-based algorithm consisting of five steps where the first step is to perform a set of (conditional) independence tests while the remaining four steps relate to identifying the structure of the Bayesian network using the results of the (conditional) independence tests. In this paper, we describe a new approach to parallelisation of the (conditional) independence testing as experiments illustrate that this is by far the most time consuming step. The proposed parallel PC algorithm is evaluated on data sets generated at random from five different real-world Bayesian networks. The results demonstrate that significant time performance improvements are possible using the proposed algorithm.},
  file = {/Users/i500763/Zotero/storage/W55YJJPV/Madsen et al. - 2015 - Parallelisation of the PC Algorithm.pdf},
  isbn = {978-3-319-24598-0},
  keywords = {Bayesian network,Parallelisation,PC algorithm},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{mattheisWorkStealingStrategies2012,
  title = {Work {{Stealing Strategies}} for {{Parallel Stream Processing}} in {{Soft Real}}-{{Time Systems}}},
  author = {Mattheis, Sebastian and Schuele, Tobias and Raabe, Andreas and Henties, Thomas and Gleim, Urs},
  year = {2012},
  month = feb,
  volume = {7179},
  pages = {172--183},
  doi = {10.1007/978-3-642-28293-5_15},
  abstract = {Work stealing has proven to be an efficient technique for scheduling parallel computations. In its basic form, however, work stealing is not suitable for real-time applications, since the latency of a task is hardly predictable. In this paper, we propose a number of variants and extensions of work stealing suitable for stream processing applications. Such applications are frequently encountered in embedded systems, which often have to obey real-time constraints. Moreover, we give bounds on the maximum latency for certain stealing strategies. Our experimental results show a significant reduction of the latency using these strategies.},
  file = {/Users/i500763/Zotero/storage/CZX88YBV/Mattheis et al. - 2012 - Work Stealing Strategies for Parallel Stream Proce.pdf},
  isbn = {978-3-642-28292-8}
}

@article{mattsonPatternsParallelProgramming2004,
  title = {Patterns for {{Parallel Programming}}},
  author = {Mattson, Tim and Sanders, Beverly and Massingill, Berna},
  year = {2004},
  month = sep,
  issn = {0321228111},
  file = {/Users/i500763/Zotero/storage/V58FAWMX/Mattson et al. - 2004 - Patterns for Parallel Programming.pdf}
}

@misc{MemoryManagement,
  title = {Memory {{Management}}},
  file = {/Users/i500763/Zotero/storage/SU9A277C/group__CUDART__MEMORY.html},
  howpublished = {http://docs.nvidia.com/cuda/cuda-runtime-api/index.html},
  language = {en-us},
  type = {{{cppModule}}}
}

@article{mittalSurveyCPUGPUHeterogeneous2015,
  title = {A {{Survey}} of {{CPU}}-{{GPU Heterogeneous Computing Techniques}}},
  author = {Mittal, Sparsh and Vetter, Jeffrey S.},
  year = {2015},
  month = jul,
  volume = {47},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/2788396},
  abstract = {As both CPUs and GPUs become employed in a wide range of applications, it has been acknowledged that both of these Processing Units (PUs) have their unique features and strengths and hence, CPU-GPU collaboration is inevitable to achieve high-performance computing. This has motivated a significant amount of research on heterogeneous computing techniques, along with the design of CPU-GPU fused chips and petascale heterogeneous supercomputers. In this article, we survey Heterogeneous Computing Techniques (HCTs) such as workload partitioning that enable utilizing both CPUs and GPUs to improve performance and/or energy efficiency. We review heterogeneous computing approaches at runtime, algorithm, programming, compiler, and application levels. Further, we review both discrete and fused CPU-GPU systems and discuss benchmark suites designed for evaluating Heterogeneous Computing Systems (HCSs). We believe that this article will provide insights into the workings and scope of applications of HCTs to researchers and motivate them to further harness the computational powers of CPUs and GPUs to achieve the goal of exascale performance.},
  file = {/Users/i500763/Zotero/storage/UEXNAGC8/Mittal and Vetter - 2015 - A Survey of CPU-GPU Heterogeneous Computing Techni.pdf},
  journal = {ACM Computing Surveys},
  language = {en},
  number = {4}
}

@article{momcilovicDynamicLoadBalancing2014,
  title = {Dynamic {{Load Balancing}} for {{Real}}-{{Time Video Encoding}} on {{Heterogeneous CPU}}+{{GPU Systems}}},
  author = {Momcilovic, Svetislav and Ilic, Aleksandar and Sousa, Leonel},
  year = {2014},
  volume = {16},
  pages = {108--121},
  doi = {10.1109/TMM.2013.2284892},
  abstract = {The high computational demands and overall encoding complexity make the processing of high definition video sequences hard to be achieved in real-time. In this manuscript, we target an efficient parallelization and RD performance analysis of H.264/AVC inter-loop modules and their collaborative execution in hybrid multi-core CPU and multi-GPU systems. The proposed dynamic load balancing algorithm allows efficient and concurrent video encoding across several heterogeneous devices by relying on realistic run-time performance modeling and module-device execution affinities when distributing the computations. Due to an online adjustment of load balancing decisions, this approach is also self-adaptable to different execution scenarios. Experimental results show the proposed algorithm's ability to achieve real-time encoding for different resolutions of high-definition sequences in various heterogeneous platforms. Speed-up values of up to 2.6 were obtained when compared to the video inter-loop encoding on a single GPU device, and up to 8.5 when compared to a highly optimized multi-core CPU execution. Moreover, the proposed algorithm also provides an automatic tuning of the encoding parameters, in order to meet strict encoding constraints.},
  file = {/Users/i500763/Zotero/storage/9FYWWQ8D/Momcilovic et al. - 2014 - Dynamic Load Balancing for Real-Time Video Encodin.pdf},
  journal = {IEEE Transactions on Multimedia},
  language = {en},
  number = {1}
}

@article{mooreCrammingMoreComponents1965,
  title = {Cramming More Components onto Integrated Circuits},
  author = {Moore, Gordon E},
  year = {1965},
  volume = {38},
  pages = {4},
  file = {/Users/i500763/Zotero/storage/MMW9A2PY/Moore - 1965 - Cramming more components onto integrated circuits.pdf},
  language = {en},
  number = {8}
}

@article{nagarajanFunctionalRelationshipsGenes2010,
  title = {Functional Relationships between Genes Associated with Differentiation Potential of Aged Myogenic Progenitors},
  author = {Nagarajan, Radhakrishnan and Datta, Sujay and Scutari, Marco and Beggs, Marjorie L. and Nolen, Greg T. and Peterson, Charlotte A.},
  year = {2010},
  volume = {1},
  pages = {21},
  issn = {1664-042X},
  doi = {10.3389/fphys.2010.00021},
  abstract = {Aging is accompanied by considerable heterogeneity with possible co-expression of differentiation pathways. The present study investigates the interplay between crucial myogenic, adipogenic, and Wnt-related genes orchestrating aged myogenic progenitor differentiation (AMPD) using clonal gene expression profiling in conjunction with Bayesian structure learning (BSL) techniques. The expression of three myogenic regulatory factor genes (Myogenin, Myf-5, MyoD1), four genes involved in regulating adipogenic potential (C/EBP{$\alpha$}, DDIT3, FoxC2, PPAR{$\gamma$}), and two genes in the Wnt signaling pathway (Lrp5, Wnt5a) known to influence both differentiation programs were determined across 34 clones by quantitative reverse transcriptase polymerase chain reaction (qRT-PCR). Three control genes were used for normalization of the clonal expression data (18S, GAPDH, and B2M). Constraint-based BSL techniques, namely (a) PC Algorithm, (b) Grow-shrink (GS) algorithm, and (c) Incremental Association Markov Blanket (IAMB) were used to model the functional relationships (FRs) in the form of acyclic networks from the clonal expression profiles. A novel resampling approach that obviates the need for a user-defined confidence threshold is proposed to identify statistically significant FRs at small sample sizes. Interestingly, the resulting acyclic network consisted of FRs corresponding to myogenic, adipogenic, Wnt-related genes and their interaction. A significant number of these FRs were robust to normalization across the three house-keeping genes and the choice of the BSL technique. The results presented elucidate the delicate balance between differentiation pathways (i.e., myogenic as well as adipogenic) and possible cross-talk between pathways in AMPD.},
  file = {/Users/i500763/Zotero/storage/8ZA4NM5B/Nagarajan et al. - 2010 - Functional relationships between genes associated .pdf},
  journal = {Frontiers in Physiology},
  keywords = {aged myogenic progenitor differentiation,Bayesian structure learning,functional relationships},
  language = {eng},
  pmcid = {PMC3059939},
  pmid = {21423363}
}

@book{neapolitanLearningBayesianNetworks2003,
  title = {Learning {{Bayesian Networks}}},
  author = {Neapolitan, Richard},
  year = {2003},
  month = jan,
  doi = {10.1145/1327942.1327961},
  abstract = {An analytical framework for using powerlaw theory to estimate market size for niche products and consumer groups.},
  file = {/Users/i500763/Zotero/storage/H5R357WD/Neapolitan - 2003 - Learning Bayesian Networks.pdf},
  isbn = {978-0-12-370477-1}
}

@article{nemirovskyMultithreadingArchitecture2013,
  title = {Multithreading {{Architecture}}},
  author = {Nemirovsky, Mario and Tullsen, Dean M.},
  year = {2013},
  month = jan,
  volume = {8},
  pages = {1--109},
  publisher = {{Morgan \& Claypool Publishers}},
  issn = {1935-3235},
  doi = {10.2200/S00458ED1V01Y201212CAC021},
  file = {/Users/i500763/Zotero/storage/WLTY7MD9/Nemirovsky and Tullsen - 2013 - Multithreading Architecture.pdf;/Users/i500763/Zotero/storage/69FPGNZT/S00458ED1V01Y201212CAC021.html},
  journal = {Synthesis Lectures on Computer Architecture},
  number = {1}
}

@inproceedings{nguyenMrPCCausalStructure2020,
  title = {{{MrPC}}: {{Causal Structure Learning}} in {{Distributed Systems}}},
  shorttitle = {{{MrPC}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Nguyen, Thin and Nguyen, Duc Thanh and Le, Thuc Duy and Venkatesh, Svetha},
  editor = {Yang, Haiqin and Pasupa, Kitsuchart and Leung, Andrew Chi-Sing and Kwok, James T. and Chan, Jonathan H. and King, Irwin},
  year = {2020},
  pages = {87--94},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-63820-7_10},
  abstract = {PC algorithm (PC) \textendash{} named after its authors, Peter and Clark \textendash{} is an advanced constraint based method for learning causal structures. However, it is a time-consuming algorithm since the number of independence tests is exponential to the number of considered variables. Attempts to parallelise PC have been studied intensively, for example, by distributing the tests to all computing cores in a single computer. However, no effort has been made to speed up PC through parallelising the conditional independence tests into a cluster of computers. In this work, we propose MrPC, a robust and efficient PC algorithm, to accelerate PC to serve causal discovery in distributed systems. Alongside with MrPC, we also propose a novel manner to model non-linear causal relationships in gene regulatory data using kernel functions. We evaluate our method and its variants in the task of building gene regulatory networks. Experimental results on benchmark datasets show that the proposed MrPCgains up to seven times faster than sequential PC implementation. In addition, kernel functions outperform conventional linear causal modelling approach across different datasets.},
  file = {/Users/i500763/Zotero/storage/X8VW2T8J/Nguyen et al. - 2020 - MrPC Causal Structure Learning in Distributed Sys.pdf},
  isbn = {978-3-030-63820-7},
  keywords = {Causal structure learning,Causality,Distributed systems,Explainable AI},
  language = {en},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@article{nickollsGPUComputingEra2010,
  title = {The {{GPU Computing Era}}},
  author = {Nickolls, John and Dally, William J.},
  year = {2010},
  month = mar,
  volume = {30},
  pages = {56--69},
  issn = {1937-4143},
  doi = {10.1109/MM.2010.41},
  abstract = {GPU computing is at a tipping point, becoming more widely used in demanding consumer applications and high-performance computing. This article describes the rapid evolution of GPU architectures-from graphics processors to massively parallel many-core multiprocessors, recent developments in GPU computing architectures, and how the enthusiastic adoption of CPU+GPU coprocessing is accelerating parallel applications.},
  file = {/Users/i500763/Zotero/storage/KTZ2WW3R/5446251.html},
  journal = {IEEE Micro},
  keywords = {Acceleration,Computer applications,Computer architecture,Concurrent computing,CUDA,Fermi GPU architecture,GPU computing,GPU coprocessing,Graphics processing unit,heterogeneous CPU+,Layout,NVIDIA.,Parallel processing,Pervasive computing,Pipelines,scalable parallel computing,Tesla GPU architecture},
  number = {2}
}

@article{nickollsScalableParallelProgramming2008,
  title = {Scalable {{Parallel Programming}} with {{CUDA}}: {{Is CUDA}} the Parallel Programming Model That Application Developers Have Been Waiting For?},
  shorttitle = {Scalable {{Parallel Programming}} with {{CUDA}}},
  author = {Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
  year = {2008},
  month = mar,
  volume = {6},
  pages = {40--53},
  issn = {1542-7730},
  doi = {10.1145/1365490.1365500},
  abstract = {The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems. Furthermore, their parallelism continues to scale with Moore's law. The challenge is to develop mainstream application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3D graphics applications transparently scale their parallelism to manycore GPUs with widely varying numbers of cores.},
  file = {/Users/i500763/Zotero/storage/2BVWRZA7/Nickolls et al. - 2008 - Scalable Parallel Programming with CUDA Is CUDA t.pdf},
  journal = {Queue},
  number = {2}
}

@misc{NVIDIALibcudacxx2021,
  title = {{{NVIDIA}}/Libcudacxx},
  year = {2021},
  month = jun,
  abstract = {The C++ Standard Library for your entire system.},
  howpublished = {NVIDIA Corporation},
  keywords = {cpp,cpp11,cpp14,cpp17,cpp20,cpp23,cuda,cxx,cxx11,cxx14,cxx17,cxx20,cxx23,gpu,libcxx,llvm,nvidia,nvidia-hpc-sdk,standard,std}
}

@misc{NVIDIATESLAV1002017,
  title = {{{NVIDIA TESLA V100 GPU ARCHITECTURE}}},
  year = {2017},
  month = aug,
  publisher = {{Nvidia}},
  file = {/Users/i500763/Zotero/storage/MB9QSSQS/NVIDIA TESLA V100 GPU ARCHITECTURE.pdf}
}

@article{NVLink2021,
  title = {{{NVLink}}},
  year = {2021},
  month = may,
  abstract = {NVLink is a wire-based serial multi-lane near-range communications link developed by Nvidia. Unlike PCI Express, a device can consist of multiple NVLinks, and devices use mesh networking to communicate instead of a central hub. The protocol was first announced in March 2014 and uses a proprietary high-speed signaling interconnect (NVHS).},
  annotation = {Page Version ID: 1021388572},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {/Users/i500763/Zotero/storage/U9ME34A8/index.html},
  journal = {Wikipedia},
  language = {en}
}

@inproceedings{olmedoDissectingCUDAScheduling2020,
  title = {Dissecting the {{CUDA}} Scheduling Hierarchy: A {{Performance}} and {{Predictability Perspective}}},
  shorttitle = {Dissecting the {{CUDA}} Scheduling Hierarchy},
  booktitle = {2020 {{IEEE Real}}-{{Time}} and {{Embedded Technology}} and {{Applications Symposium}} ({{RTAS}})},
  author = {Olmedo, Ignacio Sanudo and Capodieci, Nicola and Martinez, Jorge Luis and Marongiu, Andrea and Bertogna, Marko},
  year = {2020},
  month = apr,
  pages = {213--225},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/RTAS48715.2020.000-5},
  abstract = {Over the last few years, the ever-increasing use of Graphic Processing Units (GPUs) in safety-related domains has opened up many research problems in the real-time community. The closed and proprietary nature of the scheduling mechanisms deployed in NVIDIA GPUs, for instance, represents a major obstacle in deriving a proper schedulability analysis for latency-sensitive applications. Existing literature addresses these issues by either (i) providing simplified models for heterogeneous CPUGPU systems and their associated scheduling policies, or (ii) providing insights about these arbitration mechanisms obtained through reverse engineering. In this paper, we take one step further by correcting and consolidating previously published assumptions about the hierarchical scheduling policies of NVIDIA GPUs and their proprietary CUDA application programming interface. We also discuss how such mechanisms evolved with recently released GPU micro-architectures, and how such changes influence the scheduling models to be exploited by real-time system engineers.},
  file = {/Users/i500763/Zotero/storage/FYVDAGMS/1ky1eiFto0U.html},
  isbn = {978-1-72815-499-2},
  language = {English}
}

@misc{paiHowFermiThread2014,
  title = {How the {{Fermi Thread Block Scheduler Works}} ({{Illustrated}})},
  author = {Pai, Sreepathi},
  year = {2014},
  month = mar,
  file = {/Users/i500763/Zotero/storage/YLRWW7CT/fermi-tbs.html},
  howpublished = {https://www.cs.rochester.edu/\textasciitilde sree/fermi-tbs/fermi-tbs.html}
}

@book{pearlCausality2009,
  title = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  edition = {Second},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511803161},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.},
  file = {/Users/i500763/Zotero/storage/XIQLXIDH/Pearl - CAUSALITY MODELS, REASONING, AND INFERENCE.pdf;/Users/i500763/Zotero/storage/X6P5JZWL/B0046844FAE10CBF274D4ACBDAEB5F5B.html},
  isbn = {978-0-521-89560-6}
}

@article{pearlIntroductionCausalInference2010,
  title = {An {{Introduction}} to {{Causal Inference}}},
  author = {Pearl, Judea},
  year = {2010},
  volume = {6},
  doi = {doi:10.2202/1557-4679.1203},
  journal = {The International Journal of Biostatistics},
  number = {2}
}

@book{pearlProbabilisticReasoningIntelligent1988,
  title = {Probabilistic {{Reasoning}} in {{Intelligent Systems}}: {{Networks}} of {{Plausible Inference}}},
  shorttitle = {Probabilistic {{Reasoning}} in {{Intelligent Systems}}},
  author = {Pearl, Judea},
  year = {1988},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  abstract = {From the Publisher: Probabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertaintyand offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognitionin short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.},
  file = {/Users/i500763/Zotero/storage/E3W4LJZ8/Pearl - 1988 - Probabilistic Reasoning in Intelligent Systems Ne.pdf},
  isbn = {978-1-55860-479-7}
}

@incollection{pearlTheoryInferredCausation1995,
  title = {A Theory of Inferred Causation},
  booktitle = {Studies in {{Logic}} and the {{Foundations}} of {{Mathematics}}},
  author = {Pearl, Judea and Verma, Thomas S.},
  editor = {Prawitz, Dag and Skyrms, Brian and Westerst{\aa}hl, Dag},
  year = {1995},
  month = jan,
  volume = {134},
  pages = {789--811},
  publisher = {{Elsevier}},
  doi = {10.1016/S0049-237X(06)80074-1},
  abstract = {This chapter discusses the theory of inferred causation. The study of causation is central to the understanding of human reasoning. Inferences involving changing environments require causal theories that make formal distinctions between beliefs based on passive observations and those reflecting intervening actions. In applications such as diagnosis, qualitative physics, and plan recognition, a central task is that of finding a satisfactory explanation to a given set of observations, and the meaning of explanation is intimately related to the notion of causation. In some systems, causal ordering is defined as the ordering at which subsets of variables can be solved independently of others; in other systems, it follows the way a disturbance is propagated from one variable to others. An empirical semantics for causation is important for several reasons. The notion of causation is often associated with those of necessity and functional dependence; causal expressions often tolerate exceptions, primarily because of missing variables and coarse description. Temporal precedence is normally assumed essential for defining causation, and it is one of the most important clues that people use to distinguish causal from other types of associations.},
  file = {/Users/i500763/Zotero/storage/ZD9PAQI2/S0049237X06800741.html},
  language = {en},
  series = {Logic, {{Methodology}} and {{Philosophy}} of {{Science IX}}}
}

@article{perscheidIntegrativeGeneSelection2018,
  title = {Integrative {{Gene Selection}} on {{Gene Expression Data}}: {{Providing Biological Context}} to {{Traditional Approaches}}},
  shorttitle = {Integrative {{Gene Selection}} on {{Gene Expression Data}}},
  author = {Perscheid, Cindy and Grasnick, Bastien and Uflacker, Matthias},
  year = {2018},
  month = dec,
  volume = {16},
  issn = {1613-4516},
  doi = {10.1515/jib-2018-0064},
  abstract = {The advance of high-throughput RNA-Sequencing techniques enables researchers to analyze the complete gene activity in particular cells. From the insights of such analyses, researchers can identify disease-specific expression profiles, thus understand complex diseases like cancer, and eventually develop effective measures for diagnosis and treatment. The high dimensionality of gene expression data poses challenges to its computational analysis, which is addressed with measures of gene selection. Traditional gene selection approaches base their findings on statistical analyses of the actual expression levels, which implies several drawbacks when it comes to accurately identifying the underlying biological processes. In turn, integrative approaches include curated information on biological processes from external knowledge bases during gene selection, which promises to lead to better interpretability and improved predictive performance. Our work compares the performance of traditional and integrative gene selection approaches. Moreover, we propose a straightforward approach to integrate external knowledge with traditional gene selection approaches. We introduce a framework enabling the automatic external knowledge integration, gene selection, and evaluation. Evaluation results prove our framework to be a useful tool for evaluation and show that integration of external knowledge improves overall analysis results.},
  file = {/Users/i500763/Zotero/storage/F9EQFPSG/Perscheid et al. - 2018 - Integrative Gene Selection on Gene Expression Data.pdf},
  journal = {Journal of Integrative Bioinformatics},
  keywords = {Algorithms,Computational Biology,Gene Expression Data Analysis,Gene Expression Profiling,Gene Expression Regulation; Neoplastic,Humans,Integrative Gene Selection,Knowledge Bases,Neoplasm Proteins,Neoplasms,Pattern Recognition,Pattern Recognition; Automated,Prior Knowledge},
  language = {eng},
  number = {1},
  pmcid = {PMC6798862},
  pmid = {30785707}
}

@book{pfister1998search,
  title = {In Search of Clusters},
  author = {Pfister, G.F.},
  year = {1998},
  publisher = {{Prentice Hall PTR}},
  isbn = {978-0-13-899709-0},
  lccn = {98137730},
  series = {Parallel Programming Computer Architecture}
}

@misc{POWER9BenchmarksVs,
  title = {{{POWER9 Benchmarks}} vs. {{Intel Xeon}} vs. {{AMD EPYC Performance On Debian Linux}} - {{Phoronix}}},
  file = {/Users/i500763/Zotero/storage/STU49K8X/scan.html},
  howpublished = {https://www.phoronix.com/scan.php?page=article\&item=power9-epyc-xeon\&num=3}
}

@book{PracticalApproachesCausal2015,
  title = {Practical Approaches to Causal Relationship Exploration},
  year = {2015},
  edition = {1st edition},
  publisher = {{Springer Science+Business Media}},
  address = {{New York, NY}},
  file = {/Users/i500763/Zotero/storage/5SU64RV2/Samples.pdf},
  isbn = {978-3-319-14432-0}
}

@phdthesis{prellEmbracingExplicitCommunication2016,
  title = {Embracing Explicit Communication in Work-Stealing Runtime Systems},
  author = {Prell, Andreas},
  year = {2016},
  month = jul,
  address = {{Bayreuth}},
  file = {/Users/i500763/Zotero/storage/H9XXBBDF/Prell - 2016 - Embracing explicit communication in work-stealing .pdf},
  keywords = {Channels,Explicit Communication,Load Balancing,Message Passing,Parallel Programming,Runtime Systems,Task Parallelism,Work Stealing}
}

@article{qasaimehComparingEnergyEfficiency2019,
  title = {Comparing {{Energy Efficiency}} of {{CPU}}, {{GPU}} and {{FPGA Implementations}} for {{Vision Kernels}}},
  author = {Qasaimeh, Murad and Denolf, Kristof and Lo, Jack and Vissers, Kees and Zambreno, Joseph and Jones, Phillip H.},
  year = {2019},
  month = may,
  abstract = {Developing high performance embedded vision applications requires balancing run-time performance with energy constraints. Given the mix of hardware accelerators that exist for embedded computer vision (e.g. multi-core CPUs, GPUs, and FPGAs), and their associated vendor optimized vision libraries, it becomes a challenge for developers to navigate this fragmented solution space. To aid with determining which embedded platform is most suitable for their application, we conduct a comprehensive benchmark of the run-time performance and energy efficiency of a wide range of vision kernels. We discuss rationales for why a given underlying hardware architecture innately performs well or poorly based on the characteristics of a range of vision kernel categories. Specifically, our study is performed for three commonly used HW accelerators for embedded vision applications: ARM57 CPU, Jetson TX2 GPU and ZCU102 FPGA, using their vendor optimized vision libraries: OpenCV, VisionWorks and xfOpenCV. Our results show that the GPU achieves an energy/frame reduction ratio of 1.1-3.2x compared to the others for simple kernels. While for more complicated kernels and complete vision pipelines, the FPGA outperforms the others with energy/frame reduction ratios of 1.2-22.3x. It is also observed that the FPGA performs increasingly better as a vision application's pipeline complexity grows.},
  archiveprefix = {arXiv},
  eprint = {1906.11879},
  eprinttype = {arxiv},
  file = {/Users/i500763/Zotero/storage/AQF52B2P/Qasaimeh et al. - 2019 - Comparing Energy Efficiency of CPU, GPU and FPGA I.pdf;/Users/i500763/Zotero/storage/FD44EMH6/1906.html},
  journal = {arXiv:1906.11879 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryclass = {cs, eess}
}

@article{quinnParallelProgramming2003,
  title = {Parallel Programming},
  author = {Quinn, Michael J},
  year = {2003},
  volume = {526},
  pages = {105},
  file = {/Users/i500763/Zotero/storage/27PM82CQ/Quinn - 2003 - Parallel programming.pdf},
  journal = {TMH CSE}
}

@article{rauJointEstimationCausal2013,
  title = {Joint Estimation of Causal Effects from Observational and Intervention Gene Expression Data},
  author = {Rau, Andrea and Jaffr{\'e}zic, Florence and Nuel, Gr{\'e}gory},
  year = {2013},
  month = oct,
  volume = {7},
  pages = {111},
  issn = {1752-0509},
  doi = {10.1186/1752-0509-7-111},
  abstract = {In recent years, there has been great interest in using transcriptomic data to infer gene regulatory networks. For the time being, methodological development in this area has primarily made use of graphical Gaussian models for observational wild-type data, resulting in undirected graphs that are not able to accurately highlight causal relationships among genes. In the present work, we seek to improve the estimation of causal effects among genes by jointly modeling observational transcriptomic data with arbitrarily complex intervention data obtained by performing partial, single, or multiple gene knock-outs or knock-downs.},
  file = {/Users/i500763/Zotero/storage/WEBTMQYL/Rau et al. - 2013 - Joint estimation of causal effects from observatio.pdf;/Users/i500763/Zotero/storage/YTULDTPU/1752-0509-7-111.html},
  journal = {BMC Systems Biology},
  keywords = {Causal inference,Gaussian Bayesian network,Intervention calculus,Maximum likelihood,Metropolis-Hastings},
  number = {1}
}

@inproceedings{rayPracticalTechniquesPerformance2005,
  title = {Practical Techniques for Performance Estimation of Processors},
  booktitle = {Fifth {{International Workshop}} on {{System}}-on-{{Chip}} for {{Real}}-{{Time Applications}} ({{IWSOC}}'05)},
  author = {Ray, A. and Srikanthan, T. and {Wu Jigang}},
  year = {2005},
  pages = {308--311},
  publisher = {{IEEE}},
  address = {{Banff, Alta., Canada}},
  doi = {10.1109/IWSOC.2005.94},
  abstract = {Performance estimation of processor is important to select the right processor for an application. Poorly chosen processors can either under perform very badly or over perform but with high cost. Most previous work on performance estimation are based on generating the development tools, i.e., compilers, assemblers etc from a processor description file and then additionally generating an instruction set simulator to get the performance. In this work we present a simpler strategy for performance estimation. We propose an estimation technique based on the intermediate format of an application. The estimation process does not require the generation of all the development tools as in the prevalent methods. As a result our method is not only cheaper but also faster.},
  file = {/Users/i500763/Zotero/storage/TPM8ITB8/Ray et al. - 2005 - Practical techniques for performance estimation of.pdf},
  isbn = {978-0-7695-2403-0},
  language = {en}
}

@misc{sakharnykhEVERYTHINGYOUNEED,
  title = {{{EVERYTHING YOU NEED TO KNOW ABOUT UNIFIED MEMORY}}},
  author = {Sakharnykh, Nikolay},
  copyright = {NVIDIA},
  file = {/Users/i500763/Zotero/storage/Z7AQK26E/Sakharnykh - EVERYTHING YOU NEED TO KNOW ABOUT UNIFIED MEMORY.pdf},
  language = {en}
}

@article{saravananStudyFactorsInfluencing2011,
  title = {A {{Study}} on {{Factors Influencing Power Consumption}} in {{Multithreaded}} and {{Multicore CPUs}}},
  author = {Saravanan, Vijayalakshmi and Chandran, Senthil Kumar and Kothari, D P},
  year = {2011},
  volume = {10},
  pages = {11},
  abstract = {The ever-growing demand for computational power and high performance has led to a rapid growth in the semiconductor industry. This evolution has seen a continuous increase in CPU performance and the number of transistors on a chip has roughly doubled every two years \textendash{} proving Moore's law. An inevitable consequence when achieving this is that more functional units, deeper pipelining and larger cache sizes have had to be implemented on the CPU chip. The result is a significant increase in the power consumption. Achieving high performance with low power consumption has been the traditional goal in high-end processors. In order to accomplish high performance, multithreaded and multicore CPUs have become the recent trend in semi-conductor technology. The purpose of this paper is to statistically analyze the various factors that affect power, to study their relationship to quantify their influence on power consumption in multithreaded and multicore CPUs. This paper explores the on-chip power modeling simulation techniques with the existing processors and compares the performance and power trade-off between multicore and multithreaded CPUs. In this paper, we also present review/tutorial of recent advancements in power savings through the implementation of power-limiting micro-architectural features (e.g. out-of-order execution, branch prediction, caching and prefetching) in contemporary multi-core processors, such as Intel Nehalem and AMD's Istanbul processors. The results show that the statistical findings on power consumption are encouraging and useful for low power application and power-aware processor designers.},
  file = {/Users/i500763/Zotero/storage/IJYICR64/Saravanan et al. - 2011 - A Study on Factors Influencing Power Consumption i.pdf},
  language = {en},
  number = {3}
}

@inproceedings{schmidtLoadBalancedParallelConstraintBased2019,
  title = {Load-{{Balanced Parallel Constraint}}-{{Based Causal Structure Learning}} on {{Multi}}-{{Core Systems}} for {{High}}-{{Dimensional Data}}},
  author = {Schmidt, Christopher and Huegle, Johannes and Bode, Philipp and Uflacker, Matthias},
  year = {2019},
  month = aug,
  volume = {104},
  pages = {59--77},
  publisher = {{PMLR}},
  address = {{Anchorage, Alaska, USA}},
  abstract = {In the context of high-dimensional data state-of-the-art methods for constraint-based causal structure learning, such as the PC algorithm, are limited in their application through their worst case exponential computational complexity. To address the resulting long execution time, several parallel extensions have been developed to exploit modern multi-core systems. These extensions apply a static distribution of tasks to the execution units to achieve parallelism, which introduces the problem of load imbalance. In our work, we propose a parallel implementation that follows a dynamic task distribution in order to avoid situations of load imbalance and improve the execution time. On the basis of an experimental evaluation on real-world high dimensional datasets, we show that our implementation has a better load balancing compared to an existing parallel implementation in the context of multivariate normal distributed data. For datasets that introduce load imbalance, our dynamic task distribution approach outperforms existing static approaches by factors up to 2.4. Overall, we increase the speed up from factors of up to 27, for the static approach, to factors of up to 39 for the dynamic approach, when scaling to 80 cores compared to a non-parallel execution.},
  file = {/Users/i500763/Zotero/storage/TVC7UIAW/Schmidt et al. - Load-Balanced Parallel Constraint-Based Causal Str.pdf},
  language = {en},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{schmidtOrderIndependentConstraintBasedCausal2018,
  title = {Order-{{Independent Constraint}}-{{Based Causal Structure Learning}} for {{Gaussian Distribution Models}} Using {{GPUs}}},
  author = {Schmidt, Christopher and Huegle, Johannes},
  year = {2018},
  pages = {10},
  abstract = {Learning the causal structures in high-dimensional datasets allows deriving advanced insights from observational data, thus creating the potential for new applications. One crucial limitation of stateof-the-art methods for learning causal relationships, such as the PC algorithm, is their long execution time. While, in the worst case, the execution time is exponential to the dimension of a given dataset, it is polynomial if the underlying causal structures are sparse. To address the long execution time, parallelized extensions of the algorithm have been developed addressing the Central Processing Unit (CPU) as the primary execution device. While modern multicore CPUs expose a decent level of parallelism, coprocessors, such as Graphics Processing Units (GPUs), are specifically designed to process thousands of data points in parallel, providing superior parallel processing capabilities compared to CPUs. In our work, we leverage the parallel processing power of GPUs to address the drawback of the long execution time of the PC algorithm and develop an efficient GPU-accelerated implementation for Gaussian distribution models. Based on an experimental evaluation of various high-dimensional real-world gene expression datasets, we show that our GPU-accelerated implementation outperforms existing CPU-based versions, by factors up to 700.},
  file = {/Users/i500763/Zotero/storage/ANBH9IMQ/Schmidt and Huegle - 2018 - Order-Independent Constraint-Based Causal Structur.pdf},
  language = {en}
}

@incollection{schmidtOutofCoreGPUAcceleratedCausal2020,
  title = {Out-of-{{Core GPU}}-{{Accelerated Causal Structure Learning}}},
  booktitle = {Algorithms and {{Architectures}} for {{Parallel Processing}}},
  author = {Schmidt, Christopher and Huegle, Johannes and Horschig, Siegfried and Uflacker, Matthias},
  editor = {Wen, Sheng and Zomaya, Albert and Yang, Laurence T.},
  year = {2020},
  volume = {11944},
  pages = {89--104},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-38991-8_7},
  abstract = {Learning the causal structures in high-dimensional datasets enables deriving advanced insights from observational data. For example, the construction of gene regulatory networks inferred from gene expression data supports solving biological and biomedical problems, such as, in drug design or diagnostics. With the adoption of Graphics Processing Units (GPUs) the runtime of constraint-based causal structure learning algorithms on multivariate normal distributed data is significantly reduced. For extremely high-dimensional datasets, e.g., provided by The Cancer Genome Atlas (TCGA), state-of-the-art GPU-accelerated algorithms hit the device memory limit of single GPUs and consequently, execution fails. In order to overcome this limitation, we propose an outof-core algorithm for GPU-accelerated constraint-based causal structure learning on multivariate normal distributed data. We experimentally validate the scalability of our algorithm, beyond GPU device memory capacities and compare our implementation to a baseline using Unified Memory (UM). In recent GPU generations, UM overcomes the device memory limit, by utilizing the GPU page migration engine. On a real-world gene expression dataset from the TCGA, our approach outperforms the baseline by a factor of 95 and is faster than a parallel Central Processing Unit (CPU)-based version by a factor of 236.},
  file = {/Users/i500763/Zotero/storage/2S9RV98K/Schmidt et al. - 2020 - Out-of-Core GPU-Accelerated Causal Structure Learn.pdf},
  isbn = {978-3-030-38990-1 978-3-030-38991-8},
  language = {en}
}

@article{scutariBayesianNetworkConstraintBased2017,
  title = {Bayesian {{Network Constraint}}-{{Based Structure Learning Algorithms}}: {{Parallel}} and {{Optimized Implementations}} in the Bnlearn {{R Package}}},
  shorttitle = {Bayesian {{Network Constraint}}-{{Based Structure Learning Algorithms}}},
  author = {Scutari, Marco},
  year = {2017},
  volume = {77},
  pages = {1--20},
  issn = {1548-7660},
  doi = {10.18637/jss.v077.i02},
  abstract = {It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: Its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios. Efficient implementations of score-based structure learning benefit from past and current research in optimization theory, which can be adapted to the task by using the network score as the objective function to maximize. This is not true for approaches based on conditional independence tests, called constraint-based learning algorithms. The only optimization in widespread use, backtracking, leverages the symmetries implied by the definitions of neighborhood and Markov blanket. In this paper we illustrate how backtracking is implemented in recent versions of the bnlearn R package, and how it degrades the stability of Bayesian network structure learning for little gain in terms of speed. As an alternative, we describe a software architecture and framework that can be used to parallelize constraint-based structure learning algorithms (also implemented in bnlearn) and we demonstrate its performance using four reference networks and two real-world data sets from genetics and systems biology. We show that on modern multi-core or multiprocessor hardware parallel implementations are preferable over backtracking, which was developed when single-processor machines were the norm.},
  file = {/Users/i500763/Zotero/storage/QXLG6CR4/Scutari - 2017 - Bayesian Network Constraint-Based Structure Learni.pdf},
  journal = {Journal of Statistical Software, Articles},
  keywords = {Bayesian networks,parallel programming,R,structure learning},
  language = {en},
  number = {2}
}

@article{scutariLearningBayesianNetworks2010,
  title = {Learning {{Bayesian Networks}} with the Bnlearn {{R Package}}},
  author = {Scutari, Marco},
  year = {2010},
  month = jul,
  abstract = {bnlearn is an R package which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables. Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package to improve their performance via parallel computing. Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use. Advanced plotting options are provided by the Rgraphviz package.},
  archiveprefix = {arXiv},
  eprint = {0908.3817},
  eprinttype = {arxiv},
  file = {/Users/i500763/Zotero/storage/CZRSPCZH/Scutari - 2010 - Learning Bayesian Networks with the bnlearn R Pack.pdf;/Users/i500763/Zotero/storage/Q4U93FS2/0908.html},
  journal = {arXiv:0908.3817 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryclass = {stat}
}

@article{shanHeterogeneousProcessingStrategy2006,
  title = {Heterogeneous {{Processing}}: A {{Strategy}} for {{Augmenting Moore}}'s {{Law}}},
  author = {Shan, Amar},
  year = {2006},
  journal = {Linux Journal}
}

@inproceedings{shoshaniSequencingTasksMultiprocess1970,
  title = {Sequencing Tasks in Multiprocess Systems to Avoid Deadlocks},
  booktitle = {11th {{Annual Symposium}} on {{Switching}} and {{Automata Theory}} (Swat 1970)},
  author = {Shoshani, A. and Coffman, E. G.},
  year = {1970},
  month = oct,
  pages = {225--235},
  issn = {0272-4847},
  doi = {10.1109/SWAT.1970.20},
  abstract = {The "deadlock" problem is a logical problem which may occur in a system in which resources are shared among users. Deadlock is a situation in which two or more jobs cannot be completed because of conflicting resource requirements. In this paper we assume that advance information about future resource requirements of jobs is available. In particular, we assume that jobs are represented as a sequence of "job steps" during which the resource usage remains constant. An algorithm which uses this information to determine whether a request can be granted without causing a deadlock is presented.},
  file = {/Users/i500763/Zotero/storage/APPBA9QP/4569651.html},
  keywords = {Availability,Concurrent computing,Degradation,Multiprocessing systems,Resource management,System recovery}
}

@article{singhSurveyStaticScheduling2015,
  title = {A {{Survey}} of {{Static Scheduling Algorithm}} for {{Distributed Computing System}}},
  author = {Singh, Khushboo and Alam, Mahfooz and Kumar, Sushil},
  year = {2015},
  month = nov,
  volume = {129},
  pages = {25--30},
  doi = {10.5120/ijca2015906828},
  abstract = {The static scheduling algorithms are widely used to evaluate the performance of distributed computing system. In such systems, purpose of scheduling algorithm is to allocate tasks to available processor so as to efficiently utilize this processor and to reduce the makespan, total computational cost and various other such factors, with the motive of achieving optimal solution. Scheduling algorithms are classified into two broad categories i.e., static scheduling algorithms and dynamic scheduling algorithms. In this paper we are discussing various static scheduling algorithm and numerous problems in various levels of the homogeneous and heterogeneous distributed systems. Also we are comparing these algorithms on the basis of various factors such as speedup, time complexity, scheduling length ratio, normalized scheduling length and so on.},
  file = {/Users/i500763/Zotero/storage/FX84U6KS/Singh et al. - 2015 - A Survey of Static Scheduling Algorithm for Distri.pdf},
  journal = {International Journal of Computer Applications}
}

@article{smartFullyHomomorphicSIMD2014,
  title = {Fully Homomorphic {{SIMD}} Operations},
  author = {Smart, N. P. and Vercauteren, F.},
  year = {2014},
  month = apr,
  volume = {71},
  pages = {57--81},
  issn = {1573-7586},
  doi = {10.1007/s10623-012-9720-4},
  abstract = {At PKC 2010 Smart and Vercauteren presented a variant of Gentry's fully homomorphic public key encryption scheme and mentioned that the scheme could support SIMD style operations. The slow key generation process of the Smart\textendash Vercauteren system was then addressed in a paper by Gentry and Halevi, but their key generation method appears to exclude the SIMD style operation alluded to by Smart and Vercauteren. In this paper, we show how to select parameters to enable such SIMD operations. As such, we obtain a somewhat homomorphic scheme supporting both SIMD operations and operations on large finite fields of characteristic two. This somewhat homomorphic scheme can be made fully homomorphic in a naive way by recrypting all data elements separately. However, we show that the SIMD operations can be used to perform the recrypt procedure in parallel, resulting in a substantial speed-up. Finally, we demonstrate how such SIMD operations can be used to perform various tasks by studying two use cases: implementing AES homomorphically and encrypted database lookup.},
  file = {/Users/i500763/Zotero/storage/YGJKHYKR/Smart and Vercauteren - 2014 - Fully homomorphic SIMD operations.pdf},
  journal = {Designs, Codes and Cryptography},
  language = {en},
  number = {1}
}

@inproceedings{smithStudyBranchPrediction1998,
  title = {A Study of Branch Prediction Strategies},
  booktitle = {25 Years of the International Symposia on {{Computer}} Architecture (Selected Papers)  - {{ISCA}} '98},
  author = {Smith, James E.},
  year = {1998},
  pages = {202--215},
  publisher = {{ACM Press}},
  address = {{Barcelona, Spain}},
  doi = {10.1145/285930.285980},
  abstract = {In high-performance computer systems. performance losses due to conditional branch instructrons can be minrmized by predicting a branch outcome and fetching, decoding, and/or issuing subsequent instructions before the actual outcome is known. This paper discusses branch prediction strategies wrth the goal of maxtmizing prediction accuracy. First. currently used techniques are discussed and analyzed using instruction tmce data. Then, new techniques are proposed and are shown to provide greater accuracy and more flexibility at low cost.},
  file = {/Users/i500763/Zotero/storage/RJRECFRF/Smith - 1998 - A study of branch prediction strategies.pdf},
  isbn = {978-1-58113-058-4},
  language = {en}
}

@article{sosuthaHeterogeneousParallelComputing2015,
  title = {Heterogeneous {{Parallel Computing Using Cuda}} for {{Chemical Process}}},
  author = {Sosutha, S},
  year = {2015},
  pages = {10},
  abstract = {CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. Using CUDA, the GPUs can be used for general purpose processing which involves parallel computation. CUDA has been used to accelerate non-graphical applications in computational biology, cryptography and other fields by an order of magnitude or more. Chemical processes need validation of their experimental data. It was found that Chemical process could become one such application where CUDA can be efficiently used. These validations of Chemical processes normally involve calculation of many coefficients. The chemical process that has been chosen for parallelizing is Heat Transfer process. This process involves calculation of coefficients for multiple iterations. As each of these iterations is independent of one another, CUDA was used to parallelize the calculation process. The execution time analysis shows that though CPU outperforms GPU when the numbers of iterations are less, when the number of iterations increase the GPU outperforms CPU greatly.},
  file = {/Users/i500763/Zotero/storage/QRV46AFU/Sosutha - 2015 - Heterogeneous Parallel Computing Using Cuda for Ch.pdf},
  journal = {Procedia Computer Science},
  language = {en}
}

@book{spirtesCausationPredictionSearch1993,
  title = {Causation, {{Prediction}}, and {{Search}}},
  author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
  editor = {Berger, J. and Fienberg, S. and Gani, J. and Krickeberg, K. and Olkin, I. and Singer, B.},
  year = {1993},
  volume = {81},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-2748-9},
  file = {/Users/i500763/Zotero/storage/HF7T6BYB/Spirtes et al. - 1993 - Causation, Prediction, and Search.pdf},
  isbn = {978-1-4612-7650-0 978-1-4612-2748-9},
  language = {en},
  series = {Lecture {{Notes}} in {{Statistics}}}
}

@inproceedings{srivastavaParallelFrameworkConstraintBased2020,
  title = {A {{Parallel Framework}} for {{Constraint}}-{{Based Bayesian Network Learning}} via {{Markov Blanket Discovery}}},
  booktitle = {{{SC20}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Srivastava, Ankit and Chockalingam, Sriram P. and Aluru, Srinivas},
  year = {2020},
  month = nov,
  pages = {1--15},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  doi = {10.1109/SC41405.2020.00011},
  abstract = {Bayesian networks (BNs) are a widely used graphical model in machine learning. As learning the structure of BNs is NP-hard, high-performance computing methods are necessary for constructing large-scale networks. In this paper, we present a parallel framework to scale BN structure learning algorithms to tens of thousands of variables. Our framework is applicable to learning algorithms that rely on the discovery of Markov blankets (MBs) as an intermediate step. We demonstrate the applicability of our framework by parallelizing three different algorithms: Grow-Shrink (GS), Incremental Association MB (IAMB), and Interleaved IAMB (Inter-IAMB). Our implementations are able to construct BNs from real data sets with tens of thousands of variables and thousands of observations in less than a minute on 1024 cores, with a speedup of up to 845X and 82.5\% efficiency. Furthermore, we demonstrate using simulated data sets that our proposed parallel framework can scale to BNs of even higher dimensionality.},
  file = {/Users/i500763/Zotero/storage/SAAUJ3GS/Srivastava et al. - 2020 - A Parallel Framework for Constraint-Based Bayesian.pdf},
  isbn = {978-1-72819-998-6},
  language = {en}
}

@article{stoneAcceleratingMolecularModeling2007,
  title = {Accelerating Molecular Modeling Applications with Graphics Processors},
  author = {Stone, John E. and Phillips, James C. and Freddolino, Peter L. and Hardy, David J. and Trabuco, Leonardo G. and Schulten, Klaus},
  year = {2007},
  volume = {28},
  pages = {2618--2640},
  issn = {1096-987X},
  doi = {10.1002/jcc.20829},
  abstract = {Molecular mechanics simulations offer a computational approach to study the behavior of biomolecules at atomic detail, but such simulations are limited in size and timescale by the available computing resources. State-of-the-art graphics processing units (GPUs) can perform over 500 billion arithmetic operations per second, a tremendous computational resource that can now be utilized for general purpose computing as a result of recent advances in GPU hardware and software architecture. In this article, an overview of recent advances in programmable GPUs is presented, with an emphasis on their application to molecular mechanics simulations and the programming techniques required to obtain optimal performance in these cases. We demonstrate the use of GPUs for the calculation of long-range electrostatics and nonbonded forces for molecular dynamics simulations, where GPU-based calculations are typically 10\textendash 100 times faster than heavily optimized CPU-based implementations. The application of GPU acceleration to biomolecular simulation is also demonstrated through the use of GPU-accelerated Coulomb-based ion placement and calculation of time-averaged potentials from molecular dynamics trajectories. A novel approximation to Coulomb potential calculation, the multilevel summation method, is introduced and compared with direct Coulomb summation. In light of the performance obtained for this set of calculations, future applications of graphics processors to molecular dynamics simulations are discussed. \textcopyright{} 2007 Wiley Periodicals, Inc. J Comput Chem, 2007},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcc.20829},
  copyright = {Copyright \textcopyright{} 2007 Wiley Periodicals, Inc.},
  file = {/Users/i500763/Zotero/storage/XDP8BYFM/Stone et al. - 2007 - Accelerating molecular modeling applications with .pdf;/Users/i500763/Zotero/storage/3ITZL462/jcc.html},
  journal = {Journal of Computational Chemistry},
  keywords = {CUDA,electrostatic potential,GPU computing,graphics processing unit,ion placement,molecular dynamics,molecular modeling,multilevel summation,multithreading,parallel computing},
  language = {en},
  number = {16}
}

@article{sutterFreeLunchFundamental2013,
  title = {The {{Free Lunch Is Over A Fundamental Turn Toward Concurrency}} in {{Software}}},
  author = {Sutter, H.},
  year = {2013},
  abstract = {The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have run out of room with most of their traditional approaches to boosting CPU performance. Instead of driving clock speeds and straight-line instruction throughput ever higher, they are instead turning en masse to hyperthreading and multicore architectures. Both of these features are already available on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors, and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 InStat/MDR Fall Processor Forum was multicore devices, as many companies showed new or updated multicore processors. Looking back, it's not much of a stretch to call 2004 the year of multicore.},
  file = {/Users/i500763/Zotero/storage/HGBJVUIA/Free_Lunch.pdf;/Users/i500763/Zotero/storage/N57BKIBX/324d81807507a03b9e301def1887f686b18c13bc.html},
  journal = {undefined},
  language = {en}
}

@inproceedings{tomeOptimizingGroupByAggregation2018,
  title = {Optimizing {{Group}}-{{By And Aggregation}} Using {{GPU}}-{{CPU Co}}-{{Processing}}},
  booktitle = {International {{Workshop}} on {{Accelerating Analytics}} and {{Data Management Systems Using Modern Processor}} and {{Storage Architectures}}, {{ADMS}}@{{VLDB}} 2018, {{Rio}} de {{Janeiro}}, {{Brazil}}, {{August}} 27, 2018},
  author = {Tome, Diego and Gubner, Tim and Raasveldt, Mark},
  year = {2018},
  pages = {1--10},
  abstract = {While GPU query processing is a well-studied area, real adoption is limited in practice as typically GPU execution is only significantly faster than CPU execution if the data resides in GPU memory, which limits scalability to small data scenarios where performance tends to be less critical. Another problem is that not all query code (e.g. UDFs) will realistically be able to run on GPUs. We therefore investigate CPU-GPU co-processing, where both the CPU and GPU are involved in evaluating the query in scenarios where the data does not fit in the GPU memory.},
  file = {/Users/i500763/Zotero/storage/MAPIXF23/Tome et al. - Optimizing Group-By And Aggregation using GPU-CPU .pdf},
  language = {en}
}

@article{topcuogluPerformanceeffectiveLowcomplexityTask2002,
  title = {Performance-Effective and Low-Complexity Task Scheduling for Heterogeneous Computing},
  author = {Topcuoglu, H. and Hariri, S. and Wu, Min-You},
  year = {2002},
  month = mar,
  volume = {13},
  pages = {260--274},
  issn = {1558-2183},
  doi = {10.1109/71.993206},
  abstract = {Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics.},
  file = {/Users/i500763/Zotero/storage/TU2WXSDH/Topcuoglu et al. - 2002 - Performance-effective and low-complexity task sche.pdf;/Users/i500763/Zotero/storage/IDMR4DKB/993206.html},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {Processor scheduling},
  number = {3}
}

@misc{UNIFIEDMEMORYP9,
  title = {{{UNIFIED MEMORY ON P9}}+{{V100}}},
  copyright = {NVIDIA},
  file = {/Users/i500763/Zotero/storage/J8VIK8NZ/ORNL_workshop_mar2018.pdf},
  language = {en}
}

@misc{VectorExtensionsUsing,
  title = {Vector {{Extensions}} ({{Using}} the {{GNU Compiler Collection}} ({{GCC}}))},
  file = {/Users/i500763/Zotero/storage/X2LZYSEG/Vector-Extensions.html},
  howpublished = {https://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html}
}

@inproceedings{vermaEquivalenceSynthesisCausal1990,
  title = {Equivalence and Synthesis of Causal Models},
  booktitle = {Proceedings of the {{Sixth Annual Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Verma, Thomas and Pearl, Judea},
  year = {1990},
  month = jul,
  pages = {255--270},
  publisher = {{Elsevier Science Inc.}},
  address = {{USA}},
  isbn = {978-0-444-89264-5},
  series = {{{UAI}} '90}
}

@article{weinsteinCancerGenomeAtlas2013,
  title = {The {{Cancer Genome Atlas Pan}}-{{Cancer}} Analysis Project},
  author = {Weinstein, John N. and Collisson, Eric A. and Mills, Gordon B. and Shaw, Kenna R. Mills and Ozenberger, Brad A. and Ellrott, Kyle and Shmulevich, Ilya and Sander, Chris and Stuart, Joshua M.},
  year = {2013},
  month = oct,
  volume = {45},
  pages = {1113--1120},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1718},
  doi = {10.1038/ng.2764},
  abstract = {Current clinical practice is organized according to tissue or organ of origin of tumors. Now, The Cancer Genome Atlas (TCGA) Research Network has started to identify genomic and other molecular commonalities among a dozen different types of cancer. Emerging similarities and contrasts will form the basis for targeted therapies of the future and for repurposing existing therapies by molecular rather than histological similarities of the diseases.},
  copyright = {2013 The Author(s)},
  file = {/Users/i500763/Zotero/storage/I7RFFWYZ/Weinstein et al. - 2013 - The Cancer Genome Atlas Pan-Cancer analysis projec.pdf;/Users/i500763/Zotero/storage/UFEI8W5D/ng.html},
  journal = {Nature Genetics},
  language = {en},
  number = {10}
}

@article{younessLoadBalancingCPUGPU,
  title = {Load {{Balancing}} on {{CPU}}-{{GPU Heterogeneous System}}},
  author = {Youness, Hassan and Osama, Muhammed and Tarek, Aiman},
  pages = {7},
  abstract = {In this paper, a High Performance Computing (HPC) technique with load balancing on Personal Computers (PC) with CPU-GPU Heterogeneous System is presented. The technique is based on a Householder algorithm that used mainly for matrix reduction techniques. The purpose for implementing this algorithm is to perform a benchmark to compare the execution performance among three different implementations CPU, GPU and CPU-GPU with load balancing. Our goal is to achieve the best performance by load balancing between multi-core CPU and many-core massively threaded GPU using a Scale Optimization Technique (SOT) to build a HPC system.},
  file = {/Users/i500763/Zotero/storage/F423E5FA/Youness et al. - Load Balancing on CPU-GPU Heterogeneous System.pdf},
  language = {en}
}

@article{yuEfficientMatrixFactorization,
  title = {Efficient {{Matrix Factorization}} on {{Heterogeneous CPU}}-{{GPU Systems}}},
  author = {Yu, Yuanhang and University, Zhejiang Gongshang},
  pages = {6},
  abstract = {Matrix Factorization (MF) has been widely applied in machine learning and data mining. Due to the large computational cost of MF, we aim to improve the efficiency of SGD-based MF computation by utilizing the massive parallel processing power of heterogeneous multiprocessors. The main challenge in parallel SGD algorithms on heterogeneous CPUGPU systems lies in the strategy to assign tasks. We design a novel strategy to divide the matrix into a set of blocks by considering two aspects. First, we observe that the matrix should be divided nonuniformly, and relatively large blocks should be assigned to GPUs to saturate the computing power of GPUs. In addition to exploiting the characteristics of hardware, the workloads assigned to two types of hardware should be balanced. We design a cost model tailored for our problem to accurately estimate the performance of hardware on different data sizes. Extensive experiments show that our proposed algorithm achieves high efficiency with a high quality of training quality.},
  file = {/Users/i500763/Zotero/storage/SDKDY797/Yu and University - Efficient Matrix Factorization on Heterogeneous CP.pdf},
  language = {en}
}

@article{zarebavaniCuPCCUDAbasedParallel2018,
  title = {{{cuPC}}: {{CUDA}}-Based {{Parallel PC Algorithm}} for {{Causal Structure Learning}} on {{GPU}}},
  author = {Zarebavani, Behrooz and Jafarinejad, Foad and Hashemi, Matin and Salehkaleybar, Saber},
  year = {2018},
  volume = {abs/1812.08491},
  pages = {14},
  abstract = {The main goal in many fields in the empirical sciences is to discover causal relationships among a set of variables from observational data. PC algorithm is one of the promising solutions to learn underlying causal structure by performing a number of conditional independence tests. In this paper, we propose a novel GPU-based parallel algorithm, called cuPC, to execute an order-independent version of PC. The proposed solution has two variants, cuPC-E and cuPC-S, which parallelize PC in two different ways for multivariate normal distribution. Experimental results show the scalability of the proposed algorithms with respect to the number of variables, the number of samples, and different graph densities. For instance, in one of the most challenging datasets, the runtime is reduced from more than 11 hours to about 4 seconds. On average, cuPC-E and cuPC-S achieve 500 X and 1300 X speedup, respectively, compared to serial implementation on CPU. The source code of cuPC is available online [1].},
  annotation = {\_eprint: 1812.08491},
  file = {/Users/i500763/Zotero/storage/X56FYKSI/Zarebavani et al. - cuPC CUDA-based Parallel PC Algorithm for Causal .pdf},
  journal = {IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS},
  language = {en}
}

@article{zargesEvaluationOnNodeGPU,
  title = {Evaluation of {{On}}-{{Node GPU Interconnects}} for {{Training Deep Neural Networks}}},
  author = {Zarges, Nane-Maiken},
  pages = {103},
  file = {/Users/i500763/Zotero/storage/WSDN3C3M/Zarges - Evaluation of On-Node GPU Interconnects for Traini.pdf},
  language = {en}
}

@inproceedings{zhangDynamicStaticLoad1991,
  title = {Dynamic and Static Load Scheduling Performance on a {{NUMA}} Shared Memory Multiprocessor},
  booktitle = {Proceedings of the 5th International Conference on {{Supercomputing}}},
  author = {Zhang, Xiaodong},
  year = {1991},
  month = jun,
  pages = {128--135},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/109025.109062},
  file = {/Users/i500763/Zotero/storage/2TPJGPTA/109025.pdf},
  isbn = {978-0-89791-434-5},
  series = {{{ICS}} '91}
}


