%!TEX root = ../thesis.tex
\chapter{Background}
In this chapter, we will look into heterogeneous computing, heterogeneous systems and the processing units frequently used in such systems. While heterogeneous systems power heterogeneous computing algorithms, they can consist of processing units of all kinds. This thesis focus lies on the central processing unit (CPU) and the graphics processing unit (GPU) as the processing power of a heterogeneous system.
After that the theoretical background of Causal Structure Learning and the PC-algorithm will be explained, which we will connect to the possible execution on heterogeneous systems.

\section{Heterogeneous Computing}
Heterogeneous computing refers to systems which include conventional as well as specialized processors. Those different processors work cooperatively on the same workload. \cite{shan_heterogeneous_2006} Heterogeneous computing aims to use more than one processing unit of the systems resources and increase the execution effectiveness. Heterogeneity in the computing context refers to different instruction-set architectures (ISA), which means that the processors in this system are heterogeneous in their architecture.
Abstractions can eliminate this heterogeneity for the software developer, but since those processors have different advantages and disadvantages over another the developer still has to think on the optimal usage of such.

Most modern computing systems are heterogeneous and consist of one or more CPU and one or more GPU. Therefore a heterogeneous computing approach could be beneficial in their effectiveness for such systems. Both, the CPU and the GPU are very different in their architecture and function, so most modern applications only use one of those processors effectively.

CPU-GPU heterogeneous computing approaches are also referred to as hybrid or co-operative execution. In most applications one of GPU and CPU is called the host, which controls the execution flow, and the device, that accelerates the execution of he host. Therefore the device is also called the accelerator. In this thesis both the CPU and the GPU are called processing units (PU) as a term that includes both processors.

\subsection{Central Processing Unit}
The CPU, sometimes also just called processor, is the main instruction executing component of a computer. While modern CPUs introduce parallelism concepts, the main purpose is still low latency execution of given instructions.
Without those later introduced parallelism concepts, a CPU is only able to execute instructions in a serial manner. The ability to run concurrent applications is mostly done by introducing a thread and scheduling concept, which switches those threads periodically using a set of rules. Programs can create such threads using an operating system interface.
Additional performance improvements such as superscalar architectures and instruction pipelines, which can be extended through optimizations such as branch prediction or speculative execution, help to support the main goal of the CPU, which is a robust and moderately fast way to run any code.
Modern CPU architectures include instruction set extensions for data level parallelism, called Single Instruction Multiple Data (SIMD). Those instructions execute operations, which would normally mutate scalar values, on vector registers in memory. Those SIMD extensions are common in most modern CPU architectures such as x86\_64 (SSE, AVX) or PowerPC (AltiVec). Mathematical operations benefit of SIMD. Especially in linear algebra optimized implementations for commonly used vector and matrix operations are implemented in Basic Linear Algebra Subprogram (BLAS) implementations. For realizing an optimal CPU side implementation for this thesis, using BLAS is essential and enables using the CPUs resources.
Modern CPUs have multiple so called cores, which can independently execute instructions and therefore are able to run multiple parallel threads concurrently. This is also called task-level parallelism.
Concurrency between threads, that share a common resource introduce another problem. Since there can not be any assumption about the relative speed of those threads, analogue inteferences are possible. Two threads accessing a resource at nearly the same time, can result in a so called race condition, since there is no way to identify the thread that accesses the resource first. To prevent this behavior, critical sections for safe concurrent access are introduced. At any moment, only one thread is allowed into the critical section shared between the threads. The implementation of a critical section requires atomic read / write behavior.
Such access mechanisms like atomic read / write and critical sections are also important in heterogeneous computing where concurrent memory access of multiple processors can occur.
Frameworks such as OpenMP can help the developer to handle multi-threading and safe concurrency with abstractions of those concepts. OpenMP, a shared/memory parallel programming framework for C/C++ and Fortran framework, helps annotating parallel regions like parallelizable for loops and creates multiple execution threads for those regions using a run-time.
The thesis implementation uses OpenMP for efficiency and parallelism purposes.

\subsection{Graphics Processing Unit}
The GPU handles graphics processing in computer systems. In the beginning video games were the main driver for technical advancements in developing better GPUs. For computing graphics the processing of multiple calculations in parallel is essential. The same operation has to be done on multiple voxels or vertices. Other use cases such as machine learning algorithms later also extensively used the systems GPU, because of their parallelizability and frameworks such as OpenCL or CUDA, that made developing programs running on the GPU easier.
CUDA is a super-set language of C/C++ that allows the definition of functions, that can be run on the GPU. The Nvidia CUDA compiler compiles CUDA code which then runs on the GPU as well as CUDA code that runs on the host processor.

As an example Nvidia GPU architecture is build hierarchical 
\begin{lstlisting}[language=C++, caption=Cuda kernel example, label=lst:kernel]
__global__
void add(int n, float *x, float *y)
{
  int index = threadIdx.x;
  int stride = blockDim.x;
  for (int i = index; i < n; i += stride)
      y[i] = x[i] + y[i];
}
\end{lstlisting}

\ref{lst:kernel}
%% TODO

- Hardware characteristics
- CUDA Basics
- Nvidia GPU Basics (Warp, Block, Streaming Multiprocessor)
- Memory management
- Unified Memory
- 

\subsection{Constraint Based Causal Structure Learning}
In this section, I introduce necessary terminology in the context of Causal Structure Learning which is the problem class I look into in this thesis. This section introduces the Causal Graphical Model and the PC-Algorithm based on the definition given by Schmidt et al \cite{schmidt_order-independent_2018}. The PC-Algorithm is the base for the heterogeneous approach this thesis is about.

\subsubsection{Causal Graphical Model}
Given a finite set of $N$ vertices $V = (V_1,...,V_N)$ and a set of edges $E \subseteq V \times V$, let $G = (V,E)$. The vertices each represent an observation. An edge $(V_i, V_j) \in E$ is called directed, i.e., $V_i \rightarrow V_j$, if $(V_i,V_j) \in E$ but $(V_j, V_i) \notin E$. An edge is called undirected, if both $(V_i,V_j) \in E$ and $(V_j, V_i) \in E$, i.e. $V_i - V_j$. Two vertices $V_i$, $V_j$ are called adjacent if there is an undirected edge $V_i - V_j$. The adjacenecy set $adj(G, V_i)$ of the vertex $V_i \in V$ in $G$ are all vertices $V_j \in V$ that are connected to $V_i$ by an undirected edge.

Some graph $G$ where all edges are directed and $G$ does not contain any cycle is a Directed Acyclic Graph (DAG). In the context of Causal Structure Learning a directed edge $E$ in such a DAG represents a direct causal relationship between the connected vertices.
The d-seperation criterion used in the DAG, enables information about the conditional independence between variables. Two variables $V_i, V_j \in V$ are conditionally independent given a set $S \subset V \backslash \{V_i, V_j\}$ if the vertices $V_i$ and $V_j$ are d-seperated by the set $S$, which is called seperation set.
A distribution $P$ of the variable set $V_1, ..., V_N$ that satisfies the above condition is called faithful.
Several different DAGs, that describe the same conditional independence information, form a Markov equivalent class. \cite{andersson_characterization_1997}. If two DAGs are Markov equivalent, they share the same skeleton $C$ or the underlying undirected graph and the same v-structures.
These v-structures are triples $V_i,V_j,V_k$ with $(V_i,V_k) \notin E$ and $(V_k,V_i) \notin E$ and directed edges $V_i \rightarrow V_j$ and $V_k \rightarrow V_j$. Moreover, it is possible to uniquely describe the corresponding Markov equivalent class by a Complete Partially Directed Acyclic Graph (CPDAG) \cite{chickering_optimal_2003}.
A CPDAG is a partially directed acyclic graph where all DAGs in the Markov equivalence class incorporate the dame directed edges, and there exist two DAGs that incorporate the two directed versions of every undirected edge $V_i - V_j$ in the Markov equivalence class.
Hence, the focus lies on the estimation of the equivalence class of the DAG G based on the corresponding probability distribution $P$ of the involved variables $V_1,...,V_N$. In particular, under the assumption that the distribution $P$ is generated from the true causal DAG $G$, there is an edge $V_i - V_j$ in the skeleton of $G$ if and only if $V_i,V_j$ are dependent given all $S \subseteq V\backslash \{V_i,V_j\}$ \cite{spirtes_causation_1993}.
Hence, the examination of the conditional independence information of the observed variables $V_1,...,V_N$ allows for the estimation of the undirected skeleton $C$ of the corresponding DAG $G$. The extension of the skeleton $C$ to the equivalence class of the DAG $G$ can be done by the repeated application of deterministic edge orientation rules on the skeleton \cite{colombo_order-independent_nodate,kalisch_estimating_2007,pearl_introduction_2010}.

\subsubsection{Gaussian Distribution Model}


\subsubsection{Constraint-Based Methods}



\subsection{PC Algorithm}
The PC Algorithm (Spirtes et al. \cite{spirtes_causation_1993}) was designed for learning DAGs. It assumes causal suffieceny, which means there are no umneasured common causes. The PC Algorithm is mostly used in high-dimensional data settings, because it aims to be performant for sparse graphs, that contain thousands of variables. \cite{kalisch_understanding_2010}
I now describe the PC Algorithm, as it is explained in Colombo et al. \cite{colombo_order-independent_nodate}. The version of the PC Algorithm is also called PC-stable, because of its order independence regarding the different calculations.

\begin{algorithm}
    \caption{Adjacency search of PC-stable algorithm \cite{colombo_order-independent_nodate}}
    \label{alg:pcstable}
    \begin{algorithmic}[1]
    \Require Vertex set $V$, correlation matrix $Cor$
    \Ensure Estimated skeleton $C$, separation sets \textbf{Sepset}
    \State with fully connected skeleton $C$ and $l = -1$
    \Repeat 
        \State $l=l+1$
        \For{all Variables $V_i$ in $C$}
            \State Let $a(V_i) = adj(C,V_i)$
        \EndFor
        \Repeat
            \State Select pairs $(V_i,V_j)$ adjacent in $C$ with $|a(V_i)\backslash\{V_j\}| \geq l$
            \Repeat
                \State Choose separation set $S \subseteq a(V_i ) \ {V_j }$ with $| S | = l$
                \If{$p(V_i,V_j|S) \leq \alpha$}
                    \State Delete edge $V_i - V_j$ from $C$
                    \State Set Sepset(i,j) = Sepset(j,i) = S
                \EndIf
            \Until{edge $V_i - V_j$ is deleted in $C$ or all $S \subseteq a(V_i) \backslash \{V_j\}$ with $|S| = l$ have been chosen}
        \Until{all adjacent vertices $V_i$, and $V_j$ in $C$ such that $|a(V_i)\backslash\{V_j\}| \geq l$ have been considered}
    \Until{each adjacent pair $V_i$, $V_j$ in $C$ satisfy $|a(V_i)\backslash\{V_j\}| \geq l$}
    \State \textbf{return} C, Sepset
    \end{algorithmic}
\end{algorithm}

As can be seen in \ref{alg:pcstable} the starting point of the algorithm is a set of vertices, that represent the given variables, and a correlation matrix 
- Explain PC Algorithm
- Pseudocode listing
- Loops