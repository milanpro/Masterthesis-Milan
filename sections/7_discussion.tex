%!TEX root = ../thesis.tex
\chapter{Discussion}
\label{chap:discussion}
In this chapter, the results of the experiments in Chapter \ref{chap:experiments} are discussed. In the discussion, the results critic points are worked out. In the end, we answer the research question of the problem statement chapter \ref{chap:problem_statement}.

% Performance improvements could be shown for level 2 and 3
% But only in specific contexts
%   - doees not scale well
%   - perfect for situations, where the GPU gets to its (memory) limit
% 
As can be seen in Table \ref{table:ac922_vs_delos} performance improvements are achievable through introducing heterogeneous computing in the PC algorithm context. With a performance improvement of 16,5\% compared to the GPU-only variant, using the additional resources of the heterogeneous system is benefitial for the execution time.

With a deeper look into this performance by looking at the different levels in Figure \ref{fig:levelwise_delos} it is clear that the performance improvement is conditionally bound to the level and dataset used. This result is solidified by the examination of the dataset in Chapter \ref{chap:est_speedup}. The estimation of level-wise potential speedup shows, that levels 3 and 4 tend to have the most and strongest outliers of maximum iteration counts. This is reflected by the level-wise tests, where level 3 shows the most speedup in Figure \ref{fig:levelwise_delos}, Figure \ref{fig:levelwise_ac922} and even clearer in Figure \ref{fig:levelwise_scaled_delos}.

The speedup estimation showed an even better potential speedup for the 4th level. The slowdown seen in the benchmarks can be explained by a very sparse adjacency matrix as a starting point to level 4, so that the execution time of that level is low, with around 200 milliseconds fot the TCGA-1000 dataset. The low execution time leads to the overhead of introducing heterogeneous computing being higher than the benefits gained.

A preliminary speedup estimation analysis of the dataset could be beneficial for using the heterogeneous computing approach and to decide which levels should be optimized using that method.

Solidifying the dataset and level dependency, using a 10 000 variables dataset slowed down the execution and only opened a performance improvement when looking at the level-wise performance. In level 3, the GPU-only variants execution duration is 2,467,224 ms, and the Delos workstealing execution duration is 2,107,416 ms.
The difference in level 3 is a 17,1\% improvement of performance by using both CPU and GPU. However, since every other level is slower with the workstealing execution, the whole execution time worsens. The 17,1\% performance improvement of level 3 is a solid increase, looking at the estimated possible speedup of around 30\%.

Level 2 is faster in the heterogeneous approach than in the GPU-only variant only for the 1000 variable dataset. There is a performance regression with the 10 000 variables dataset, which could hint at a scaling problem.
This scaling issue is irrelevant as soon as the memory limit of the GPU is reached, at which the heterogeneous approach helps a lot with its 6,61x speedup in comparison to the GPU-only variant \ref{table:workst_delos_scaling}.

% Workstealing better than pre-balanced through dynamic nature
%   - does not have to be tuned
% Pre balanced is very hard to tune (might be even harder for different datasets)
%   - perforamnce characteristic has to be look at while tuning
%   - what is perforamnce? Too complex to decide. Just heuristics, which could not be portable to other systems
%   - The bigger the dataset, the worse to tune (long runtimes)
%   - Tuning after execution does not help, because the tuned values are not really reusable

\paragraph{Limitation of the Heterogeneous Approaches}

As outlined in the experiments chapter \ref{chap:experiments} tuning the load-balancer for the pre-balanced approach is necessary and non-trivial. The tuning problem could be handled by moving from balancing rows, whose execution time can vary greatly and is only approximately estimated, to single edges as tasks. The execution time of an edge as a task could still be estimated via the row length of the adjacency matrix this edge is on, but the run time impact of a single balanced task would be less. Therefore a more fine-grained balance would be achieved.

Still, tuning the parameters before execution leads to non-optimal balanced task distribution on CPU and GPU. For a better balance, multiple executions with different parameters approximate the best balance, or hardware performance characteristics are used for a better first execution balance. Multiple executions can be inefficient in real-world use cases, where the relevant result is already accessible after the first execution. Hardware characteristics, such as CPU performance, are hard to grasp with modern CPUs, where complex optimizations such as SIMD instruction set extensions and simultaneous multithreading (SMT) exist. Due to those optimizations, clock speed and core count of a CPU do not estimate the modern CPU's performance enough for smart load-balancing.

Due to the better execution times for single invocations and its hardware specification independence, the workstealing/dynamic scheduling approach is better suited for real-world use cases. Especially for large-scale datasets, where a single execution of the PC algorithm could run for days, the dynamic nature of the approach helps to optimize in the first run and therefore save time.

Disregarding the multithreading issues seen on the AC922 system, with a single thread of the CPU helping the heterogeneous computing variant, the execution does not seem very hardware reliant. Different CPUs with varying clock speeds and optimizations change the execution time results insignificantly. By looking at the fastest single-threaded workstealing experiments of both the Delos and the AC922 system, the AC922 single thread steals 108 edges in level 2 and 23 edges in level 3. In contrast, the single thread of the Delos steals 104 edges in level 2 and 25 edges in level 3. The execution times of both levels on both systems are nearly the same, with a deviation of around 40 ms.

Those single-threaded results also solidify that some multithreading issue is happening on the AC922 system, where additional threads slow the execution down. Since there should be a speedup by using more threads, this can be seen as a bug in the implementation and needs further investigation in the future work. Fixing this bug could boost the AC922 experiment's performance and lead to even better performance results.

Both introduced approaches in this thesis use the resources of the underlying heterogeneous system and speed up the PC algorithms execution. Further improvements for optimizing the approaches are looked into in the following future work chapter. Therefore, the solution proposed seems promising for future heterogeneous computing efforts in the Constraint-Based Causal Structure Learning context.
% Hardware impacts execution times alot (SMT, difference AC922 vs Delos)
% Has to be decided for each system if applying is worth
% Multithreading not efficient (could be fixed)
% 
% 
% 
% Topic has to be further look into but seems promising
% 
% 
% 