%!TEX root = ../thesis.tex
\chapter{Discussion}
\label{chap:discussion}
In this chapter, the results of the experiments in Chapter \ref{chap:experiments} are discussed. In the discussion, the results critic points are worked out. In the end, we answer the research question of the problem statement from Chapter \ref{chap:problem_statement}.

% Performance improvements could be shown for level 2 and 3
% But only in specific contexts
%   - doees not scale well
%   - perfect for situations, where the GPU gets to its (memory) limit
% 
As can be seen in Table \ref{table:ac922_vs_delos} performance improvements are achievable through introducing heterogeneous computing in the PC algorithm context. With a performance improvement of 16,5\% compared to the GPU-only variant, using the additional resources of the heterogeneous system is beneficial for the execution time.

With a deeper look at the performance, especially the different levels in Figure \ref{fig:levelwise_delos}, it is clear that the performance improvement depends on the level and dataset used. The examination of the dataset solidifies this result in Chapter \ref{chap:est_speedup}. Estimating level-wise potential speedup shows that levels 3 and 4 tend to have the most and strongest outliers of maximum iteration counts. This is reflected by the level-wise tests, where level 3 shows the most speedup in Figure \ref{fig:levelwise_delos}, Figure \ref{fig:levelwise_ac922} and even clearer in Figure \ref{fig:levelwise_scaled_delos}.

The speedup estimation showed an even better potential speedup for the 4th level. The slowdown seen in the benchmarks can be explained by a very sparse adjacency matrix as a starting point to level 4. The execution time of that level is low, with around 200 milliseconds for the TCGA-1000 dataset. The low execution time leads to the overhead of introducing heterogeneous computing that is higher than the benefits gained.

A preliminary speedup estimation of the dataset is beneficial for the heterogeneous computing approaches to decide which levels should be optimized.

Solidifying the dataset and level dependency, using a dataset with 10 000 variables slowed down the execution and resulted in a performance improvement for level 3. In level 3, the GPU-only variants execution duration is 2,467,224 ms, and the Delos workstealing execution duration is 2,107,416 ms.
The difference in level 3 is a 17,1\% improvement of performance by using both CPU and GPU. However, since every other level is slower with the workstealing execution, the whole execution time worsens. The 17,1\% performance improvement of level 3 is a solid increase, looking at the estimated possible speedup of around 30\%.

Level 2 is faster in the heterogeneous workstealing approach than in the GPU-only variant only for the 1000 variable dataset. There is a performance regression with the 10 000 variables dataset, hinting at a scaling problem.
This scaling issue is irrelevant as soon as the memory limit of the GPU is reached, at which the heterogeneous approach helps a lot with its 6,61x speedup in comparison to the GPU-only variant \ref{table:workst_delos_scaling}.

% Workstealing better than pre-balanced through dynamic nature
%   - does not have to be tuned
% Pre balanced is very hard to tune (might be even harder for different datasets)
%   - perforamnce characteristic has to be look at while tuning
%   - what is perforamnce? Too complex to decide. Just heuristics, which could not be portable to other systems
%   - The bigger the dataset, the worse to tune (long runtimes)
%   - Tuning after execution does not help, because the tuned values are not really reusable

\paragraph{Limitation of the Heterogeneous Approaches}

As outlined in the experiments Chapter \ref{chap:experiments} tuning the load-balancer for the pre-balanced approach is necessary and non-trivial. Tuning could be realized by moving from balancing rows, whose execution time can vary greatly and is only approximately estimated, to single edges as tasks. The execution time of an edge as a task could still be estimated via the row length of the adjacency matrix this edge is on, but the run time impact of a single balanced task would be reduced. Therefore a more fine-grained balance would be achieved.

Still, tuning the parameters before execution leads to non-optimal task distributions on CPU and GPU. For a better balance, multiple executions with different parameters approximate the best balance. Additionally, hardware performance characteristics support a better first execution balance estimation. Multiple executions can be inefficient in real-world use cases, where the relevant result is already accessible after the first execution. Hardware characteristics, such as CPU performance, are hard to grasp with modern CPUs, where complex features such as SIMD instruction set extensions and simultaneous multithreading (SMT) exist. Due to this complexity, clock speed and core count of a CPU do not estimate CPU's performance enough for smart load-balancing.

Due to the better execution times for single invocations and its hardware architecture independence, the workstealing/dynamic scheduling approach is better suited for real-world use cases. Especially for large-scale datasets, where a single execution of the PC algorithm could run for days, the dynamic approach allows for good performance in the first run and saves time.

Disregarding the multithreading issues seen on the AC922 system, with a single thread of the CPU helping the heterogeneous computing variant, the execution does not seem very hardware-dependent. The two different CPUs with varying clock speeds and optimizations changed the execution time results insignificantly. By looking at the fastest single-threaded workstealing experiments of both the Delos and the AC922 system, the AC922 single thread steals 108 edges in level 2 and 23 edges in level 3. In contrast, the single thread of the Delos steals 104 edges in level 2 and 25 edges in level 3. The execution times of both levels on both systems are nearly the same, with a deviation of around 40 ms.

The single-threaded results also solidify that some multithreading issue is happening on the AC922 system, where additional threads slow the execution down. Since there should be a speedup by using more threads, this can be seen as a bug in the implementation and needs further investigation in future work. Fixing this bug could boost the AC922 experiment's performance and lead to even better performance results.

Both approaches introduced in this thesis use the resources of the heterogeneous system and speed up the PC algorithms execution. Further improvements for optimizing the approaches are looked into in the following future work chapter. The solution proposed seems promising for future heterogeneous computing efforts in the Constraint-Based Causal Structure Learning context.
% Hardware impacts execution times alot (SMT, difference AC922 vs Delos)
% Has to be decided for each system if applying is worth
% Multithreading not efficient (could be fixed)
% 
% 
% 
% Topic has to be further look into but seems promising
% 
% 
% 