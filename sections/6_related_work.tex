%!TEX root = ../thesis.tex
\chapter{Related Work}
\label{chap:relwork}
In this chapter, related work that helped to build the heterogeneous computing approaches is studied. Work that is relevant in the context of the thesis is elaborated on and compared to the results in this thesis.

In the context of Constraint-Based Causal Structure Learning, specifically the PC algorithm, there is related work with a focus on accelerating and improving the performance through utilization of specific hardware resources. The most in-depth researched question is based on the general performance of CPU-based systems \cite{leFastPCAlgorithm2019, leParallelPCPackageEfficient2018, schmidtLoadBalancedParallelConstraintBased2019, colomboOrderIndependentConstraintBasedCausal2014,kalischEstimatingHighDimensionalDirected2007,scutariBayesianNetworkConstraintBased2017, madsenParallelAlgorithmBayesian2017,madsenParallelisationPCAlgorithm2015,nguyenMrPCCausalStructure2020}. 

Using the PC-stable variant of the PC algorithm by Colombo et al. \cite{colomboOrderIndependentConstraintBasedCausal2014}, Le et al. \cite{leFastPCAlgorithm2019, leParallelPCPackageEfficient2018} developed a modified PC-stable algorithm for parallel multi-core execution on the CPU by splitting each levels' CI-tests for an edge into tasks and process them in parallel. The algorithm is called ParallelPC and, as shown in the paper, does perform better than the single-threaded PC-stable algorithm. The thesis CPU execution of both approaches follows the task split approach to get a similar parallelism speedup. Both the papers and the thesis' versions are possible because of the order independence of the PC-stable algorithm.

While the tasks of the ParallelPC are statically scheduled onto the cores of the CPU, an imbalance can occur, which is described in Schmidt et al. \cite{schmidtLoadBalancedParallelConstraintBased2019}. The load-balanced version of the PC-stable algorithm described in the paper uses a dynamic mapping technique to prevent an imbalanced execution. A central task queue is created for each level, where workers running on each core can take their next task and process that task. The thesis' CPU execution uses OpenMP's dynamic task scheduling \cite{breshearsArtConcurrencyThread2009}. The workstealing approaches' heterogeneous dynamic scheduling is based on the dynamic load-balancing idea of Schmidt et al. \cite{schmidtLoadBalancedParallelConstraintBased2019}, but avoids the task queue's global system-wide singleton and introduces task-specific synchronization primitives to minimize communication between workers over the CPU-GPU interconnect.

Based on the CPU based PC algorithm related work, approaches for GPU accelerated variants \cite{schmidtOrderIndependentConstraintBasedCausal2018,zarebavaniCuPCCUDAbasedParallel2018} emerged. Both papers base their work on the PC algorithm described in Colombo et al. \cite{colomboOrderIndependentConstraintBasedCausal2014} where tasks are scheduled statically. The advantage of a GPU parallelized variant is that the developer of the algorithm does not need to define scheduling. The GPU runtime decides how tasks are scheduled \cite{olmedoDissectingCUDAScheduling2020}. Using the GPU scheduler, all tasks are placed on the GPU, and the GPU handles execution. The same approach is used for the GPU implementation in this thesis, but some tasks are filtered in the pre-balanced approach before placing them on the GPU. In this thesis the implementation of Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} is followed for the GPU implementation.

The related work for the PC algorithm is highly relevant to the work done in this thesis. The base implementation is heavily influenced by them and helped to form the heterogeneous computing variant \cite{colomboOrderIndependentConstraintBasedCausal2014,schmidtOrderIndependentConstraintBasedCausal2018,schmidtLoadBalancedParallelConstraintBased2019,leFastPCAlgorithm2019,leParallelPCPackageEfficient2018}.

For an entry into the heterogeneous computing topic surveys such as heterogeneous computing techniques by Mittal and Vetter \cite{mittalSurveyCPUGPUHeterogeneous2015} are a good starting point, where related work for different scheduling techniques such as static and dynamic scheduling is introduced. Other related work can be found for a more detailed introduction into heterogeneous systems like Khokar et al. \cite{khokharHeterogeneousComputingChallenges1993} where Fosters' Methodology \cite{fosterDesigningBuildingParallel1995} for parallel algorithm design is considered, which is also used in this thesis, and the heterogeneous computing environment impact is looked into. Parts of the scheduling, synchronization, and interconnect thoughts of Khokar et al. \cite{khokharHeterogeneousComputingChallenges1993} influenced the thesis' approach and heterogeneous computing considerations.

In a paper by Carabano et al. \cite{carabanoExplorationHeterogeneousSystems2013}, differences between the CPU, GPU, and other processing units that are commonly used in heterogeneous systems are elaborated and clarified. The main aspects of the CPU, which is said to use best on serial tasks, and the GPU, whose properties are aligned to parallelism, are further dealt with in this thesis. Such work supports the argumentation of both the GPUs' and CPUs' detailed properties, such as energy efficiency and performance, which are the basis of heterogeneous systems. With a broader look at CPU and GPU properties and capabilities, better scheduling can be achieved.

Zhang et al. \cite{zhangDynamicStaticLoad1991}, and Topcuoglu et al. \cite{topcuogluPerformanceeffectiveLowcomplexityTask2002} explain the differences between static and dynamic scheduling, which formed the basis for the two different approaches portrait in this thesis. Specifically, in Zhang et al. \cite{zhangDynamicStaticLoad1991} dynamic and static scheduling and their effects in the context of NUMA shared memory multiprocessing is examined. The analysis presented in their work helped to explore effective scheduling strategies on heterogeneous NUMA systems and develop an efficient parallel program. The paper and this thesis find the same results: static scheduling is generally faster, but it has to be adjusted, and dynamic scheduling while having the scheduling overhead is more adaptable to a specific system.
The pre-balanced approach, which is inspired by static scheduling, balances tasks before level execution. However, it still adds its overhead to the total execution time, and the dynamic scheduling approach is, therefore, more promising.

In every heterogeneous computing scenario, tasks have to be scheduled to different processing units. Therefore related work based on parallel tasks and load-balancing in the context of heterogeneous computing is also relevant for this thesis. There is related work for a general view on the topics of scheduling and load balancing in heterogeneous computing \cite{cirouTripletClusteringScheduling2001, abdelkaderDynamicTaskScheduling2012,binottoDynamicReconfigurableLoadbalancing2010,galindoDynamicLoadBalancing2008,kopetzRealTimeScheduling1997,kwokStaticSchedulingAlgorithms1999,momcilovicDynamicLoadBalancing2014,singhSurveyStaticScheduling2015}. In the papers by Cirou et al. \cite{cirouTripletClusteringScheduling2001} and Abdelkader et al. \cite{abdelkaderDynamicTaskScheduling2012} dynamic task scheduling algorithms for heterogeneous systems are explored. The Heterogeneous Earliest Finish Time (HEFT) and the Triplet clustering algorithm are two important algorithms \cite{abdelkaderDynamicTaskScheduling2012}. Still, for best performance results, a domain-specific parallelization developed for the PC-stable algorithm showed the best effect using a task length estimation by inspecting adjacency row lengths.

By studying the related work on load balancing, a workstealing scheduler seemed best fitting for the PC algorithm use-case. It is very close to the basic static scheduling (pre-balanced) approach. In literature about both dynamic and static scheduling, efficient algorithms are found for scheduling based on different factors of heterogeneous systems: The utilization of the processing units is an essential factor and helped to align both approaches to the heterogeneous system requirement. 

Workstealing as a concept evolved from being used in CPU thread scheduling \cite{blumofeSchedulingMultithreadedComputations1999} into being relevant in parallel computing as well \cite{letzWorkStealingScheduler2010,mattheisWorkStealingStrategies2012,prellEmbracingExplicitCommunication2016}. As said in Blumofe et al. \cite{blumofeSchedulingMultithreadedComputations1999}, the workstealing scheduler minimizes the amount of necessary communication, which reduces the costly the interconnect usage. The idea of the workstealing approach is based on this work.

