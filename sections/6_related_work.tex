%!TEX root = ../thesis.tex
\chapter{Related Work}
In this chapter related work that helped to build the heterogeneous computing approaches is looked into. Work that is relevant in the thesis context is elaborated and compared to the work done in this thesis.

In the context of Constraint Based Causal Structure Learning or more specifically the PC algorithm, there is related work with a focus on accelerating and improving the performance through utilization of the hardware. The most in-depth researched question is based on the general performance of CPU-based systems \cite{leFastPCAlgorithm2019, leParallelPCPackageEfficient2018, schmidtLoadBalancedParallelConstraintBased2019, colomboOrderIndependentConstraintBasedCausal,kalischEstimatingHighDimensionalDirected2007,scutariBayesianNetworkConstraintBased2017, madsenParallelAlgorithmBayesian2017,madsenParallelisationPCAlgorithm2015,nguyenMrPCCausalStructure2020}. 

Using the PC-stable variant of the PC algorithm by Colombo et al. \cite{colomboOrderIndependentConstraintBasedCausal}, Le et al. \cite{leFastPCAlgorithm2019, leParallelPCPackageEfficient2018} developed a modified PC-stable algorithm for parallel multi-core execution on the CPU by splitting each levels CI tests for an edge into tasks and process them in parallel. The algorithm is also called ParallelPC and as can be seen in the paper, does perform better than the single threaded PC-stable algorithm. The thesis CPU execution of both approaches follows this task split for aiming to get a similar parallelism speed. Both the papers and the thesis' versions are possible because of the order independence of the PC-stable algorithm.

While the tasks of the ParallelPC are statically scheduled onto the cores of the CPU, an imbalance can occur which is described in Schmidt et al. \cite{schmidtLoadBalancedParallelConstraintBased2019}. The load-balanced version of the PC-stable algorithm described in this paper uses a dynamic mapping technique to prevent an imbalanced execution. A central task queue is created in each level, where workers running on each core can take their next task and process that task. The thesis' CPU side execution uses OpenMP's possibility of dynamic task scheduling \cite{breshearsArtConcurrencyThread2009}. The workstealing approaches heterogeneous dynamic scheduling is based on the dynamic load-balancing idea of Schmidt et al. \cite{schmidtLoadBalancedParallelConstraintBased2019}, but avoids the task-queue's global system-wide singleton and introduces task specific synchronization primitives, to minimize communication between workers over the CPU-GPU interconnect.

Based on the work of the CPU based PC algorithm approaches, related work for a few GPU accelerated variants \cite{schmidtOrderIndependentConstraintBasedCausal2018,zarebavaniCuPCCUDAbasedParallel2018} exists. Both papers base their work on the PC algroithm described in Colombo et al. \cite{colomboOrderIndependentConstraintBasedCausal} where tasks are scheduled statically. The advantage of a GPU parallelized variant is, that the developer of the algroithm does not handle scheduling. The GPU runtime decides, how tasks are scheduled \cite{olmedoDissectingCUDAScheduling2020}. Using the GPU scheduler, all tasks are placed on the GPU and execution is handled by the GPU. The same approach is used in the GPU side execution of this thesis but in the pre-balanced approach some tasks are filtered before placing them on the GPU. In this thesis the implementation of Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} is followed for the GPU side execution.

The related work for the PC algorithm is highly relevant to the work done in this thesis. The base implementation is heavily influenced by them and helped to form the heterogeneous computing variant.

For an entry into the heterogeneous computing topic surveys such as heterogeneous computing techniques by Mittal and Vetter \cite{mittalSurveyCPUGPUHeterogeneous2015}, where work for different scheduling techniques such as static and dynamic scheduling are introduced. Based on this, other related work can be found for a more detailed introduction into heterogeneous systems like Khokar et al. \cite{khokharHeterogeneousComputingChallenges1993} where Fosters' Methodology for parallel algorithm design is considered which is also used in this thesis and the heterogeneous computing environment impact is looked into. Parts of the scheduling, synchronization and interconnect thoughts of Khokar et al. \cite{khokharHeterogeneousComputingChallenges1993} influenced the thesis' approach and heterogeneous computing considerations.

In the paper by Carabano et al. \cite{carabanoExplorationHeterogeneousSystems2013}, differences between the CPU, GPU and other processing units, that are commonly used in hetterogeneous system, are elaborated and clarified. The main aspects of the CPU, which is said to used best on serial tasks, and the GPU, whichs properties are many aligned to parallelism, are further dealt with in this thesis. Such work supports the argumentation of both the GPUs and CPUs detailed properties, such as energy efficiency and performance, which are the basis of heterogeneous systems. With the broader look at CPU and GPU capabilities, a better scheduling can be achieved.

Zhang et al. \cite{zhangDynamicStaticLoad1991} and Topcuoglu et al. \cite{topcuogluPerformanceeffectiveLowcomplexityTask2002} explain the differences between static and dynamic scheduling, which formed the two different approaches based on those scheduling variants. Specifically in Zhang et al. \cite{zhangDynamicStaticLoad1991} dynamic and static scheduling and their effects in the context of NUMA shared memory multiprocessing is examined. The results of that analysis helped exploring effective scheduling strategies on heterogeneous NUMA systems and developing an effienect parallel program. The same obtained insights are made in both this thesis and the papers analysis, such as static scheduling is generally faster but has to be adjusted and dynamic scheduling while having the scheduling overhead, is more adaptble to the system.
The pre-balanced approach, which is inspried by static scheduling, does the balancing before level execution, but it still adds its overhead to the total execution time and the dynamic scheduling approach is therefore more promising.

In every heterogeneous computing, tasks have to be scheduled on different processing units, therefore related work based on the parallel task and tis load balancing in context of heterogeneous computing is also relevant for this thesis. There is work for the general view on the heterogeneous computing scheduling and load balancing topic \cite{cirouTripletClusteringScheduling2001, abdelkaderDynamicTaskScheduling2012,binottoDynamicReconfigurableLoadbalancing2010,galindoDynamicLoadBalancing2008,kopetzRealTimeScheduling1997,kwokStaticSchedulingAlgorithms1999,momcilovicDynamicLoadBalancing2014,singhSurveyStaticScheduling2015}. In the papers by Cirou et al. \cite{cirouTripletClusteringScheduling2001} and Abdelkader et al. \cite{abdelkaderDynamicTaskScheduling2012} dynamic task scheduling algorithms for hetergoeneous systems are explored. The Heterogeneous Earliest Finish Time (HEFT) and the Triplet clustering algorithm are two important algorithms in this topic, that both show promising results. Still for best performance results a domain specific parallelization developed for the PC-stable algorithm showed the best effect using a task length estimation by inspecting adjacency row lengths.

By inspecting the work on load balancing, the workstealing scheduler seemed best fitting for the PC algorithm use-case, next to the basic static scheduling (pre-balanced) approach. In literature about both dynamic and static scheduling, efficient algorithms are found for scheduling, that look into different factors of hetergeneous systems, where the utilization of the processing units is an important factor and helped forming both approaches aligned to the heterogeneous system requirement. 

Workstealing as a concept evolved from being used in CPU thread scheduling \cite{blumofeSchedulingMultithreadedComputations1999} into being relevant in parallel computing as well \cite{letzWorkStealingScheduler2010,mattheisWorkStealingStrategies2012,prellEmbracingExplicitCommunication2016}. As said in Blumofe et al. \cite{blumofeSchedulingMultithreadedComputations1999}, the workstealing scheduler reduces necessary communication, which limits the interconnect usage. The idea of the workstealing approach is based on this work.

