%!TEX root = ../thesis.tex
\chapter{Heterogeneous Constraint-Based Causal Structure Learning}
Parallelizing the PC algorithm on heterogeneous systems starts with choosing a task size that is flexible enough for the different processing units executing the workload. As a basis, the implementation of the PC algorithm from the paper \cite{schmidtOrderIndependentConstraintBasedCausal2018} is used since efforts for parallelizing the PC algorithm have already been done to utilize the throughput of the GPU. After that, the chosen task must be balanced on the CPU and the GPU using a balancing algorithm. The balancing algorithm has to be adaptable to the environmental parameters of the system it runs on.

\section{Parallelize the PC Algorithm}
Parallelizing workloads is the basis of parallel programming. A well-known design methodology used to architect a workload such that it can be run in parallel efficiently is Foster's Methodology \cite{fosterDesigningBuildingParallel1995}. Foster's Methodology is also often used for distributed-memory systems. Since GPU-CPU heterogeneous systems are distributed-memory systems, I will apply this method to parallelize the workload. Foster's Methodology uses four distinct stages, which are applied to the workload one after another:

\begin{enumerate}
    \item Partitioning
    \item Communication
    \item Agglomeration 
    \item Mapping
\end{enumerate}

The workload is decomposed into the smallest possible tasks in the Partitioning stage. For a correct execution, the Communication stage defines the necessary coordination of task execution. Both the first and the second stage of Foster's Methodology are essential for the search of concurrency and scalability in the given workload while achieving algorithm correctness. In the Agglomeration stage, multiple tasks are agglomerated, better to align the computation and data to the underlying system. The Mapping stage can be seen as analog to scheduling and maps the agglomerated tasks onto the different processing units.

\subsection{Partitioning and Communication}
In the Partitioning stage, opportunities for parallel execution are exposed. A good partition keeps computational parts and their respective data together to minimize the later communication efforts. Using fine-grained decomposition of the workload can be done in two different ways. Domain decomposition defines small data fragments first and specifies the computation later. Functional decomposition splits the computation into disjoint tasks and handles the data after. Domain decomposition is more appropriate than functional decomposition if there is a significant data overlap.
More generalized functional decomposition can also be seen as splitting a multivariate function $f(x_1,x_2,...,x_n)$ into multiple functions ${g_1,g_2,...g_m}$ such that $f(x_1,x_2,...,x_n) = \theta(g_1(x_1,x_2,...,x_n),g_2(x_1,x_2,...,x_n),...g_m(x_1,x_2,...,x_n))$ where $\theta$ is another function combining the results. Every function $g_n$ gets the same input as $f$. If the functional approach is applied to some algorithmic problem, loops with independent iterations can be mapped to this mathematical model and make parallelizing single iterations possible. If iterations of such loops mapped to a task do have side effect communication or synchronization must be done.
The Communication stage specifies links between data consumers and producers. Domain decomposition problems can have more complex communication infrastructures due to data dependencies. Distributing computation and communication is more efficient than centralizing the algorithm through some central manager communicating with every task. Independent and balanced tasks communication-wise are more capable of performant execution.

Using the PC-stable GPU-accelerated Algorithm proposed in the paper by Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} makes handling the Partitioning and Communication step straightforward. The algorithm is based on the gaussian distribution model and handles only the CI-Test for normal distributed data. For simplicity purposes, the thesis implementation incorporates the same approach and looks into other CI-Tests later.

\begin{algorithm}
    \caption{Adjacency search of PC-stable algorithm with gaussian distribution model \cite{schmidtOrderIndependentConstraintBasedCausal2018, colomboOrderIndependentConstraintBasedCausal}}
    \label{alg:pcstable_gaussian}
    \begin{algorithmic}[1]
    \Require Vertex set $V$, correlation matrix $Cor$
    \Ensure Estimated skeleton $C$, separation sets \textbf{Sepset}
    \State with fully connected skeleton $C$ and $l = -1$
    \Repeat \label{alg:pcstable_gaussian:level_loop}
        \State $l=l+1$
        \For{all Variables $V_i$ in $C$}
            \State Let $a(V_i) = adj(C,V_i)$
        \EndFor
        \Repeat \label{alg:pcstable_gaussian:edge_loop}
            \State Select pairs $(V_i,V_j)$ adjacent in $C$ with $|a(V_i)\backslash\{V_j\}| \geq l$
            \Repeat \label{alg:pcstable_gaussian:sepset_loop}
                \State Choose separation set $S \subseteq a(V_i ) \backslash \{V_j\}|$ with $| S | = l$
                \If{$p(V_i,V_j|S) \leq \alpha$}
                    \State Delete edge $V_i - V_j$ from $C$
                    \State Set Sepset(i,j) = Sepset(j,i) = S
                \EndIf
            \Until{edge $V_i - V_j$ is deleted in $C$ or all $S \subseteq a(V_i) \backslash \{V_j\}$ with $|S| = l$ have been chosen}
        \Until{all adjacent vertices $V_i$, and $V_j$ in $C$ such that $|a(V_i)\backslash\{V_j\}| \geq l$ have been considered}
    \Until{each adjacent pair $V_i$, $V_j$ in $C$ satisfy $|a(V_i)\backslash\{V_j\}| \geq l$}
    \State \textbf{return} C, Sepset
    \end{algorithmic}
\end{algorithm}

As can be seen in algorithm \ref{alg:pcstable_gaussian} the main input next to the vertices is the correlation matrix for those vertices. The correlation matrix is necessary for the gaussian-based CI-Test. The algorithm \ref{alg:pcstable} is a CI-Test generalized version of the algorithm \ref{alg:pcstable_gaussian}.
As already stated, loops can be split into tasks if their iteration execution flow is independent of the input of other iterations. This statement does not apply to the repetition in line \ref{alg:pcstable_gaussian:level_loop} because every level depends on its predecessor's results. Synchronization must occur between each level.
Only two loops are independent iteration-wise. The proposed algorithm can be split between different edge as in line \ref{alg:pcstable_gaussian:edge_loop} or between different separation sets as in line \ref{alg:pcstable_gaussian:sepset_loop}. The smaller and, therefore, the correct task for the Partitioning phase is a separation set split.

In the proposed GPU-accelerated algorithm variant of Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} level 0 is split into edges, because there is only one separation set $S \subseteq a(V_i ) \backslash \{V_j\}$ with $| S | = l$ which is the empty set ${}$. Therefore only one CI-Test has to be done per edge.
Level $1,2 ... m$ all split into per separation set tasks in the Partitioning stage. Performance improvements can be made by having an early stop criterion such as a deleted edge, but this requires additional communication between tasks. Data that is shared can not be properly aligned to the tasks. Task data access is nearly randomized on the input correlation in level 1 and following because the separation sets can be build from all existing vertices. 

%- Partitioning und Communication genauer erklären
%- GPU paper basis
%    - partitioning schon getan
%    - Task ist eine edge
%    - Wiederverwenden
%    - Durch parallelisierung auf processing units weiterhin diese partitionierung
    
\subsection{Agglomeration}
The Agglomeration stage uses the correct algorithm produced by the Partitioning and Communication stages and specializes that for the execution environment. Tasks are agglomerated so that their execution is efficient. In the purpose of this thesis, the execution environment is a heterogeneous system containing CPUs and GPUs as processing units. The resulting number of tasks of the Agglomeration stage can be greater than the number of processing units. The subsequent conflicting decisions are necessary to guide through the Agglomeration stage:

\begin{enumerate}
    \item Reduce communication cost through agglomerated computation
    \item Preserve flexibility concerning the later Mapping stage and maintain parallelism
\end{enumerate}

With regards to the GPU implementation of the paper by Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} which splits the algorithm into separation set tasks and parallelizes those on the GPU, agglomerating those tasks into larger task sets can be done by 
taking the next bigger loop into consideration. The per edge iteration in line \ref{alg:pcstable_gaussian:edge_loop} is suitable for combining multiple tasks because it is also not dependent on the other edge iterations while also being able to be parallelised even more on the underlying processing unit, that executes the edge.

The PC-stable algorithm is commonly used in high-dimensional settings with thousands of variables \cite{nagarajanFunctionalRelationshipsGenes2010}. An edge between each vertex representing a variable can exists so that $|E| \leq |V|^{2}$. Therefore millions of edges might exists in a level, so that agglomerating an edge as a task is not feasible for a GPU-CPU heterogeneous system.

%TODO Maybe add illustration for both graphs (adj list vs matrix)
Looking further into the GPU side implementation leads to another possible task that agglomerates multiple edges. The adjacency set that is populated in the algorithm can be implemented using an adjacency matrix or adjacency lists. An adjacency matrix is a standard graph representation that encodes every edge between two vertices $V_1, V_2$ as a field in the matrix at some position to $A_G(V_1, V_2) = \{0,1\}$ where 1 represents an existing and 0 a deleted edge. This representation is constant in its memory usage, and accessing a random element can be done in O(1). At every level, this representation is used to delete edges from the graph. Because of the order independence nature of the algorithm, another copy of the graph exists, which is only read from and the changes of the graph used for deletion are migrated to this graph at the end of each level.
Adjacency lists, on the other hand, are memory-efficient because only existing edges are saved. Every row in a matrix represents a list of vertices that are connected to the vertex of the row. So the edge count of some vertex is the length of the vertex row and for performance reasons, the length of each row is calculated before each level and appended to the row. This representation is only used for reading existing edges in the level. Since every edge has to be processed, an index-based approach is used and with that accessing a single edge can still be done in $O(1)$.
Between each level, the graph that contains the deleted edges of the last level has to be moved to the adjacency list representation used for reading existing edges. This step is called the compacting step \cite{zarebavaniCuPCCUDAbasedParallel2018}.

%TODO Add Gpu pseudocode for better understanding of iteration length etc

% - Agglomeration genauer erklären
% - Durch adjazenz matrix basis bietet sich row basiert an
% - row bundelt mehrere edges und ist damit etwas geeigneter als Task
% - einzelne edge können weiterhin parallel bearbeitet werden auf den PUs
% - genauer die ausführung der GPU anschauen
%  - GPU als basis -> wie geschwindigkeit der GPU erhöhen
%  - serieller/paralleler anteil
%  - GPU Block besteht meist aus 64 Threads
%  - (Beispiel code für ausführung)
%  - Iterationsanzahl bestimmt längste thread ausführung
%  - iterationsanzahl ist abhängig von row length
%  - Testanzahl berechnen aus und daraus iteration (wie)
%  - Diagram Iterations länge
%  - erklären wie dadurch der serielle anteil gekürzt werden kann
 
\subsubsection{Compact Adjacency Lists}
% - durch compact step werden adjacency lists zwischen leveln berechnet
% - länge der liste ist anzahl an edges in einer row
% - letztes element genutzt als längenspeicher
% - grafik zur verbesserten darstellung
% - wird auch in \cite{zarebavani_cupc_2018} verwendet

\subsection{Mapping}
The last stage of Foster's Methodology, called Mapping, applies the tasks created in the Agglomeration stage to the given system. This stage is vital for heterogeneous systems because the underlying system has to be utilized efficiently. Utilizing heterogeneous systems means maximizing CPU and GPU usage and minimizing inter-processor communication. Minimizing the inter-processor communication includes the connection between GPU and CPU and the core/thread connections in the GPU and CPU.
Mapping the tasks on the system PUs is also called scheduling. There are two common ways of scheduling tasks: Static and dynamic scheduling \cite{kopetzRealTimeScheduling1997}.
A scheduler is called static if it makes its scheduling decisions pre-execution. The static scheduler needs prior knowledge of the capabilities each processing unit has. The static scheduler needs to know the performance implications and characteristics of the given task set so that the tasks are balanced optimally on the system. Therefore a static scheduler that does not schedule on compile time is also called a static load-balancer.
A scheduler is called dynamic if scheduling decisions are done at run-time. Due to the run-time decision, the current system state can be taken into account. Prior knowledge about the processing unit capabilities is not necessary. Knowledge about the task set and its performance implications is still helpful for optimally balancing tasks onto the heterogeneous PUs.
A special case of a dynamic scheduling algorithm is the work-stealing scheduler. While one processing unit executes the whole task set, the others "steal" tasks at run-time from the executing PU to minimize the workload gradually.
% - Mapping genauer erklären
% - Scheduling spezifizieren
%     - Static/Dynamic scheduling
%     - static = prebalanced
%         vor nachteile
%     - dynamic 
%         vor nachteile
% - Two approaches pre balanced and workstealing

\section{Heterogeneous PC Algorithm Approaches}
Using the insights gathered from Foster's Methodology applied to the heterogeneous PC-stable algorithm, two approaches arise. First, the static scheduling approach that balances rows pre-execution on the heterogeneous system. This approach is called the pre-balanced approach. The second approach uses dynamic scheduling by work-stealing rows from an executing processing unit.
\subsection{Pre-Balanced Approach}
% - Static scheduling ansatz
% - Load balancer schiebt längste rows auf CPU
% ...
% - Viel Memory/ communication -> nvprof?
% - Limitieren durch Migrierungsschritt der CPU

\subsection{Workstealing Approach}
% - Dynamic scheduling
% - Workstealing erklären (herkunft etc)
% - GPU läuft normal durch
% - CPU nimmt sich längste rows als erstes vor und arbeitet von hinten ab
% - regelmäßiges überprüfen ob schon fertig
% - vermehrte kommunikation

\section{Implementation}

\subsection{Architecture}
% - could be also integrated into the Approach sections
% - one state containing unified memory data
% - Executor abstraction
% - Task as a struct containing row index and how many rows
% - Load balancer invoked only in prebalanced case
% - else GPU has all rows
% - Workstealing approach guards
% - Maybe show some code