%!TEX root = ../thesis.tex
\chapter{Heterogeneous Constraint-Based Causal Structure Learning}
\label{chap:hetcsl}
Parallelizing the PC algorithm on heterogeneous systems starts with choosing a task size that is flexible enough for the different processing units executing the workload. The PC algorithm has been executed efficiently on different execution units \cite{leParallelPCPackageEfficient2018,leFastPCAlgorithm2019,schmidtOrderIndependentConstraintBasedCausal2018,zarebavaniCuPCCUDAbasedParallel2018}. Heterogeneous systems can contain very diverse execution units, which could be all combined for execution, in this thesis the focus lays on CPU and GPU combined execution. As a basis, the implementation of the PC algorithm from the paper \cite{schmidtOrderIndependentConstraintBasedCausal2018} is used where efforts for parallelizing the PC algorithm have already been made to utilize the throughput of the GPU. After that, the chosen task must be balanced on the CPU and the GPU using a balancing algorithm. The balancing algorithm has to be adaptable to the environmental parameters of the system it runs on. In this chapter, the PC algorithm is parallelized for execution on a heterogeneous CPU-GPU system by applying state-of-the-art parallelization methods. An optimized and flexible task size is found using such a method, and optimal balancing of the task is elaborated in this chapter.

\section{Parallelizing the PC Algorithm}
Parallelizing workloads is the basis of parallel programming. A well-known design methodology used to architect a workload such that it can be run in parallel efficiently is Foster's Methodology \cite{fosterDesigningBuildingParallel1995}. Foster's Methodology is also often used for distributed-memory systems. A GPU-CPU heterogeneous system is a distributed-memory system because modern high-performance computing systems include dedicated memory for each processing unit and GPU memory is different from CPU memory. CPU assigned memory is mainly optimized for latency to support the nature of a CPU. GPU Memory, on the other hand, is optimized for throughput for the same reasons.

Foster's Methodology uses four distinct stages, which are applied to the workload one after another:

\begin{enumerate}
    \item Partitioning
    \item Communication
    \item Agglomeration 
    \item Mapping
\end{enumerate}

The workload is decomposed into the smallest possible tasks in the Partitioning stage. For a correct execution, the Communication stage defines the necessary coordination of task execution. Both the first and the second stages of Foster's Methodology are essential for the search of concurrency and scalability in the given workload while achieving algorithmic correctness. In the Agglomeration stage, multiple tasks are agglomerated to align the computation and data to the underlying system. The Mapping is analog to scheduling and maps the agglomerated tasks onto the different processing units. In the following chapters, Foster's Methodology is applied to the Order independent PC-stable algorithm.

\subsection{Partitioning and Communication}
In the Partitioning stage, opportunities for parallel execution are exposed. A good partition keeps computational parts and their respective data together to minimize the later communication efforts. Using fine-grained decomposition of the workload can be done in two different ways. Domain decomposition defines small data fragments first and specifies the computation later. Functional decomposition splits the computation into disjoint tasks and handles the data after. Domain decomposition is more appropriate than functional decomposition if there is a significant data overlap.
More generalized functional decomposition can also be seen as splitting a multivariate function $f(x_1,x_2,...,x_n)$ into multiple functions ${g_1,g_2,...g_m}$ such that $f(x_1,x_2,...,x_n) = \theta(g_1(x_1,x_2,...,x_n),g_2(x_1,x_2,...,x_n),...g_m(x_1,x_2,...,x_n))$ where $\theta$ is another function combining the results. Every function $g_n$ gets the same input as $f$. If the functional approach is applied to some algorithmic problem, loops with independent iterations can be mapped to this mathematical model and make parallelizing single iterations possible. If iterations of such loops, mapped to a task, include side effects, communication or synchronization must be done.
The Communication stage specifies links between data consumers and producers. Domain decomposition problems can have more complex communication infrastructures due to data dependencies. Distributing computation and communication is more efficient than centralizing the algorithm through some central manager communicating with every task. Independent and balanced tasks are more capable of performant execution communication-wise.

Using the PC-stable GPU-accelerated Algorithm proposed in the paper by Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} makes handling the Partitioning and Communication step straightforward. The algorithm is based on the gaussian distribution model and handles only the CI-Test for multivariate normal distributed data. For simplicity purposes, the thesis implementation incorporates the same approach and looks into other CI-Tests later.

\begin{algorithm}
    \caption{Adjacency search of PC-stable algorithm with Gaussian distribution model \cite{schmidtOrderIndependentConstraintBasedCausal2018, colomboOrderIndependentConstraintBasedCausal2014}}
    \label{alg:pcstable_gaussian}
    \begin{algorithmic}[1]
    \Require Vertex set $V$, correlation matrix $Cor$
    \Ensure Estimated skeleton $C$, separation sets \textbf{Sepset}
    \State with fully connected skeleton $C$ and $l = -1$
    \Repeat \label{alg:pcstable_gaussian:level_loop}
        \State $l=l+1$
        \For{all Variables $V_i$ in $C$}
            \State Let $a(V_i) = adj(C,V_i)$ \label{alg:pcstable_gaussian:adj}
        \EndFor
        \Repeat \label{alg:pcstable_gaussian:edge_loop}
            \State Select pairs $(V_i,V_j)$ adjacent in $C$ with $|a(V_i)\backslash\{V_j\}| \geq l$
            \Repeat \label{alg:pcstable_gaussian:sepset_loop}
                \State Choose separation set $S \subseteq a(V_i ) \backslash \{V_j\}$ with $| S | = l$
                \If{$p = CITest(V_i,V_j,S,Cor) \land p \leq \alpha$}
                    \State Delete edge $V_i - V_j$ from $C$
                    \State Set Sepset(i,j) = Sepset(j,i) = S
                \EndIf
            \Until{edge $V_i - V_j$ is deleted in $C$ or all $S \subseteq a(V_i) \backslash \{V_j\}$ with $|S| = l$ have been chosen}
        \Until{all adjacent vertices $V_i$, and $V_j$ in $C$ such that $|a(V_i)\backslash\{V_j\}| \geq l$ have been considered}
    \Until{each adjacent pair $V_i$, $V_j$ in $C$ satisfy $|a(V_i)\backslash\{V_j\}| \geq l$}
    \State \textbf{return} C, Sepset
    \end{algorithmic}
\end{algorithm}

As can be seen in Algorithm \ref{alg:pcstable_gaussian} the main input next to the vertices is the correlation matrix for those vertices. The correlation matrix is necessary for the gaussian-based CI-Test. Algorithm \ref{alg:pcstable} is a CI-Test generalized version of algorithm \ref{alg:pcstable_gaussian}.
As already stated, loops can be split into tasks if their iteration execution flow is independent of the input of other iterations. This statement does not apply to the repetition in line \ref{alg:pcstable_gaussian:level_loop} because every level depends on its predecessor's results. Synchronization must occur between each level.
Only two loops are independent iteration-wise. The proposed algorithm can be split between different edges as in line \ref{alg:pcstable_gaussian:edge_loop} or between different separation sets as in line \ref{alg:pcstable_gaussian:sepset_loop}. The smaller and, therefore, the better fitting task for the Partitioning phase is a separation set split.

In the proposed GPU-accelerated algorithm variant of Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} level 0 is split into edges, because there is only one separation set $S \subseteq a(V_i ) \backslash \{V_j\}$ with $| S | = l$ which is the empty set $\{\}$. Therefore only one CI-Test has to be done per edge. Concretely this means each edge is a single task in level 0.

In level $1,2 ... m$, every separation set that has to be tested in combination with an edge is a task. Performance improvements can be made by having an early stop criterion such as a deleted edge so that other separation sets for that edge can be ignored, but this requires additional communication between tasks.

Data that is shared can not be appropriately aligned to the tasks, which means all tasks have to access the data at any time. Task data access in the PC-stable algorithm is nearly randomized on correlation matrix in level 1 and following because the separation sets can be build from all existing vertices. 

%- Partitioning und Communication genauer erklären
%- GPU paper basis
%    - partitioning schon getan
%    - Task ist eine edge
%    - Wiederverwenden
%    - Durch parallelisierung auf processing units weiterhin diese partitionierung
    
\subsection{Agglomeration}
\label{chap:agglo}
The Agglomeration stage uses the correct algorithm produced by the Partitioning and Communication stages and specializes the algorithm for the execution environment. Tasks are agglomerated so that their execution is efficient. For this thesis, the execution environment is a heterogeneous system containing CPUs and GPUs as processing units. The resulting number of tasks of the Agglomeration stage can be greater than the number of processing units. The subsequent conflicting decisions are necessary to guide through the Agglomeration stage:

\begin{enumerate}
    \item Reduce communication cost through agglomerated computation
    \item Preserve flexibility concerning the later Mapping stage and maintain parallelism
\end{enumerate}

With regards to the GPU implementation of the paper by Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} which splits the algorithm into separation set tasks and parallelizes those on the GPU, agglomerating those tasks into larger task sets can be done by considering the next bigger loop. The per edge iteration in line \ref{alg:pcstable_gaussian:edge_loop} is suitable for combining multiple tasks because it is also not dependent on the other edge iterations while also being able to be parallelized even more on the underlying processing unit that executes the edge.

The PC-stable algorithm is commonly used in high-dimensional settings with thousands of variables \cite{nagarajanFunctionalRelationshipsGenes2010}. An edge between each vertex representing a variable can exists so that $|E| \leq |V|^{2}$. Therefore millions of edges might exist per level so that agglomerating an edge as a task is not feasible for a GPU-CPU heterogeneous system.

%TODO Maybe add illustration for both graphs (adj list vs. matrix)
\paragraph{Adjancency Lists in the PC-stable Algorithm}
Looking further into the GPU side implementation leads to another possible task that agglomerates multiple edges. The adjacency set populated in the algorithm in line \ref{alg:pcstable_gaussian:adj} of the PC-stable algroithm can be implemented using an adjacency matrix or adjacency list. An adjacency matrix is a standard graph representation that encodes every edge between two vertices $V_1, V_2$ as a field in the matrix at some position to $A_G(V_1, V_2) = \{0,1\}$ where 1 represents an existing and 0 a deleted edge. This representation is constant in its memory usage, and accessing a random element can be done in O(1). At every level, this representation is used to delete edges from the graph. Because of the order independence nature of the algorithm, another copy of the graph exists, which is only read from, and the changes of the graph used for deletion are migrated to this graph at the end of each level.

Adjacency lists, on the other hand, are memory-efficient because only existing edges are stored in memory. Every row in the adjacency matrix represents a vertex with a list of vertices connected to the represented vertex. So the edge count of some vertex is the length of the adjacency list for that vertex. In the following row is used analog to the adjacency list for the vertex with the same row number as in the adjacency matrix.

For performance reasons, the length of each row is calculated before each level and appended to the row. This representation is only used for reading existing edges in the level. Since every edge has to be processed, an index-based approach is used, and with that accessing a single edge can still be done in $O(1)$.
In between each level, the graph that contains the edge-changes migrates to the adjacency list representation. This step is called the compacting step \cite{zarebavaniCuPCCUDAbasedParallel2018}.

\paragraph{Estimating Execution Times of an Edge}

\begin{lstlisting}[language=C++, caption=Separation set loop in level 1, label=lst:sepsetloopl1]
    for (size_t offset = threadIdx.x; offset < row_length; offset += blockDim.x)
    {
        double pVal = performIndependenceTestForSeparationSet(
            blockIdx.x,
            blockIdx.y,
            offset);
        if (pVal > alpha) {
            deleteEdge();
        }
    }
\end{lstlisting}

In the listing \ref{lst:sepsetloopl1} the separation set loop executed per thread in a CUDA block is shown. The separation set tests for an edge are therefore distributed equally for all threads in a CUDA block. Due to that, a single CUDA thread can perform multiple independence tests. Because of the long execution time of the CI-Test, the looping over multiple CI-Tests mainly affects the thread's execution time. In higher levels, the behavior is comparable. Instead of looping over the edge of a row, a more complex mechanism is used to loop over all permutations of separation sets whose size is equal to the level number.

Consequently, in level 1 and higher, the execution time of a single thread can be linked to the iteration count. With $I_n$ as the maximum iteration count of a row, the estimated execution time can be defined as $t_{\text{CUDA-Thread}}(I_n)$. A CUDA thread itself is the smallest execution unit of the GPU and thus determines the execution time of the sequential part. In regards to Amdahl's law \cite{amdahlValiditySingleProcessor1967} a theoretical speedup of a fixed workload through parallelization is limited by the sequential part. If the sequential part limits the fixed workload of processing a row placed on the GPU, the minimum execution time of this row per level is $t_{\text{Row}_n}(I_n)$ if every edge is processed in parallel.

The maximum iteration count per row can be calculated by dividing the CI-Test count (or separation set count) for a given edge by the number of threads working in parallel on that edge. In the implementation done for this thesis, Nvidia GPUs are used, and in levels one and following, each edge is processed in parallel by a thread block. The thread block size is heuristically chosen as a multiple of the warp size because threads in a warp execute instructions synchronized. In this thesis, a block in levels one and following is defined as 64 threads. 64 CI-tests are done in parallel using those 64 thread-sized blocks, and the CI-test count has to be divided by 64 to calculate the maximum iteration count.

The CI-test count can be calculated using the row length $L_{row}$ which is the count of vertices connected to the rows vertex $V_i$ via an edge. For an edge $E=(V_i,V_j)$ and a separation set for that edge in a level $l$ is defined as $S \subseteq a(V_i ) \backslash \{V_j\}$ with $| S | = l$, the count of seperation set combinations can be calculated as $|Sepset(i,j)| = {L_{row} - 1 \choose level}$. The seperation set combination count is equal to the maximum CI-tests done for an edge.

Suppose the maximum execution time of every edge in a row is proportional to the maximum possible iterations. In that case, the execution time of a single row can be estimated by the row length because with a larger row length, the CI-Test count also grows.

\paragraph{Reducing the Sequential Part of the GPUs Workload}

Applying Amdahl's law \cite{amdahlValiditySingleProcessor1967} to a whole level as a fixed workload, the sequential part of that level processed by the GPU is the longest row in the adjacency matrix, and speedup can be realized by minimizing the longest row. So the execution time of that level on the GPU can also be written as $t_{l}(\max I_n) \equiv O_{l}(\max L_n)$ where $L_n$ is the length of the row $n$. According to Amdahl's law, a possible speedup for the workload of the GPU is only possible by minimizing the sequential part.
%  By adding the CPU as an additional processing unit which is also specialized in executing sequential tasks fast, moving the longest rows of the adjacency matrix to the CPU is the obvious approach.

The agglomerated task mapped to the processing units in the Mapping stage is defined as one or more rows of the adjacency matrix. The length of those rows influences the runtime of that task and is used for the Mapping stage by limiting the GPU's sequential part.
%TODO Add Gpu pseudocode for better understanding of iteration length etc

% - Agglomeration genauer erklären
% - Durch adjazenz matrix basis bietet sich row basiert an
% - row bundelt mehrere edges und ist damit etwas geeigneter als Task
% - einzelne edge können weiterhin parallel bearbeitet werden auf den PUs
% - genauer die ausführung der GPU anschauen
%  - GPU als basis -> wie geschwindigkeit der GPU erhöhen
%  - serieller/paralleler anteil
%  - GPU Block besteht meist aus 64 Threads
%  - (Beispiel code für ausführung)
%  - Iterationsanzahl bestimmt längste thread ausführung
%  - iterationsanzahl ist abhängig von row length
%  - Testanzahl berechnen aus und daraus iteration (wie)
%  - Diagram Iterations länge
%  - erklären wie dadurch der serielle anteil gekürzt werden kann
 
% \subsubsection{Compact Adjacency Lists}
% - durch compact step werden adjacency lists zwischen leveln berechnet
% - länge der liste ist anzahl an edges in einer row
% - letztes element genutzt als längenspeicher
% - grafik zur verbesserten darstellung
% - wird auch in \cite{zarebavani_cupc_2018} verwendet

\subsection{Mapping}
The last stage of Foster's Methodology \cite{fosterDesigningBuildingParallel1995}, called Mapping, applies the tasks created in the Agglomeration stage to the given system. This stage is vital for heterogeneous systems because the underlying system has to be utilized efficiently. Utilizing heterogeneous systems means maximizing CPU and GPU usage and minimizing inter-processor communication. Minimizing the inter-processor communication includes the connection between GPU and CPU and the core/thread connections in the GPU and CPU.
Mapping the tasks on the system PUs is also called scheduling. There are two common ways of scheduling tasks: Static and dynamic scheduling \cite{kopetzRealTimeScheduling1997}.

A scheduler is called static if it makes its scheduling decisions pre-execution. The static scheduler needs prior knowledge of the capabilities of each processing unit. In addition, the static scheduler needs to know the performance implications and characteristics of the given task set so that the tasks are balanced optimally on the system. Therefore a static scheduler that does not schedule on compile time is also called a static load-balancer.

A scheduler is called dynamic if scheduling decisions are made at runtime. Due to the runtime decision, the current system state can be taken into account. Prior knowledge about the processing unit's capabilities is not necessary. Knowledge about the task set and its performance implications is still helpful for optimally balancing tasks onto the heterogeneous PUs.
A particular case of a dynamic scheduling algorithm is the workstealing scheduler. While one processing unit executes the whole task set, the others "steal" tasks at runtime from the executing PU to minimize the workload gradually.

In the Agglomeration chapter, the longest sequential part of the workload is estimated, by calculating the longest execution time of the smallest parallelizable part, the task. The CPU as a general-purpose processor evolved from being good at both sequential and parallel workloads in contrast to the GPU, which is developed solely with parallel workloads in mind. Placing those longest sequential parts on the CPU could speed up those tasks and therefore optimize the performance of the workload in regards to Amdahl's law. The following proposed heterogeneous PC algorithm solutions are primarily based on this insight.
% - Mapping genauer erklären
% - Scheduling spezifizieren
%     - Static/Dynamic scheduling
%     - static = prebalanced
%         vor nachteile
%     - dynamic 
%         vor nachteile
% - Two approaches pre balanced and workstealing

\section{Heterogeneous PC Algorithm Approaches}
Using the insights gathered from Foster's Methodology applied to the heterogeneous PC-stable algorithm, two approaches arise. First, the static scheduling approach that balances rows pre-execution on the heterogeneous system. This approach is called the pre-balanced approach. The second approach uses dynamic scheduling by workstealing rows from an executing processing unit. Both approaches use the length information gained from the adjacency matrix to place the rows with the longest estimated execution time on the CPU, which is specialized for executing sequential tasks fast. Every unscheduled row is executed on the highly parallel GPU.

\subsection{Pre-Balanced Approach}
Static scheduling is done ahead of execution and, in most cases, at compile time \cite{singhSurveyStaticScheduling2015}. Because of the varying size of task and dataset dimensions, the scheduling approach proposed schedules between levels and just before execution. A task, which is defined as a row of the adjacency matrix, is scheduled using its length as an identifier for the execution time parameter. The scheduled tasks are passed to the execution unit, and all execution units process their tasks concurrently. The different characteristics of the given execution units have to be considered during load balancing. The GPUs processing power lies in parallelization. Therefore most tasks should be scheduled on this processing unit. The task with the estimated longest execution time gets placed on the CPU, which is better at processing serial tasks than the GPU. The ratio between tasks placed on the CPU and tasks placed on the GPU is an estimation and can be optimized using hardware characteristics. Such a characteristic used in this approach is the core count of the CPU, which can be an indicator of how many tasks can be processed concurrently. Additional characteristics that could improve the balance of tasks placed on the processing units are the clock-speed of both CPU and GPU, which can approximate the speed at which tasks are processed, and the maximum thread count schedulable on the GPU. Because of the approximative nature of the static scheduling approach, each additional parameter used does not necessarily mean a better balancing. Balancing parameters need tuning, which is done by adding and setting thresholds.
The maximum count of tasks placed on the CPU is thresholded by a proportion of the CPU core count. Because of the specific execution times of a single test in different levels, this threshold is tuned for each level. The threshold tuning is done by rerunning the execution multiple time and aligning the CPU and GPU execution times in each level.

During the processing of the assigned tasks, the CI-Test might access data associated with nodes in the iterated separation sets. The separation set composition cannot be decided before execution, and due to that, a random access pattern is assumed. For concurrently processed tasks in a  heterogeneous system, a random memory access pattern can imply that both CPU and GPU access the same data. Even though this access could be only read access, memory locality is not always given in such situations. Therefore data has to be copied over the CPU-GPU interconnect. Interconnect memory access is slow in comparison to local processing units' memory access and should be avoided. Nvidia tools such as "nvprof" can help profiling interconnect access by counting page migrations between the CPU and GPU. Additionally, the page size can be multiple times larger than the data accessed. By placing the node-associated data next to each other in memory, access to nodes close to each other can also trigger the same effect.

In early tests of this approach, many page migrations occurred by deleting edges concurrently in the graph. An edge migration step is added to the CPU task-processing that minimizes the effect of contemporary edge deletions. Edge deletions on the CPU are queued and migrated after the GPU task-processing is done.
Data that is only read during the task execution can be copied in both the CPU and the GPU memory to prevent page migrations and boost memory access performance. In the case of gaussian CI-Tests, a global correlation matrix is only read through the whole algorithm. For performance optimization, this correlation matrix is stored in both the CPU and the GPU memory.
% - Static scheduling ansatz
% - Load balancer schiebt längste rows auf CPU
% ...
% - Viel Memory/ communication -> nvprof?
% - Limitieren durch Migrierungsschritt der CPU

\subsection{Workstealing Approach}
While the pre-balanced approach works through setting thresholds pre-execution and using hardware characteristics to estimate execution time, this estimation can vary significantly from the actual execution time. This problem does not apply to a dynamic scheduling approach. In dynamic scheduling, tasks are scheduled during execution time by placing the task on an idle execution unit. There are different types of dynamic scheduling algorithms applied to this problem.
Because of the chosen structure of the GPU as a main executing component and the CPU helping by taking off the longest serial execution time tasks, a workstealing approach fits best. The workstealing scheduling strategy is often used in multithreading. A processor has a fixed processing core count, and the workstealing algorithm maintains a queue with threads to be executed on those cores. As soon as the queue is not empty, a core that is done with its current thread "steals" the next task of the queue, which is then not executable by other threads. The workstealing algorithm has to maintain the execution order that is necessary for correct execution \cite{blumofeSchedulingMultithreadedComputations1999}.
In the case of the order-independent PC-stable algorithm, the execution order of CI tests is irrelevant. Due to that, a simplified version of the workstealing algorithm can be used, where the execution units take the next open task.

Limiting the workstealing approach to using rows as a task is unnecessary since there is no task assignment. A better task for this approach is to use the same approach as the GPU-only implementation and use each edge as a task. Execution units can work on the same row, and there is less chance that some execution unit idles.

Both the GPU side of execution and the CPU side mark tasks before they start working on the marked tasks. The task-marking operation is atomic so that a task is not done twice, and another matrix data structure containing those marks is introduced. Atomic operations such as compare-and-swap allow concurrent running threads to prevent data races and are commonly implemented as instructions in the CPU.
Libraries such as libcu++ \cite{NVIDIALibcudacxx2021} even allow for system-wide atomic use. System-wide atomics directly updates CPU and GPU memory on write and prevents page faults, improving performance.
Task scheduling in CUDA is done by the GPU runtime and is closed source \cite{olmedoDissectingCUDAScheduling2020}.

The behavior of the GPU threads scheduling algorithm cannot be influenced without major performance hits by introducing synchronization. The CPU task scheduling, in contrast, is entirely customizable without much effort. The CPU threads start with those longest estimated tasks to move the longest tasks, identified through the row length, onto the CPU execution unit.
If one of the execution units checked every task, a global flag is set, which indicates that the level is done. After that, every running task finishes and then terminates itself to prevent unnecessary mark checking.

% - Dynamic scheduling
% - Workstealing erklären (herkunft etc)
% - GPU läuft normal durch
% - CPU nimmt sich längste rows als erstes vor und arbeitet von hinten ab
% - regelmäßiges überprüfen ob schon fertig
% - vermehrte kommunikation

\section{Implementation}
Both approaches' implementations\footnote{https://github.com/milanpro/Masterthesis-Heterogeneous-PC/tree/state/thesis} were done using C++ as the base programming language and CUDA for the GPU side code. System programming languages such as C++ enable optimizations by developing close to the underlying system. Global data structures, which have to be accessed in every task, such as the adjacency lists of the skeleton, are held as pointers in a struct, which can be easily cloned and passed to CPU and GPU threads. Memory is allocated using the CUDA runtime unified memory allocator so that data pointers are globally accessible.

The main loop iterates over each level, starting with level 0 until the last level is executed and the skeleton is estimated. In level 0 the GPU code for processing each edge is called. The following levels all share the same procedure. First, the changes of the write-only graph in matrix form are migrated to the read-only adjacency list form. After that, the load-balancer starts placing tasks in the queues of the CPU and GPU for the pre-balanced approach. The workstealing approach directly moves to the execution instead.

After the balancing step, execution starts. An executor class abstracts each processing unit so that additional processing units can be added in further code extensions. A thread is spawned for every processing unit by the executor that orchestrates its execution flow. While every task is executed independently in the pre-balanced approach because all tasks have their respective owning processing unit, in the workstealing approach, every task is marked with a system-wide atomic flag before execution, using the libcu++ library by Nvidia \cite{NVIDIALibcudacxx2021}, to prevent doubly executed tasks.

CPU side execution is handled by OpenMP, which spawns the necessary threads. For the pre-balanced approach, OpenMP dynamic scheduling is used so that threads do not idle. Dynamic scheduling is preferred because task execution time can differ significantly, and due to that, static scheduling is not sufficient.

% - could be integrated into the Approach sections
% - one state containing unified memory data
% - Executor abstraction
% - Task as a struct containing row index and how many rows
% - Load balancer invoked only in pre-balanced case
% - else GPU has all rows
% - Workstealing approach guards
% - Maybe show some code