%!TEX root = ../thesis.tex
\chapter{Heterogeneous Constraint-Based Causal Structure Learning}
\label{chap:hetcsl}
Parallelizing the PC algorithm on heterogeneous systems starts with choosing a task size that meets the requirements of the different processing units executing the workload. The PC algorithm has been executed efficiently on different execution units \cite{leParallelPCPackageEfficient2018,leFastPCAlgorithm2019,schmidtOrderIndependentConstraintBasedCausal2018,zarebavaniCuPCCUDAbasedParallel2018}. Heterogeneous systems can contain diverse execution units, which could be all combined for execution. In this thesis, the focus lies on CPU - GPU combined execution. As a basis, the implementation of the PC algorithm from Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} is used, where efforts for parallelizing the PC algorithm have already been made to utilize the throughput specialization of the GPU. The chosen task must be balanced on the CPU and the GPU using a balancing algorithm. The balancing algorithm has to be adaptable to the environmental parameters of the system it runs on. In this chapter, the PC algorithm is parallelized for execution on a heterogeneous CPU-GPU system by applying state-of-the-art parallelization methods. An optimized task size is found using Foster's Methodology \cite{fosterDesigningBuildingParallel1995}, and optimal balancing of the task is elaborated on in this chapter.

\section{Parallelizing the PC Algorithm}
The parallelization of workloads is the basis of parallel programming. A well-known design methodology used to split a workload such that it can be run in parallel efficiently is Foster's Methodology \cite{fosterDesigningBuildingParallel1995}. Foster's Methodology is also often used for distributed-memory systems. A GPU-CPU heterogeneous system is a distributed-memory system because modern high-performance computing systems include dedicated memory for each processing unit and GPU memory is different from CPU memory. CPU-assigned memory is mainly optimized for latency, supporting the latency-focused nature of a CPU. GPU-assigned memory, on the other hand, is optimized for data throughput.

Foster's Methodology uses four distinct stages, which are applied to the workload one after another:

\begin{enumerate}
    \item Partitioning
    \item Communication
    \item Agglomeration 
    \item Mapping
\end{enumerate}

The workload is decomposed into the smallest possible tasks in the Partitioning stage. For correct execution, the Communication stage defines the necessary coordination of task execution. Foster's Methodology's first and second stages are essential, achieving concurrency and scalability in the given workload while algorithmic correctness is sustained. In the Agglomeration stage, multiple tasks are agglomerated to align the computation and data to the underlying system. The Mapping stage is analogous to scheduling and maps the agglomerated tasks onto the different processing units. In the following chapters, Foster's Methodology is applied to the order-independent PC-stable algorithm.

\subsection{Partitioning and Communication}
In the Partitioning stage, opportunities for parallel execution are exposed. An efficient partitioning keeps computational parts and their respective data together to minimize the later communication efforts. A fine-grained decomposition of the workload can be established in two different ways. Domain decomposition defines small data fragments first and specifies the computation later. Functional decomposition splits the computation into disjoint tasks and handles the data after. Domain decomposition is more appropriate than functional decomposition if there is a significant data overlap.
More generalized functional decomposition can also be seen as splitting a multivariate function $f(x_1,x_2,...,x_n)$ into multiple functions ${g_1,g_2,...g_m}$ such that $f(x_1,x_2,...,x_n) = \theta(g_1(x_1,x_2,...,x_n),g_2(x_1,x_2,...,x_n),...g_m(x_1,x_2,...,x_n))$ where $\theta$ is another function combining the results. Every function $g_n$ gets the same input as $f$. If the functional approach is applied to some algorithmic problem, loops with independent iterations can be mapped to this mathematical model, making the parallelization of single iterations possible. If iterations of such loops, mapped to a task, include side effects, communication or synchronization must be implemented.
The Communication stage specifies links between data consumers and producers. Domain decomposition problems can have more complex communication infrastructures due to data dependencies. Distributing computation and communication is more efficient than centralizing the algorithm through some central manager communicating with every task. Independent and balanced tasks are more capable of efficient execution communication-wise.

The PC-stable GPU-accelerated algorithm proposed in the paper by Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} allows for a straightforward applying of the Partitioning and Communication stages. The algorithm is based on the Gaussian distribution model and handles only the CI-Test for multivariate normal distributed data. For simplicity purposes, the thesis implementation incorporates the same approach and looks into other CI-Tests later.

\begin{algorithm}
    \caption{Adjacency search of PC-stable algorithm with Gaussian distribution model \cite{schmidtOrderIndependentConstraintBasedCausal2018, colomboOrderIndependentConstraintBasedCausal2014}}
    \label{alg:pcstable_gaussian}
    \begin{algorithmic}[1]
    \Require Vertex set $V$, correlation matrix $Cor$
    \Ensure Estimated skeleton $C$, separation sets \textbf{Sepset}
    \State with fully connected skeleton $C$ and $l = -1$
    \Repeat \label{alg:pcstable_gaussian:level_loop}
        \State $l=l+1$
        \For{all Variables $V_i$ in $C$}
            \State Let $a(V_i) = adj(C,V_i)$ \label{alg:pcstable_gaussian:adj}
        \EndFor
        \Repeat \label{alg:pcstable_gaussian:edge_loop}
            \State Select pairs $(V_i,V_j)$ adjacent in $C$ with $|a(V_i)\backslash\{V_j\}| \geq l$
            \Repeat \label{alg:pcstable_gaussian:sepset_loop}
                \State Choose separation set $S \subseteq a(V_i ) \backslash \{V_j\}$ with $| S | = l$
                \If{$p = CITest(V_i,V_j,S,Cor) \land p \leq \alpha$}
                    \State Delete edge $V_i - V_j$ from $C$
                    \State Set Sepset(i,j) = Sepset(j,i) = S
                \EndIf
            \Until{edge $V_i - V_j$ is deleted in $C$ or all $S \subseteq a(V_i) \backslash \{V_j\}$ with $|S| = l$ have been chosen}
        \Until{all adjacent vertices $V_i$, and $V_j$ in $C$ such that $|a(V_i)\backslash\{V_j\}| \geq l$ have been considered}
    \Until{each adjacent pair $V_i$, $V_j$ in $C$ satisfy $|a(V_i)\backslash\{V_j\}| \geq l$}
    \State \textbf{return} C, Sepset
    \end{algorithmic}
\end{algorithm}

As can be seen in Algorithm \ref{alg:pcstable_gaussian}, the main input apart from the vertices is the correlation matrix for those vertices. The correlation matrix is necessary for the Gaussian-based CI-Test. Algorithm \ref{alg:pcstable} is a generalized version of algorithm \ref{alg:pcstable_gaussian}, without the dependence on a specific CI-test.
As already stated, loops can be split into tasks if their iteration execution flow is independent of the input of other iterations. This statement does not apply to the repetition in line \ref{alg:pcstable_gaussian:level_loop} because every level depends on its predecessor's results. Synchronization must occur between each level.
Only two loops are independent iteration-wise. The proposed algorithm can be split between different edges (line \ref{alg:pcstable_gaussian:edge_loop}) or between different separation sets (line \ref{alg:pcstable_gaussian:sepset_loop}). Therefore, the smaller and better fitting task for the Partitioning phase is a separation set split.

In the proposed GPU-accelerated algorithm variant of Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018}, level 0 is split into edges, because there is only one separation set $S \subseteq a(V_i ) \backslash \{V_j\}$ with $| S | = l$, which is the empty set $\{\}$. Therefore only one CI-Test per edge must be processed. Therefore each edge is a single task in level 0.

In level $1,2 ... m$, every separation set that has to be tested in combination with an edge is a task. Performance improvements can be made by having an early stop criterion such as a deleted edge so that other separation sets for that edge can be ignored. An early stop criterion in a parallelized context requires additional communication between execution units working on that edge.

Data that is shared cannot be appropriately aligned to the tasks, and due to that, all tasks must have the ability to access the shared data at any time. Task data access in the PC-stable algorithm is nearly random on the correlation matrix in level 1 and the following levels because the separation sets can be built from all existing vertices. 

%- Partitioning und Communication genauer erklären
%- GPU paper basis
%    - partitioning schon getan
%    - Task ist eine edge
%    - Wiederverwenden
%    - Durch parallelisierung auf processing units weiterhin diese partitionierung
    
\subsection{Agglomeration}
\label{chap:agglo}
The Agglomeration stage uses the algorithm produced by the Partitioning and Communication stages and specializes the algorithm for the execution environment. Tasks are agglomerated so that their execution is efficient. The resulting number of tasks of the Agglomeration stage can be greater than the number of processing units. The subsequent conflicting decisions are necessary to guide through the Agglomeration stage:

\begin{enumerate}
    \item Reduce communication cost through agglomerated computation
    \item Preserve flexibility concerning the later Mapping stage and maintain parallelism
\end{enumerate}

With regards to the GPU implementation of the paper by Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018}, which splits the algorithm into tasks for each separation set for an edge and parallelizes these on the GPU. Agglomerating these tasks into larger task sets can be done by considering the next bigger loop. The per edge iteration in line \ref{alg:pcstable_gaussian:edge_loop} is suitable for combining multiple tasks because it is independent of the other edge iterations while the task can still be parallelized further on the underlying processing unit that executes the edge.

The PC-stable algorithm is commonly used in high-dimensional settings with thousands of variables \cite{nagarajanFunctionalRelationshipsGenes2010}. An edge between each vertex representing a variable can exist so that $|E| \leq |V|^{2}$. Therefore, millions of edges might exist per level, so agglomerating an edge as a task is not feasible for a GPU-CPU heterogeneous system.

%TODO Maybe add illustration for both graphs (adj list vs. matrix)
\paragraph{Adjancency Lists in the PC-stable Algorithm}
Looking further into the GPU-side implementation leads to another possible task that agglomerates multiple edges. The adjacency set populated in the algorithm in line \ref{alg:pcstable_gaussian:adj} of the PC-stable algorithm can be implemented using an adjacency matrix or adjacency list. An adjacency matrix is a standard graph representation that encodes every edge between two vertices $V_1, V_2$ as a field in the matrix at some position to $A_G(V_1, V_2) = \{0,1\}$, where 1 represents an existing and 0 a deleted edge. This representation is constant in its memory usage, and a random element can be accessed in O(1). At every level, this representation is used to delete edges from the graph. Because the algorithm is order-independent, another copy of the graph exists, which is only read from. The changes of the graph used for deletion are migrated to this graph at the end of each level.

On the other hand, adjacency lists are memory-efficient because only existing edges are stored in memory. Every row in the adjacency matrix represents a vertex with a list of vertices connected to the represented vertex. Hence the edge count of a vertex is the length of the adjacency list for that vertex. In the following, "row" is used analogously to "adjacency list" for any vertex with the same row number as in the adjacency matrix.

For performance reasons, the length of each row is calculated before each level and appended to the row. This representation is only used for reading existing edges in the level. Since every edge has to be processed, an index-based approach is used. Thus a single edge is still accessible in $O(1)$.
Between two subsequent levels, the graph that contains the edge changes migrates to the adjacency list representation. This step is called the compacting step \cite{zarebavaniCuPCCUDAbasedParallel2018}.
\newpage
\paragraph{Estimating Execution Times of an Edge}

\begin{lstlisting}[language=C++, caption=Separation set loop in level 1, label=lst:sepsetloopl1]
    for (size_t offset = threadIdx.x; offset < row_length; offset += blockDim.x)
    {
        double pVal = performIndependenceTestForSeparationSet(
            blockIdx.x,
            blockIdx.y,
            offset);
        if (pVal > alpha) {
            deleteEdge();
        }
    }
\end{lstlisting}

In listing \ref{lst:sepsetloopl1} the separation set loop executed per thread in a CUDA block is shown. The separation set tests for an edge are therefore distributed equally for all threads in a CUDA block. Due to that, a single CUDA thread can perform multiple independence tests. Because of the long execution time of the CI-Test, the looping over multiple CI-Tests mainly affects the thread's execution time. In higher levels, the behavior is comparable. Instead of looping over the edge of a row, a more complex mechanism is used to loop over all permutations of separation sets whose size is equal to the level number.

Consequently, in level 1 and higher, the execution time of a single thread can be linked to the iteration count. With $I_n$ as the maximum iteration count of a row, the estimated execution time can be defined as $t_{\text{CUDA-Thread}}(I_n)$. A CUDA thread itself is the smallest execution unit of the GPU and thus determines the execution time of the sequential part. According to Amdahl's law \cite{amdahlValiditySingleProcessor1967}, a theoretical speedup of a fixed workload through parallelization is limited by the sequential part. If the sequential part limits the fixed workload of processing a row placed on the GPU, the minimum execution time of this row per level is $t_{\text{Row}_n}(I_n)$ if every edge is processed in parallel.

The maximum iteration count per row can be calculated by dividing the CI-Test count (or separation set count) for a given edge by the number of threads working in parallel on that edge. In this thesis' experiments, Nvidia Volta GPUs \cite{NVIDIATESLAV1002017} are used, and in levels 1 and the following levels, each edge is processed in parallel by a thread block. The thread block size is heuristically chosen as a multiple of the warp size because threads in a warp execute instructions in a synchronized manner. In this thesis, a thread block in levels 1 and the following level is defined as 64 threads. Therefore 64 CI-tests are processed in parallel using 64 thread-sized blocks, and the CI-test count has to be divided by 64 to calculate the maximum iteration count.

The CI-test count can be calculated using the row length $L_{row}$ which is the count of vertices connected to the rows vertex $V_i$ via an edge. For an edge $E=(V_i,V_j)$ and a separation set for that edge in a level $l$ is defined as $S \subseteq a(V_i ) \backslash \{V_j\}$ with $| S | = l$, the count of seperation set combinations can be calculated as $|Sepset(i,j)| = {L_{row} - 1 \choose level}$. The seperation set combination count is equal to the maximum number of CI-tests processed for an edge.

Suppose the maximum execution time of every edge in a row is proportional to the maximum possible iterations. In that case, the execution time of a single row can be estimated by the row length because with a larger row length, the CI-Test count also grows.

\paragraph{Reducing the Sequential Part of the GPUs Workload}

Applying Amdahl's law \cite{amdahlValiditySingleProcessor1967} to a whole level as a fixed workload, the sequential part of that level processed by the GPU is the longest row in the adjacency matrix, and speedup can be realized by minimizing the longest row. So the execution time of that level on the GPU can also be written as $t_{l}(\max I_n) \equiv O_{l}(\max L_n)$ where $L_n$ is the length of the row $n$. According to Amdahl's law, a possible speedup for the workload of the GPU is only possible by minimizing the sequential part.
%  By adding the CPU as an additional processing unit which is also specialized in executing sequential tasks fast, moving the longest rows of the adjacency matrix to the CPU is the obvious approach.

The agglomerated task mapped to the processing units in the Mapping stage is defined as one or more rows of the adjacency matrix. The length of those rows influences the runtime of that task and is used for the Mapping stage by limiting the GPU's sequential part.
%TODO Add Gpu pseudocode for better understanding of iteration length etc

% - Agglomeration genauer erklären
% - Durch adjazenz matrix basis bietet sich row basiert an
% - row bundelt mehrere edges und ist damit etwas geeigneter als Task
% - einzelne edge können weiterhin parallel bearbeitet werden auf den PUs
% - genauer die ausführung der GPU anschauen
%  - GPU als basis -> wie geschwindigkeit der GPU erhöhen
%  - serieller/paralleler anteil
%  - GPU Block besteht meist aus 64 Threads
%  - (Beispiel code für ausführung)
%  - Iterationsanzahl bestimmt längste thread ausführung
%  - iterationsanzahl ist abhängig von row length
%  - Testanzahl berechnen aus und daraus iteration (wie)
%  - Diagram Iterations länge
%  - erklären wie dadurch der serielle anteil gekürzt werden kann
 
% \subsubsection{Compact Adjacency Lists}
% - durch compact step werden adjacency lists zwischen leveln berechnet
% - länge der liste ist anzahl an edges in einer row
% - letztes element genutzt als längenspeicher
% - grafik zur verbesserten darstellung
% - wird auch in \cite{zarebavani_cupc_2018} verwendet

\subsection{Mapping}
The last stage of Foster's Methodology \cite{fosterDesigningBuildingParallel1995}, called Mapping, applies the tasks created in the Agglomeration stage to the specific hardware characteristics of a given system. This stage is vital for heterogeneous systems because the underlying system has to be utilized efficiently. Utilizing heterogeneous systems means maximizing CPU and GPU usage and minimizing inter-processor communication. Minimizing the inter-processor communication includes the connection between GPU and CPU and the core/thread connections in the GPU and CPU.
Mapping the tasks on the system PUs is also called scheduling. There are two common ways of scheduling tasks and distributing data: Static and dynamic scheduling \cite{kopetzRealTimeScheduling1997}.

A scheduler is called static if it makes its scheduling decisions pre-execution. The static scheduler needs prior knowledge of the capabilities of each processing unit. In addition, the static scheduler needs to know the performance implications and characteristics of the given task set so that the tasks are balanced optimally on the system. Therefore a static scheduler that does not schedule on compile time is also called a static load-balancer.

A scheduler is called dynamic if scheduling decisions are made at runtime. Due to the runtime decision, the current system state can be taken into account. Prior knowledge about the processing unit's capabilities is not necessary. Knowledge about the task set and its performance implications is still helpful for optimally balancing tasks onto the heterogeneous PUs.
A particular case of a dynamic scheduling algorithm is the workstealing scheduler. While one processing unit executes the whole task set, the others "steal" tasks at runtime from the executing PU to minimize the workload gradually.

In the Agglomeration chapter, the longest sequential part of the workload is estimated, by calculating the longest execution time of the smallest parallelizable part, the task. The CPU as a general-purpose processor evolved to being efficient for both sequential and parallel workloads in contrast to the GPU, which is developed solely with parallel workloads in mind. Placing the longest sequential parts on the CPU could speed up the execution and therefore optimize the workload's performance in regards to Amdahl's law. The following proposed heterogeneous PC algorithm solutions are primarily based on this insight.
% - Mapping genauer erklären
% - Scheduling spezifizieren
%     - Static/Dynamic scheduling
%     - static = prebalanced
%         vor nachteile
%     - dynamic 
%         vor nachteile
% - Two approaches pre balanced and workstealing

\section{Heterogeneous PC Algorithm Approaches}
Using the insights gathered from Foster's Methodology applied to the heterogeneous PC-stable algorithm, two approaches arise. First, the static scheduling approach that balances rows pre-execution on the heterogeneous system. This approach is called the pre-balanced approach. The second approach uses dynamic scheduling by workstealing rows from the task queues of other processing units. Both approaches use the length information gained from the adjacency matrix to place the rows with the longest estimated execution time on the CPU. Every unscheduled row is executed on the highly parallel GPU.

\subsection{Pre-Balanced Approach}
Static scheduling is done ahead of execution and, in most cases, at compile time \cite{singhSurveyStaticScheduling2015}. Because of the varying size of task and dataset dimensions, the scheduling approach proposed schedules between levels and just before execution. A task, defined as a row of the adjacency matrix, is scheduled using its length as an identifier for the execution time parameter. The scheduled tasks are passed to the execution unit, and all execution units process their tasks concurrently. The different characteristics of the given execution units have to be considered during load balancing. The GPUs processing power lies in parallelization. Therefore most tasks should be scheduled on this processing unit. The task with the estimated longest execution time gets placed on the CPU. The ratio between tasks placed on the CPU and tasks placed on the GPU is an estimation and can be optimized based on hardware characteristics. Such a characteristic used in this approach is the core count of the CPU, which can be an indicator of how many tasks can be processed concurrently. Additional characteristics that could improve the balance of tasks placed on the processing units are the clock-speed of both CPU and GPU, which can approximate the speed at which tasks are processed, and the maximum thread count schedulable on the GPU. Because of the approximative nature of the static scheduling approach, each additional parameter used does not necessarily mean a better balancing. Balancing parameters need tuning, which is done by adding and setting thresholds.
The maximum count of tasks placed on the CPU is thresholded by a proportion of the CPU core count. Because of the specific execution times of a single test in different levels, this threshold is tuned for each level. The threshold tuning is done by rerunning the execution multiple time and aligning the CPU and GPU execution times in each level.

During the processing of the assigned tasks, the CI-Test might access data associated with nodes in the iterated separation sets. The separation set composition cannot be decided before execution, and due to that, a random access pattern is assumed. For concurrently processed tasks in a  heterogeneous system, a random memory access pattern can imply that both CPU and GPU access the same data. Even though this access could be only read access, memory locality is not always given in such situations. Therefore data has to be copied over the CPU-GPU interconnect. Interconnect memory access is slow in comparison to local processing units' memory access and should be avoided. Nvidia tools such as "nvprof" can help profiling interconnect access by counting page migrations between the CPU and GPU. Additionally, the page size can be multiple times larger than the data accessed. By placing the node-associated data next to each other in memory, access to nodes close to each other can also trigger the same effect.

In early tests of this approach, many page migrations occurred by deleting edges concurrently in the graph. An edge migration step is added to the CPU task-processing minimizing the effect of contemporary edge deletions. Edge deletions on the CPU are queued and migrated after the GPU task-processing is finished.
Data only read during the task execution can be copied in both the CPU and the GPU memory to prevent page migrations and boost memory access performance. In the case of gaussian CI-Tests, a global correlation matrix is created in the beginning and only accessed in a read-only fashion during the execution of the parallel parts of the algorithm. For performance optimization, this correlation matrix is stored in both the CPU and the GPU memory.
% - Static scheduling ansatz
% - Load balancer schiebt längste rows auf CPU
% ...
% - Viel Memory/ communication -> nvprof?
% - Limitieren durch Migrierungsschritt der CPU

\subsection{Workstealing Approach}
While the pre-balanced approach works through setting thresholds pre-execution and using hardware characteristics to estimate execution time, this estimation can vary significantly from the actual execution time. Since execution time estimates are not central to dynamic scheduling, this problem is mitigated for a dynamic scheduling approach. In dynamic scheduling, tasks are scheduled during execution by placing tasks on an idle execution unit. There are different types of dynamic scheduling algorithms.

Because the GPU is a main executing component in the proposed algorithm, and the CPU helps by taking off the longest sequential execution time tasks, a workstealing approach fits best. The workstealing scheduling strategy is often used in multithreading \cite{nemirovskyMultithreadingArchitecture2013}. A processor has a fixed processing core count, and the workstealing algorithm maintains a central queue with tasks to be executed on those cores. As soon as the queue contains a task, a core that is finished with its current task "steals" the next task from the queue, and the stolen task is not executable by other cores anymore. The workstealing algorithm has to maintain the execution order that is necessary for correct execution \cite{blumofeSchedulingMultithreadedComputations1999}.
In the case of the order-independent PC-stable algorithm, the execution order of CI-tests is irrelevant. Consequently, a simplified version of the workstealing algorithm can be used, where the execution units take the next task. A central task queue is not necessary when the order of execution is irrelevant.

Limiting the workstealing approach to using rows as tasks is unnecessary since there is no task assignment. A more promising approach uses the same algorithm as the GPU-only implementation does and handles each edge as a task. Execution units can work on the same row, and there is less chance that some execution unit idles.

Both the GPUs and the CPUs execution units mark tasks before they start working on them. The task-marking operation is atomic so that a task is not marked twice, and another matrix data structure containing those marks is introduced. Atomic operations such as compare-and-swap allow concurrent running threads to prevent data races and are commonly implemented as instructions in the CPU.
Libraries such as libcu++ \cite{NVIDIALibcudacxx2021} even allow for system-wide atomic use. System-wide atomics directly updates CPU and GPU memory on write and prevents page faults, improving performance.
Task scheduling in CUDA is managed by the GPU runtime and is closed source \cite{olmedoDissectingCUDAScheduling2020}.

The order of the GPU thread scheduling algorithm cannot be influenced without major performance hits by introducing synchronization. The CPU task scheduling order, in contrast, is entirely customizable without much effort. The CPU threads iterate over a list of tasks, sorted by row length. Therefore the longest tasks are processed first by the CPU execution unit.
If one of the execution units processed every task, a global flag is set, which indicates that the level is finished. After that, every running task finishes, and then the remaining execution units terminate their search for additional tasks to prevent unnecessary processing.

% - Dynamic scheduling
% - Workstealing erklären (herkunft etc)
% - GPU läuft normal durch
% - CPU nimmt sich längste rows als erstes vor und arbeitet von hinten ab
% - regelmäßiges überprüfen ob schon fertig
% - vermehrte kommunikation

\section{Implementation}
Both approaches were implemented\footnote{https://github.com/milanpro/Masterthesis-Heterogeneous-PC/tree/state/thesis} using C++ as the base programming language and CUDA for the GPU code. System programming languages such as C++ enable optimizations by developing based on the hardware of the underlying system. Global data structures, which have to be accessed in every task, such as the adjacency lists of the skeleton, are held as pointers in a struct, which can be easily cloned and passed to CPU and GPU threads. Memory is allocated using the CUDA runtime unified memory allocator so that data pointers are globally accessible.

The main loop iterates over each level, starting with level 0 until the last level is executed and the skeleton is estimated. In level 0 the GPU code for processing each edge is called. The following levels all share the same procedure. First, the changes of the write-only graph in matrix form are converted to the read-only adjacency list form. After that, the load-balancer starts placing tasks in the queues of the CPU and GPU for the pre-balanced approach. The workstealing approach directly moves to the execution instead.

After the balancing step, execution starts. An executor class abstracts each processing unit so that additional processing units can be added in further code extensions. A thread is spawned for every processing unit by the executor that orchestrates its execution flow. While every task is executed independently in the pre-balanced approach because all tasks have their respective owning processing unit, in the workstealing approach, every task is marked with a system-wide atomic flag before execution, using the libcu++ library by Nvidia \cite{NVIDIALibcudacxx2021}, to prevent doubly executed tasks.

CPU-side execution is handled by OpenMP, which spawns the necessary threads. For the pre-balanced approach, OpenMP dynamic scheduling is used so that threads do not idle. Dynamic scheduling is preferred because task execution time can differ significantly, and due to that, static scheduling is not sufficient.

% - could be integrated into the Approach sections
% - one state containing unified memory data
% - Executor abstraction
% - Task as a struct containing row index and how many rows
% - Load balancer invoked only in pre-balanced case
% - else GPU has all rows
% - Workstealing approach guards
% - Maybe show some code