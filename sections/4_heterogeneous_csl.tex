%!TEX root = ../thesis.tex
\chapter{Heterogeneous Constraint-Based Causal Structure Learning}
Parallelizing the PC algorithm on heterogeneous systems starts with choosing a task size that is flexible enough for the different processing units executing the workload. As a basis, the implementation of the PC algorithm from the paper \cite{schmidtOrderIndependentConstraintBasedCausal2018} is used since efforts for parallelizing the PC algorithm have already been done to utilize the throughput of the GPU. After that, the chosen task must be balanced on the CPU and the GPU using a balancing algorithm. The balancing algorithm has to be adaptable to the environmental parameters of the system it runs on.

\section{Parallelize the PC Algorithm}
Parallelizing workloads is the basis of parallel programming. A well-known design methodology used to architect a workload such that it can be run in parallel efficiently is Foster's Methodology \cite{fosterDesigningBuildingParallel1995}. Foster's Methodology is also often used for distributed-memory systems. Since GPU-CPU heterogeneous systems are distributed-memory systems, I will apply this method to parallelize the workload. Foster's Methodology uses four distinct stages, which are applied to the workload one after another:

\begin{enumerate}
    \item Partitioning
    \item Communication
    \item Agglomeration 
    \item Mapping
\end{enumerate}

The workload is decomposed into the smallest possible tasks in the Partitioning stage. For a correct execution, the Communication stage defines the necessary coordination of task execution. Both the first and the second stage of Foster's Methodology are essential for the search of concurrency and scalability in the given workload while achieving algorithm correctness. In the Agglomeration stage, multiple tasks are agglomerated, better to align the computation and data to the underlying system. The Mapping stage can be seen as analog to scheduling and maps the agglomerated tasks onto the different processing units.

\subsection{Partitioning and Communication}
In the Partitioning stage, opportunities for parallel execution are exposed. A good partition keeps computational parts and their respective data together to minimize the later communication efforts. Using fine-grained decomposition of the workload can be done in two different ways. Domain decomposition defines small data fragments first and specifies the computation later. Functional decomposition splits the computation into disjoint tasks and handles the data after. Domain decomposition is more appropriate than functional decomposition if there is a significant data overlap.
More generalized functional decomposition can also be seen as splitting a multivariate function $f(x_1,x_2,...,x_n)$ into multiple functions ${g_1,g_2,...g_m}$ such that $f(x_1,x_2,...,x_n) = \theta(g_1(x_1,x_2,...,x_n),g_2(x_1,x_2,...,x_n),...g_m(x_1,x_2,...,x_n))$ where $\theta$ is another function combining the results. Every function $g_n$ gets the same input as $f$. If the functional approach is applied to some algorithmic problem, loops with independent iterations can be mapped to this mathematical model and make parallelizing single iterations possible. If iterations of such loops mapped to a task do have side effect communication or synchronization must be done.
The Communication stage specifies links between data consumers and producers. Domain decomposition problems can have more complex communication infrastructures due to data dependencies. Distributing computation and communication is more efficient than centralizing the algorithm through some central manager communicating with every task. Independent and balanced tasks communication-wise are more capable of performant execution.

Using the PC-stable GPU-accelerated Algorithm proposed in the paper by Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} makes handling the Partitioning and Communication step straightforward. The algorithm is based on the gaussian distribution model and handles only the CI-Test for normal distributed data. For simplicity purposes, the thesis implementation incorporates the same approach and looks into other CI-Tests later.

\begin{algorithm}
    \caption{Adjacency search of PC-stable algorithm with gaussian distribution model \cite{schmidtOrderIndependentConstraintBasedCausal2018, colomboOrderIndependentConstraintBasedCausal}}
    \label{alg:pcstable_gaussian}
    \begin{algorithmic}[1]
    \Require Vertex set $V$, correlation matrix $Cor$
    \Ensure Estimated skeleton $C$, separation sets \textbf{Sepset}
    \State with fully connected skeleton $C$ and $l = -1$
    \Repeat \label{alg:pcstable_gaussian:level_loop}
        \State $l=l+1$
        \For{all Variables $V_i$ in $C$}
            \State Let $a(V_i) = adj(C,V_i)$
        \EndFor
        \Repeat \label{alg:pcstable_gaussian:edge_loop}
            \State Select pairs $(V_i,V_j)$ adjacent in $C$ with $|a(V_i)\backslash\{V_j\}| \geq l$
            \Repeat \label{alg:pcstable_gaussian:sepset_loop}
                \State Choose separation set $S \subseteq a(V_i ) \backslash \{V_j\}|$ with $| S | = l$
                \If{$p(V_i,V_j|S) \leq \alpha$}
                    \State Delete edge $V_i - V_j$ from $C$
                    \State Set Sepset(i,j) = Sepset(j,i) = S
                \EndIf
            \Until{edge $V_i - V_j$ is deleted in $C$ or all $S \subseteq a(V_i) \backslash \{V_j\}$ with $|S| = l$ have been chosen}
        \Until{all adjacent vertices $V_i$, and $V_j$ in $C$ such that $|a(V_i)\backslash\{V_j\}| \geq l$ have been considered}
    \Until{each adjacent pair $V_i$, $V_j$ in $C$ satisfy $|a(V_i)\backslash\{V_j\}| \geq l$}
    \State \textbf{return} C, Sepset
    \end{algorithmic}
\end{algorithm}

As can be seen in algorithm \ref{alg:pcstable_gaussian} the main input next to the vertices is the correlation matrix for those vertices. The correlation matrix is necessary for the gaussian-based CI-Test. The algorithm \ref{alg:pcstable} is a CI-Test generalized version of the algorithm \ref{alg:pcstable_gaussian}.
As already stated, loops can be split into tasks if their iteration execution flow is independent of the input of other iterations. This statement does not apply to the repetition in line \ref{alg:pcstable_gaussian:level_loop} because every level depends on its predecessor's results. Synchronization must occur between each level.
Only two loops are independent iteration-wise. The proposed algorithm can be split between different edge as in line \ref{alg:pcstable_gaussian:edge_loop} or between different separation sets as in line \ref{alg:pcstable_gaussian:sepset_loop}. The smaller and, therefore, the correct task for the Partitioning phase is a separation set split.

In the proposed GPU-accelerated algorithm variant of Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} level 0 is split into edges, because there is only one separation set $S \subseteq a(V_i ) \backslash \{V_j\}$ with $| S | = l$ which is the empty set ${}$. Therefore only one CI-Test has to be done per edge.
Level $1,2 ... m$ all split into per separation set tasks in the Partitioning stage. Performance improvements can be made by having an early stop criterion such as a deleted edge, but this requires additional communication between tasks. Data that is shared can not be properly aligned to the tasks. Task data access is nearly randomized on the input correlation in level 1 and following because the separation sets can be build from all existing vertices. 

%- Partitioning und Communication genauer erklären
%- GPU paper basis
%    - partitioning schon getan
%    - Task ist eine edge
%    - Wiederverwenden
%    - Durch parallelisierung auf processing units weiterhin diese partitionierung
    
\subsection{Agglomeration}
The Agglomeration stage uses the correct algorithm produced by the Partitioning and Communication stages and specializes that for the execution environment. Tasks are agglomerated so that their execution is efficient. In the purpose of this thesis, the execution environment is a heterogeneous system containing CPUs and GPUs as processing units. The resulting number of tasks of the Agglomeration stage can be greater than the number of processing units. The subsequent conflicting decisions are necessary to guide through the Agglomeration stage:

\begin{enumerate}
    \item Reduce communication cost through agglomerated computation
    \item Preserve flexibility concerning the later Mapping stage and maintain parallelism
\end{enumerate}

With regards to the GPU implementation of the paper by Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018} which splits the algorithm into separation set tasks and parallelizes those on the GPU, agglomerating those tasks into larger task sets can be done by 
taking the next bigger loop into consideration. The per edge iteration in line \ref{alg:pcstable_gaussian:edge_loop} is suitable for combining multiple tasks because it is also not dependent on the other edge iterations while also being able to be parallelised even more on the underlying processing unit, that executes the edge.

The PC-stable algorithm is commonly used in high-dimensional settings with thousands of variables \cite{nagarajanFunctionalRelationshipsGenes2010}. An edge between each vertex representing a variable can exists so that $|E| \leq |V|^{2}$. Therefore millions of edges might exist per level, so that agglomerating an edge as a task is not feasible for a GPU-CPU heterogeneous system.

%TODO Maybe add illustration for both graphs (adj list vs matrix)
Looking further into the GPU side implementation leads to another possible task that agglomerates multiple edges. The adjacency set populated in the algorithm can be implemented using an adjacency matrix or adjacency list. An adjacency matrix is a standard graph representation that encodes every edge between two vertices $V_1, V_2$ as a field in the matrix at some position to $A_G(V_1, V_2) = \{0,1\}$ where 1 represents an existing and 0 a deleted edge. This representation is constant in its memory usage, and accessing a random element can be done in O(1). At every level, this representation is used to delete edges from the graph. Because of the order independence nature of the algorithm, another copy of the graph exists, which is only read from and the changes of the graph used for deletion are migrated to this graph at the end of each level.
Adjacency lists, on the other hand, are memory-efficient because only existing edges are stored in memory. Every row in the adjacency matrix represents a vertex with a list of vertices connected to the represented vertex. So the edge count of some vertex is the length of the vertex row. For performance reasons, the length of each row is calculated before each level and appended to the row. This representation is only used for reading existing edges in the level. Since every edge has to be processed, an index-based approach is used and with that accessing a single edge can still be done in $O(1)$.
In between each level, the graph that contains the edge-changes migrates to the adjacency list representation. This step is called the compacting step \cite{zarebavaniCuPCCUDAbasedParallel2018}.

\begin{lstlisting}[language=C++, caption=Basic separation set loop in level 1, label=lst:sepsetloopl1]
    for (size_t offset = threadIdx.x; offset < row_length; offset += blockDim.x)
    {
        double pVal = performIndependenceTestForSeparationSet(
            blockIdx.x,
            blockIdx.y,
            offset);
        if (pVal > alpha) {
            deleteEdge();
        }
    }
\end{lstlisting}

In the listing \ref{lst:sepsetloopl1} the separation set loop executed per thread in a CUDA block is shown. The separation set tests for an edge are therefore distributed equally for all threads in a CUDA block. Due to that, a single CUDA thread can perform multiple independence tests. Because of the long execution time of the CI-Test, the looping over multiple CI-Tests mainly affects the thread's execution time. In higher levels, the behavior is comparable. Instead of looping over the edge of a row, a more complex mechanism is used to loop over all permutations of separation sets whose size is equal to the level number.

Consequently, in level 1 and higher, the execution time of a single thread can be linked to the iteration count. With $I_n$ as the maximum iteration count of a row, the execution time can be defined as $O_{\text{CUDA-Thread}}(I_n)$. A CUDA thread itself is the smallest execution unit of the GPU and thus determines the sequential part of the execution. In regards to Amdahl's law \cite{amdahlValiditySingleProcessor1967} a theoretical speedup of a fixed workload through parallelization is limited by the sequential part. If the fixed workload of processing a row placed on the GPU is limited by the sequential part, the minimum execution time of this row per level is $O_{\text{Row}_n}(I_n)$ if every edge is processed in parallel. If the execution of every edge in a row is proportional to the maximum possible iterations which is proportional to the row length, the execution time og a single row can be estimated by the row length.

Applying Amdahl's law to a whole level as a fixed workload, the sequential part of that level processed by the GPU is the longest row in the adjancency matrix and speedup can be realized by minimizing the longest row. So the execution time of that level on the GPU can also be written as $O_{l}(\max I_n) \equiv O_{l}(\max L_n)$ where $L_n$ is the length of the row $n$. According to Amdahl's law, a possible speedup for the workload of the GPU is only possible by minimizing the sequential part. By adding the CPU as an additional processing unit which is also specialized on executing sequential tasks fast, moving the longest rows of the adjacency matrix to the CPU is the obvious approach.

The agglomerated task that is mapped to the processing units in the Mapping stage is therefore defined as one or more rows of the adjacency matrix. The length of those rows influences the run-time of that task and is used for the mapping by limiting the GPUs sequential part.
%TODO Add Gpu pseudocode for better understanding of iteration length etc

% - Agglomeration genauer erklären
% - Durch adjazenz matrix basis bietet sich row basiert an
% - row bundelt mehrere edges und ist damit etwas geeigneter als Task
% - einzelne edge können weiterhin parallel bearbeitet werden auf den PUs
% - genauer die ausführung der GPU anschauen
%  - GPU als basis -> wie geschwindigkeit der GPU erhöhen
%  - serieller/paralleler anteil
%  - GPU Block besteht meist aus 64 Threads
%  - (Beispiel code für ausführung)
%  - Iterationsanzahl bestimmt längste thread ausführung
%  - iterationsanzahl ist abhängig von row length
%  - Testanzahl berechnen aus und daraus iteration (wie)
%  - Diagram Iterations länge
%  - erklären wie dadurch der serielle anteil gekürzt werden kann
 
% \subsubsection{Compact Adjacency Lists}
% - durch compact step werden adjacency lists zwischen leveln berechnet
% - länge der liste ist anzahl an edges in einer row
% - letztes element genutzt als längenspeicher
% - grafik zur verbesserten darstellung
% - wird auch in \cite{zarebavani_cupc_2018} verwendet

\subsection{Mapping}
The last stage of Foster's Methodology, called Mapping, applies the tasks created in the Agglomeration stage to the given system. This stage is vital for heterogeneous systems because the underlying system has to be utilized efficiently. Utilizing heterogeneous systems means maximizing CPU and GPU usage and minimizing inter-processor communication. Minimizing the inter-processor communication includes the connection between GPU and CPU and the core/thread connections in the GPU and CPU.
Mapping the tasks on the system PUs is also called scheduling. There are two common ways of scheduling tasks: Static and dynamic scheduling \cite{kopetzRealTimeScheduling1997}.
A scheduler is called static if it makes its scheduling decisions pre-execution. The static scheduler needs prior knowledge of the capabilities each processing unit has. The static scheduler needs to know the performance implications and characteristics of the given task set so that the tasks are balanced optimally on the system. Therefore a static scheduler that does not schedule on compile time is also called a static load-balancer.
A scheduler is called dynamic if scheduling decisions are done at run-time. Due to the run-time decision, the current system state can be taken into account. Prior knowledge about the processing unit capabilities is not necessary. Knowledge about the task set and its performance implications is still helpful for optimally balancing tasks onto the heterogeneous PUs.
A special case of a dynamic scheduling algorithm is the work-stealing scheduler. While one processing unit executes the whole task set, the others "steal" tasks at run-time from the executing PU to minimize the workload gradually.
% - Mapping genauer erklären
% - Scheduling spezifizieren
%     - Static/Dynamic scheduling
%     - static = prebalanced
%         vor nachteile
%     - dynamic 
%         vor nachteile
% - Two approaches pre balanced and workstealing

\section{Heterogeneous PC Algorithm Approaches}
Using the insights gathered from Foster's Methodology applied to the heterogeneous PC-stable algorithm, two approaches arise. First, the static scheduling approach that balances rows pre-execution on the heterogeneous system. This approach is called the pre-balanced approach. The second approach uses dynamic scheduling by work-stealing rows from an executing processing unit. Both approaches use the length information gained from the adjacency matrix to place the rows with the longer estimated run time on the CPU, which is specialized for executing sequential tasks fast. Every unscheduled row is executed on the highly parallel GPU.

\subsection{Pre-Balanced Approach}
Static scheduling is done ahead of execution and, in most cases, at compile time \cite{singhSurveyStaticScheduling2015}. Because of the varying size of task and dataset dimensions, the scheduling approach proposed schedules between levels and just before execution. A task, which is defined as a row of the adjacency matrix, is scheduled using its length as an identifier for the execution time parameter. The scheduled tasks are passed to the execution unit, and all execution units process their tasks concurrently. The different characteristics of the given execution units have to be considered during load balancing. The GPUs processing power lies in parallelization. Therefore most tasks should be scheduled on this processing unit. The task with the estimated longest execution time gets placed on the CPU, which is better at processing serial tasks than the GPU. The ratio between tasks placed on the CPU and tasks placed on the GPU is an estimation and can be optimized using hardware characteristics. Such a characteristic used in this approach is the core count of the CPU, which can be an indicator of how many tasks can be processed concurrently. Additional characteristics that could improve the balance of tasks placed on the processing units are the clock-speed of both CPU and GPU, which can approximate the speed at which tasks are processed and the maximum thread count schedulable on the GPU. Because of the approximative nature of the static scheduling approach, each additional parameter used does not necessarily mean a better balancing. Balancing parameters need tuning, which is done by adding and setting thresholds.
The maximum count of tasks placed on the CPU is thresholded by a proportion of the CPU core count. Because of the specific execution times of a single test in different levels, this threshold is tuned for each level.
During the processing of the assigned tasks, the CI-Test might access data associated with nodes in the iterated separation sets. The separation set composition cannot decided before execution, and due to that, a random access pattern is assumed. For concurrently processed tasks in a  heterogeneous system, a random memory access pattern can imply that both CPU and GPU access the same data. Even though this access could be only read access, memory locality is not always given in such situations. Therefore data has to be copied over the CPU-GPU interconnect. Interconnect memory access is slow latency in comparison to local processing units memory access and should be avoided. Tools such as "nvprof" by Nvidia can help profiling interconnect access by counting page migrations between the CPU and GPU. Additionally, the page size can be multiple times larger than the data accessed. By placing the node-associated data next to each other in memory, access to nodes close to each other can also trigger the same effect.

In early tests of this approach, many page migrations occurred by deleting edges concurrently in the graph. An edge migration step is added to the CPU task-processing that minimizes the effect of contemporary edge deletions. Edge deletions on the CPU are queued and migrated after the GPU task-processing is done.
Data that is only read during the task execution can be copied in both the CPU and the GPU memory to prevent page migrations and boost memory access performance. In the case of gaussian CI-Tests, a global covariance matrix is only read through the whole algorithm. For performance optimization, this covariance matrix is stored in both the CPU and the GPU memory.
% - Static scheduling ansatz
% - Load balancer schiebt längste rows auf CPU
% ...
% - Viel Memory/ communication -> nvprof?
% - Limitieren durch Migrierungsschritt der CPU

\subsection{Workstealing Approach}
While the pre-balanced approach works through setting thresholds pre execution and using hardware charactersticts to estimate execution time, this estimation can vary a lot from the real execution time. This problem does not apply to a dynamic scheduling approach. In dynamic scheduling, tasks are scheduled during execution time by placing the task on an idle execution unit. There are different types of dynamic scheduling algorithms applicable to this problem. Because of the chosen structure of the GPU as a main executing component and the CPU helping by taking off the longest serial execution time tasks, a workstealing approach fits best. The workstealing scheduling strategy is often used in multithreading. A processor has a fixed processing core counts and the workstealing algorithm maintains a queue with threads to be executed. As soon as the queue is not empty a core that is done with its current thread, "steals" the next thread of the queue, which is then not executable by other threads. The workstealing algorithm has to maintain the execution order, that is necessary for a correct execution. \cite{blumofeSchedulingMultithreadedComputations1999}
In the case of the order-independent PC-stable algorithm, the execution order is irrelevant. Due to that a simplified version of the workstealing algorithm can be used, where the execution units take the next open task.
% - Dynamic scheduling
% - Workstealing erklären (herkunft etc)
% - GPU läuft normal durch
% - CPU nimmt sich längste rows als erstes vor und arbeitet von hinten ab
% - regelmäßiges überprüfen ob schon fertig
% - vermehrte kommunikation

\section{Implementation}

\subsection{Architecture}
% - could be integrated into the Approach sections
% - one state containing unified memory data
% - Executor abstraction
% - Task as a struct containing row index and how many rows
% - Load balancer invoked only in pre-balanced case
% - else GPU has all rows
% - Workstealing approach guards
% - Maybe show some code