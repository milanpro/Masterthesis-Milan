%!TEX root = ../thesis.tex
\chapter{Background}
\label{chap:background}
This chapter introduces basic terminology of parallel programming, heterogeneous computing, heterogeneous systems, and the processing units frequently used in such systems. While heterogeneous systems enable heterogeneous computing algorithms, they can consist of processing units of various kinds. The focus of this thesis lies on the central processing unit (CPU) and the graphics processing unit (GPU) as the execution units of a heterogeneous system.
In Section \ref{sec:cbcsl}, the theoretical background of Causal Structure Learning and the PC algorithm will be explained.

\section{Parallel Programming}
While heterogeneous systems are more complex than simple CPU systems, the same parallel programming paradigms apply. In this section, basic terminology and concepts are explained to introduce heterogeneous computing concepts later.

A \textbf{workload} is a collection of operations within an application or program to be executed on a computer with its defined inputs and desired outputs \cite{pfisterSearchClusters1998}. An algorithm implemented in a programming language and run on a computer is a workload because of its defined purpose and inputs/outputs. A workload itself can be split into \textbf{tasks}. A \textbf{task} is a subset of some workload and represents independent operation sequences \cite{breshearsArtConcurrencyThread2009}. An operation affects the execution state and is considered atomic. Decomposing a workload into independent tasks makes parallelization feasible and is the basic step of parallel programming.

\textbf{Parallelism} is the capability of a machine to perform multiple tasks simultaneously and requires parallel hardware. \textbf{Concurrency} is the capability of a machine to have multiple tasks in progress at the same time. Every parallel program is also a concurrent program. Concurrent execution tends to face the same challenges as parallel execution, and those terms can therefore be used interchangeably in such contexts. Parallel programming emerged with the introduction of hardware that has multiple \textbf{execution units} (e.g., cores in a CPU) \cite{mattsonPatternsParallelProgramming2004} which can execute separate tasks in a workload in parallel. This kind of parallelization can be beneficial for the execution time of a workload since more work can be done simultaneously. A \textbf{processing unit} consists of one or more \textbf{execution units}, and both terms are used interchangeably for an executing element of tasks in this thesis.

The speedup of some workload through parallelization is limited by the workload's longest-running sequential task, which cannot be split further. This task is also called the \textbf{sequential part} of the workload. Using Amdahl's law \cite{amdahlValiditySingleProcessor1967}, the maximum speedup can be estimated. Gustafson's law \cite{gustafsonReevaluatingAmdahlLaw1988} adds a potential improvement of the systems resources to Amdahl's statement and states that by adding processors, the execution time still converges to a fixed limit of the sequential part. Therefore including additional execution units has limited effects on the speedup of the workload.

Tasks can also share common resources, such as files, network access, or memory locations. By accessing shared resources in parallel, the results of tasks are sensitive to execution order. This is also called a \textbf{race condition} \cite{dijkstraCooperatingSequentialProcesses2002}. For parallel execution, where execution order is relevant for correct results, \textbf{synchronization} is accomplished. Synchronization ensures correct execution order by introducing \textbf{atomic} operations that are executed in a single atomic step, without other tasks interfering in the shared atomic resource, or \textbf{critical sections}, limiting access to the shared resource for the execution unit in the operation section. While synchronization through critical sections can be necessary for correct results with data dependencies between tasks, synchronization also introduces execution time overhead, which should be minimized \cite{breshearsArtConcurrencyThread2009, shoshaniSequencingTasksMultiprocess1970}.

In heterogeneous systems, where multiple different and specialized execution units exist, reducing latency through intelligent mapping of tasks can be achieved. Intelligently mapping the sequential part, its execution time can be further reduced, and parallel programs can benefit from further speedup. Heterogeneous computing is introduced to utilize the mapping of tasks onto the best fitting processing units.

\section{Heterogeneous Computing}
Heterogeneous computing refers to computation on heterogeneous systems that include conventional (e.g., CPU) and specialized (e.g., GPU, FPGA) processors. The different processors in a heterogeneous system work cooperatively on the same workload \cite{shanHeterogeneousProcessingStrategy2006}. Heterogeneous computing uses more than one processing unit of the system's resources and increases execution effectiveness. Heterogeneity in the computing context refers to different instruction-set architectures (ISA), which means that the processors in the system are heterogeneous in their architecture.
Abstractions can hide heterogeneity and eliminate development complexity for heterogeneous processing units. However, since heterogeneous processing units have different advantages and disadvantages, developers still have to handle the optimal and efficient usage of the processing units.

Most modern high-performance computing systems are heterogeneous and consist of one or more CPUs and one or more GPUs. Therefore a heterogeneous computing approach can be beneficial for such systems. CPUs and GPUs differ significantly in their architecture and function, so most modern applications only use the GPU or the CPU effectively.

CPU-GPU heterogeneous computing approaches are also referred to as hybrid or co-operative execution. In most applications, the CPU is called the host, which controls the execution flow, and the GPU, which is also called the device, accelerates the execution of the host. Therefore, the GPU is also called the accelerator. In this thesis, both the CPU and the GPU are called processing units (PU), a term that includes both processor types.

\subsection{Central Processing Unit (CPU)}
The CPU, sometimes just called processor, is the primary instruction executing component of a computer. While modern CPUs introduce parallelism concepts, the primary purpose is still low latency execution of instructions.
Without the introduction of parallelism concepts in modern CPUs, a CPU can only execute instructions serially. The ability to run concurrent applications is mainly achieved by introducing a thread and scheduling concept, which switches between threads periodically using a set of rules. Programs can create such threads using an operating system interface \cite{nemirovskyMultithreadingArchitecture2013}.
Additional performance and parallelization improvements such as superscalar architectures and instruction pipelines, which can be extended through optimizations such as branch prediction or speculative execution, are hidden from the programmer. These performance and parallelization improvements support the primary goal of the CPU, which is providing a robust and moderately fast way to run any code \cite{johnsonSuperScalarProcessorDesign1989, falsafiPrimerHardwarePrefetching2014, smithStudyBranchPrediction1998}.

Starting in the early 1970s, CPU architectures include instruction set extensions for \textbf{data-level parallelism}, called Single Instruction Multiple Data (SIMD) \cite{barnesILLIACIVComputer1968,smartFullyHomomorphicSIMD2014, VectorExtensionsUsing}. SIMD instructions execute operations, typically mutating scalar values, parallelized on multiple scalar values in vector registers. SIMD extensions are common in most modern CPU architectures, such as x86\_64 (SSE, AVX \cite{IntelIntrinsicsGuide}) or PowerPC (AltiVec \cite{diefendorffAltiVecExtensionPowerPC2000}). Mathematical operations benefit from SIMD. In the domain of linear algebra, optimized implementations for commonly used vector and matrix operations are included in Basic Linear Algebra Subprogram (BLAS) \cite{lawsonBasicLinearAlgebra1979} libraries. To design an optimal CPU-side implementation for this thesis, using BLAS is essential and is considered to use the CPU resources optimally.

Modern CPUs have multiple cores that can independently execute instructions and run multiple threads concurrently. Running multiple tasks in parallel via threads is also called \textbf{task-level parallelism}.
Concurrency between threads that share a common resource introduces another problem: Since there can not be any assumption about the relative speed of those threads, analog interferences are possible \cite{netzerWhatAreRace1992}. Two threads accessing a resource at nearly the same time can cause a race condition since there is no way of identifying the thread that accesses the resource first. The problem of an undefined ordering while accessing data can cause non-deterministic behavior. Critical sections for safe concurrent access are introduced to prevent those race conditions. At any moment, only one thread is allowed into the critical section shared between the threads. The implementation of a critical section requires atomic read/write behavior.
Access mechanisms, such as atomic read/write and critical sections, are also crucial in heterogeneous computing, where concurrent memory access of multiple processors can occur.
Frameworks such as OpenMP \cite{dagumOpenMPIndustryStandard1998} can help with multi-threading and safe concurrency by providing abstractions of those concepts. OpenMP, a shared/memory parallel programming framework for C/C++ and Fortran, provides annotations for parallel regions like parallelizable loops and creates multiple execution threads for those regions using a run-time.
The thesis implementation uses OpenMP for efficiency and parallelism purposes.

In conclusion, CPUs can execute parallelized workloads on multiple abstractions (e.g., cores, SIMD), and therefore, parallelized algorithms profit from being processed by a CPU. "Dark Silicon" \cite{esmaeilzadehDarkSiliconEnd2011}, which limits the addition of cores on-chip because of problems arising from core density in combination with heat and voltage, prevent the scaling of CPU parallelization by an upper bound. Due to this upper bound on parallelization and high CPU clock rates combined with caching mechanisms, latency-critical and serial workloads benefit from running on the CPU in comparison to highly parallel workloads. Utilizing highly parallel workloads can be done by incorporating accelerators that are specialized in parallelism, such as the graphics processing unit in a system.

\subsection{Graphics Processing Unit (GPU)}
The GPU handles graphics processing in computer systems. Before around 2001, video games were the main driver for technical advancements in developing better GPUs \cite{vuducBriefHistoryIntroduction2013}. However, today, scientific applications such as machine learning or crypto mining have become promising use cases of GPUs and drive the development of the GPU too. Modern GPUs can be programmed as general-purpose processing units \cite{nickollsGPUComputingEra2010}, and the design of modern GPUs is significantly different from the first GPUs, which had hard-coded pipelines. For graphics computing, the processing of multiple calculations in parallel is essential. Image and video processing primarily execute the same operation on multiple pixels in parallel. Modern recordings and screens can have up to 8 million or more pixels transformed in real-time (i.e., 60 times per second). Such high throughput rates are only possible with a domain-specific processor such as the GPU that can execute thousands of operations concurrently.

Leveraging the general-purpose programming capabilities of modern GPUs, other problems that require handling large data in parallel can be processed. These capabilities allow scientists and high-performance programmers to boost their parallelizable problems on the GPU \cite{cohenSolvingComputationalProblems2009,nickollsScalableParallelProgramming2008,stoneAcceleratingMolecularModeling2007}.
Another benefit of GPUs in comparison to CPUs is their energy efficiency \cite{qasaimehComparingEnergyEfficiency2019}. GPUs are more energy-efficient and time-efficient when executing highly parallel workloads than CPUs and are commonly incorporated in high-performance heterogeneous systems such as supercomputers \cite{dellGPUExpanseHPC2020}.
In Nvidia GPU architectures, such as Nvidia Volta \cite{NVIDIATESLAV1002017}, the GPU consists of many Streaming Multiprocessors (SM) that execute multiple thread blocks in parallel. A warp executes a thread block on Nvidia GPUs on the hardware side in the Streaming Multiprocessor. Such a warp incorporates many simple cores that are executed in SIMD groups with a single program counter. A warp on Nvidia Volta GPU architecture consists of 32 simple cores which share a common memory region.

Programmers of GPUs must be aware of the discussed architecture semantics to optimally use the underlying cache, memory, and computation structure. A thread block should, for example, be sized according to multiples of the size of a warp, even if other thread block sizes are allowed. Nvidia Volta has 64 warps per SM, which translates into a maximum of 2048 threads scheduled concurrently on that SM \cite{NVIDIATESLAV1002017}.
Frameworks such as OpenCL or CUDA are application programming interfaces (APIs) created to allow software developers to develop general-purpose applications running on the GPU. CUDA is a super-set language of C/C++ that allows the definition of functions that are run on the GPU and CPU \cite{EasyIntroductionCUDA2012,nickollsScalableParallelProgramming2008,sosuthaHeterogeneousParallelComputing2015}. Function annotations such as \texttt{\detokenize{__device__}}, \texttt{\detokenize{__global__}} and/or \texttt{\detokenize{__host__}} tell the compiler on which processing unit the code can be executed. Shared CPU-GPU code is possible, allowing code reusage on a heterogeneous system. Other advantages, such as unified memory abstractions for heterogeneous systems, are a deciding factor for using CUDA in this thesis.

\begin{lstlisting}[language=C++, caption=CUDA kernel example \cite{EasyIntroductionCUDA2012}, label=lst:kernel]
__global__
void add(int n, float *x, float *y)
{
  int index = threadIdx.x;
  int stride = blockDim.x;
  for (int i = index; i < n; i += stride)
      y[i] = x[i] + y[i];
}
\end{lstlisting}
In listing \ref{lst:kernel}, a simple "kernel" function that adds an array to another array in parallel on the GPU is shown. The \texttt{\detokenize{__global__}} keyword tells the compiler to create machine code suitable for an Nvidia GPU. By invoking this kernel, multiple instances of this function are scheduled in thread blocks on the GPU. The thread id of each invocation is accessed through the variable \emph{threadIdx}, and the number of threads in the block is accessed using the variable \emph{blockDim}. When starting multiple blocks, the block id of the kernel invocation can be accessed through the variable \emph{blockIdx}.
Data passed into the function such as the arrays x and y are created by CUDA run-time API functions, which handle memory allocation on the GPU \cite{MemoryManagement}.

Nvidia GPUs used as accelerator devices have two different memory spaces. Data structures are created and allocated on the host or CPU memory space. After allocating memory on the GPU, the data must be moved to the device from the CPU either by a direct transfer or by relying on a Unified Virtual Memory system accessible from both CPU and GPU. The CUDA Runtime API enables programmers to do both. In most cases, a single data transfer is performed by allocating memory on the GPU with \emph{cudaMalloc} and copying the data to the GPU with \emph{cudaMemcpy}. In the heterogeneous computing context, where data needs to be accessed by both the CPU and the GPU simultaneously, a Unified Virtual Memory (UVM) system helps with the data management. Since CUDA version 6.0, Nvidia introduced \emph{cudaMallocManaged} which handles necessary data transfers in both directions based on pagination \cite{gayatriComparingManagedMemory}. Both host and device keep track of the pages using a page table. On memory access, missing pages migrate to the correct location.
For performance reasons, the CUDA run-time API allows for prefetching data to a device so that page misses, which can slow down the computation performance a lot, are prevented.

% - Hardware characteristics
% - CUDA Basics
% - Nvidia GPU Basics (Warp, Block, Streaming Multiprocessor)
% - Memory management
% - Unified Memory
% - 

\section{Constraint-Based Causal Structure Learning}
\label{sec:cbcsl}
This section introduces the necessary terminology in Causal Structure Learning, which is the problem class considered in this thesis. The Causal Graphical Model and the PC algorithm are elaborated on based on the definitions given by Pearl \cite{pearlCausality2009}, and Spirtes et al. \cite{spirtesCausationPredictionSearch1993}. The order-independent GPU-based PC algorithm is the base for the heterogeneous approach of this thesis \cite{schmidtOrderIndependentConstraintBasedCausal2018}.

\subsection{Causal Graphical Model}
Given a finite set of $N$ vertices $V = (V_1,...,V_N)$ and a set of edges $E \subseteq V \times V$, let a graph G be $G = (V,E)$. The vertices each represent a variable. An edge $(V_i, V_j) \in E$ is called directed, i.e., $V_i \rightarrow V_j$, if $(V_i,V_j) \in E$ but $(V_j, V_i) \notin E$. An edge is called undirected, if both $(V_i,V_j) \in E$ and $(V_j, V_i) \in E$, i.e. $V_i - V_j$. Two vertices $V_i$, $V_j$ are called adjacent if there is an undirected edge $V_i - V_j$. The adjacency set $adj(G, V_i)$ of the vertex $V_i \in V$ in $G$ are all vertices $V_j \in V$ that are connected to $V_i$ by an undirected edge.

Some graph $G$ where all edges are directed, and $G$ does not contain any cycle is a Directed Acyclic Graph (DAG). In the context of Causal Structure Learning, a directed edge $E$ in such a DAG represents a direct causal relationship between the connected vertices.
The d-separation criterion used in the DAG enables identifying conditional independence between variables \cite{geigerIdentifyingIndependenceBayesian1990}. Two variables $V_i, V_j \in V$ are conditionally independent given a set $S \subset V \backslash \{V_i, V_j\}$ if the vertices $V_i$ and $V_j$ are d-separated by the set $S$, which is called separation set.
A distribution $P$ of the variable set $V_1, ..., V_N$ that satisfies the above condition is called faithful.
Several different DAGs that describe the same conditional independence information form a Markov equivalence class. \cite{anderssonCharacterizationMarkovEquivalence1997}. If two DAGs are Markov equivalent, they share the same skeleton $C$ or the underlying undirected graph and the same v-structures.
These v-structures are triples $V_i,V_j,V_k$ with $(V_i,V_k) \notin E$ and $(V_k,V_i) \notin E$ and directed edges $V_i \rightarrow V_j$ and $V_k \rightarrow V_j$. Moreover, it is possible to uniquely describe the corresponding Markov equivalent class by a Complete Partially Directed Acyclic Graph (CPDAG) \cite{chickeringOptimalStructureIdentification2003}.
A CPDAG is a partially directed acyclic graph where all DAGs in the Markov equivalence class incorporate the same directed edges, and there exist two DAGs that incorporate the two directed versions of every undirected edge $V_i - V_j$ in the Markov equivalence class.
Hence, the focus lies on estimating the equivalence class of the DAG G based on the corresponding probability distribution $P$ of the involved variables $V_1,...,V_N$. In particular, under the assumption that the distribution $P$ is generated from the true causal DAG $G$, there is an edge $V_i - V_j$ in the skeleton of $G$ if and only if $V_i,V_j$ are dependent given all $S \subseteq V\backslash \{V_i,V_j\}$ \cite{spirtesCausationPredictionSearch1993}.
Hence, the examination of the conditional independence information of the observed variables $V_1,...,V_N$ allows for estimating the undirected skeleton $C$ of the corresponding DAG $G$. The extension of the skeleton $C$ to the equivalence class of the DAG $G$ can be made by the repeated application of deterministic edge orientation rules on the skeleton \cite{colomboOrderIndependentConstraintBasedCausal2014,kalischEstimatingHighDimensionalDirected2007,pearlIntroductionCausalInference2010}.

\subsection{Constraint-Based Methods}
In Causal Structure Learning, two main approaches have been proposed: the constraint-based method and the score-based method. While in the constraint-based method, conditional independence tests (in the following called CI-Tests) are used to recover the CPDAG, score-based algorithms search for a high score rank to find the best fitting CPDAG. Both approaches have their advantages and disadvantages. Choosing the best approach is mostly dependent on the type of data. Constraint-based methods, for example, are more accurate on datasets with a small sample size \cite{scutariBayesianNetworkConstraintBased2017}. Exemplary constraint-based approaches are PC \cite{spirtesCausationPredictionSearch1993}, Rank PC \cite{harrisPCAlgorithmNonparanormal2013}, PC-stable \cite{colomboOrderIndependentConstraintBasedCausal2014}, IC \cite{vermaEquivalenceSynthesisCausal1990}, and FCI \cite{spirtesCausationPredictionSearch1993}.
The constraint-based PC-stable algorithm by Colombo et al. \cite{colomboOrderIndependentConstraintBasedCausal2014} lays the basis for this thesis work by introducing order independence in between different CI-tests and, due to that, the possibility to parallelize the algorithm easily. For the same reason, different parallelized GPU and CPU implementations of the PC-stable algorithm exist \cite{schmidtLoadBalancedParallelConstraintBased2019,schmidtOrderIndependentConstraintBasedCausal2018,zarebavaniCuPCCUDAbasedParallel2018}, which influenced the approach described in this thesis.
The PC algorithm avoids conditional independence checks for each pair of vertices $V_i, V_j \in V$ given all possible subsets $S \subseteq V \backslash \{V_i, V_j\}$ \cite{pearlTheoryInferredCausation1995} since processing all conditional independence checks would become infeasible for a large number of vertices. The PC algorithm independence is tested incrementally in multiple levels by increasing the separation set size in each level. Edges where the given independence test successfully meets a defined condition are deleted. By deleting edges in a level, fewer edges have to be considered in higher levels, and the test count is reduced. Reducing the test count level-wise on sparse matrices can decrease the exponential complexity to polynomial complexity concerning the number of vertices \cite{kalischEstimatingHighDimensionalDirected2007}.
The PC-stable variant of the PC algorithm removes the dependency on the order of the variable set $V_1, ...,V_N$ \cite{colomboOrderIndependentConstraintBasedCausal2014,schmidtOrderIndependentConstraintBasedCausal2018}.

\subsection{PC Algorithm}
The PC algorithm was designed for learning DAGs. It assumes causal sufficiency, which means there are no unmeasured common causes. The PC algorithm is mainly used in settings based on high-dimensional data and aims to perform well for sparse graphs that contain thousands of variables \cite{kalischUnderstandingHumanFunctioning2010}.
The version of the PC algorithm described in Colombo et al. \cite{colomboOrderIndependentConstraintBasedCausal2014} is also called PC-stable because of its order independence regarding the different calculations. Order independence of the computations helps to parallelize the PC algorithm by not relying on synchronization. The PC-stable algorithm is the algorithm basis used for the parallelization efforts in this thesis. The term PC algorithm is used as a synonym for the PC-stable variant in the following chapters.

% TODO CI test dependency => change to algorithm of colombo
\begin{algorithm}
    \caption{Adjacency search of PC-stable algorithm \cite{colomboOrderIndependentConstraintBasedCausal2014}}
    \label{alg:pcstable}
    \begin{algorithmic}[1]
    \Require Vertex set $V$, Additional data necessary for the CI-Test
    \Ensure Estimated skeleton $C$, separation sets \textbf{Sepset}
    \State with fully connected skeleton $C$ and $l = -1$
    \Repeat 
        \State $l=l+1$
        \For{all Variables $V_i$ in $C$}
            \State Let $a(V_i) = adj(C,V_i)$ \label{alg:pcstable_adjlist}
        \EndFor
        \Repeat \label{alg:pcstable_edgeloop}
            \State Select pairs $(V_i,V_j)$ adjacent in $C$ with $|a(V_i)\backslash\{V_j\}| \geq l$
            \Repeat
                \State Choose separation set $S \subseteq a(V_i ) \backslash \{V_j \}$ with $| S | = l$ \label{alg:pcstable_sepsbuild}
                \If{$p(V_i,V_j|S) \leq \alpha$} \label{alg:pcstable_alpha}
                    \State Delete edge $V_i - V_j$ from $C$
                    \State Set Sepset(i,j) = Sepset(j,i) = S
                \EndIf
            \Until{edge $V_i - V_j$ is deleted in $C$ or all $S \subseteq a(V_i) \backslash \{V_j\}$ with $|S| = l$ have been chosen}
        \Until{all adjacent vertices $V_i$, and $V_j$ in $C$ such that $|a(V_i)\backslash\{V_j\}| \geq l$ have been considered}
    \Until{each adjacent pair $V_i$, $V_j$ in $C$ satisfy $|a(V_i)\backslash\{V_j\}| \geq l$}
    \State \textbf{return} C, Sepset
    \end{algorithmic}
\end{algorithm}

As can be seen in Algorithm \ref{alg:pcstable}, the starting point of the algorithm is a set of vertices that represent the given variables. The PC algorithm incrementally identifies a skeleton at each level. Starting with a separation set of size 0, $S = \emptyset$, and a complete undirected skeleton $C$ in level 0, independence tests for every edge pair are processed. Every edge between any vertex combination exists in level 0, therefore the independence test count can be calculated as $|P_0| = \frac{(|V| - 1) * |V|}{2}$. In subsequent level iterations, separation sets are non-empty, and the existence of an edge cannot be assumed. For each level, $l$ the separation set $S$ with $|S| = l$ is built from the adjacent vertices $S \subseteq a(V_i ) \backslash \{V_j \}$ as seen in line \ref{alg:pcstable_sepsbuild}.

The result of an independence test is compared to a tuning parameter $\alpha$ in line \ref{alg:pcstable_alpha} and an edge $V_i - V_j$ is deleted in C if $p(V_i,V_j|S) \leq \alpha$. Because the deletion is made in $C$ and vertice pairs and separation sets are built using $a(V_i)$, the deletion does not affect the current levels selection of edges. By deleting edges in $C$, there is no dependence on the order of edges in the loop of line \ref{alg:pcstable_edgeloop}. Through building the adjacency lists $a(V_i)$ at the start of the level in line \ref{alg:pcstable_adjlist}, propagating changes of the previous level in $C$ are assured to be consistent in the following level.

After testing every edge with its possible separation sets $S$ for that level, the next level $l=l+1$ starts. If there is no separation set $S$ with $|S| = l$ for any vertex combination, the maximum level is reached, and the skeleton $C$ is estimated.
This skeleton is then used to apply deterministic orientation rules so that the corresponding CPDAG is found \cite{colomboOrderIndependentConstraintBasedCausal2014,kalischEstimatingHighDimensionalDirected2007,pearlIntroductionCausalInference2010,schmidtOrderIndependentConstraintBasedCausal2018}.
% - Explain PC Algorithm
% - Pseudocode listing
% - Loops