%!TEX root = ../thesis.tex
\chapter{Background}
In this chapter, we will look into parallel programming, heterogeneous computing, heterogeneous systems, and the processing units frequently used in such systems. While heterogeneous systems power heterogeneous computing algorithms, they can consist of processing units of all kinds. The focus of this thesis lies on the central processing unit (CPU) and the graphics processing unit (GPU) as the execution units of a heterogeneous system.
After that, the theoretical background of Causal Structure Learning and the PC algorithm will be explained, which we will connect to the possible execution on heterogeneous systems.

\section{Parallel Programming}
While heterogeneous systems are more complex than simple CPU systems, the same parallel programming paradigms apply. In this section, basic terminology and concepts are explained to introduce heterogeneous computing concepts later.

A \textbf{workload} is some defined applications or programs collection of operations to be executed on a computer with its defined inputs and desired outputs \cite{pfister1998search}. An algorithm implemented in a programming language and run on a computer is a workload because of its defined purpose and inputs/outputs. A workload itself can be split into \textbf{tasks}. A \textbf{task} is a subset of some workload and represents independent operation sequences \cite{breshearsArtConcurrencyThread2009}. An operation affects the execution state and is considered atomic. Decomposing a workload in independent tasks leads to possible parallelization and is the basic step of parallel programming.

\textbf{Parallelism} is the capability of a machine to perform multiple tasks simultaneously and requires parallel hardware. \textbf{Concurrency} is the capability of a machine to have multiple tasks in progress at the same time. Every parallel program is also a concurrent program. Concurrently executed tasks are primarily prone to have the same challenges as parallel executed tasks, and those terms can therefore be used interchangeably in such contexts. Parallel programming emerged with the introduction of hardware that has multiple \textbf{execution units} (e.g., cores in a CPU) \cite{mattsonPatternsParallelProgramming2004} which can execute tasks of a workload in parallel. This kind of parallelization can be beneficial for the execution time of a workload since more work can be done simultaneously. A processing unit consists of one or more execution units, and both terms are used interchangeably for an executing element of tasks in this thesis.

The speedup of some workload through parallelization is limited by the workloads longest-running sequential task, which cannot be split further. This task is also called the \textbf{sequential part} of the workload. Using Amdahl's Law \cite{amdahlValiditySingleProcessor1967}, the maximum speedup can be estimated. By converging to a fixed limit of the sequential part, adding additional execution units has limited effects on the speedup of the workload.

Tasks can also share common resources, such as files, network access, or memory locations, which are the primary shared resource. By accessing shared resources in parallel, the results of tasks are sensitive to execution order. This is also called a \textbf{race condition} \cite{dijkstraCooperatingSequentialProcesses2002}. For parallel execution, where execution order is relevant for correct results, \textbf{synchronization} is done. Synchronization ensures correct execution order by introducing, e.g., \textbf{atomic} operations that are executed in a single atomic step, without other tasks interfering in the shared atomic resource, and \textbf{critical sections}, limiting access to the shared resource for the execution unit in this critical section. While synchronization through critical sections can be necessary for correct results with data dependencies between tasks, they also introduce execution time overhead, which should be avoided \cite{breshearsArtConcurrencyThread2009, shoshaniSequencingTasksMultiprocess1970}.

In heterogeneous systems, where multiple different and specialized execution units exist, reducing latency through intelligent mapping of tasks can be achieved. Intelligently mapping the sequential part, its execution time can be further reduced, and parallel programs can benefit from further speedup. To utilize the possibility of mapping tasks onto better fitting processing units, heterogeneous computing is introduced.

\section{Heterogeneous Computing}
Heterogeneous computing refers to systems that include conventional (e.g., CPU) and specialized (e.g., GPU, FPGA) processors. Those different processors work cooperatively on the same workload \cite{shanHeterogeneousProcessingStrategy2006}. Heterogeneous computing aims to use more than one processing unit of the system's resources and increase execution effectiveness. Heterogeneity in the computing context refers to different instruction-set architectures (ISA), which means that the processors in this system are heterogeneous in their architecture.
Abstractions can eliminate this heterogeneity for the software developer. However, since the heterogeneous processing units have different advantages and disadvantages over others, the developer still has to think about the optimal and efficient usage of the PUs.

Most modern computing systems are heterogeneous and consist of one or more CPUs and one or more GPUs. Therefore a heterogeneous computing approach could be beneficial for such systems. Both the CPU and the GPU are very different in their architecture and function, so most modern applications only use one of those processors effectively.

CPU-GPU heterogeneous computing approaches are also referred to as hybrid or co-operative execution. In most applications, the CPU is called the host, which controls the execution flow, and the GPU, which is also called the device, accelerates the execution of the host. Therefore the GPU is also called the accelerator. In this thesis, both the CPU and the GPU are called processing units (PU) as a term that includes both processors.

\subsection{Central Processing Unit (CPU)}
The CPU, sometimes also just called processor, is the primary instruction executing component of a computer. While modern CPUs introduce parallelism concepts, the primary purpose is still low latency execution of given instructions.
Without those introduced parallelism concepts in modern CPUs, a CPU is only able to execute instructions serially. The ability to run concurrent applications is mainly done by introducing a thread and scheduling concept, which switches those threads periodically using a set of rules. Programs can create such threads using an operating system interface \cite{nemirovskyMultithreadingArchitecture2013}.
Additional performance and parallelization improvements such as superscalar architectures and instruction pipelines, which can be extended through optimizations such as branch prediction or speculative execution, are hidden from the programmer and help support the primary goal of the CPU, which is a robust and moderately fast way to run any code \cite{johnsonSuperScalarProcessorDesign, falsafiPrimerHardwarePrefetching2014, smithStudyBranchPrediction1998}.

Modern CPU architectures include instruction set extensions for \textbf{data-level parallelism}, called Single Instruction Multiple Data (SIMD) \cite{barnesILLIACIVComputer1968,smartFullyHomomorphicSIMD2014, VectorExtensionsUsing}. Those instructions execute operations, typically mutating scalar values, parallelized on multiple scalar values in vector registers. Those SIMD extensions are common in most modern CPU architectures, such as x86\_64 (SSE, AVX \cite{IntelIntrinsicsGuide}) or PowerPC (AltiVec \cite{diefendorffAltiVecExtensionPowerPC2000}). Mathematical operations benefit of SIMD. Especially in linear algebra, optimized implementations for commonly used vector and matrix operations are implemented in Basic Linear Algebra Subprogram (BLAS) \cite{lawsonBasicLinearAlgebra1979} implementations. For realizing an optimal CPU side implementation for this thesis, using BLAS is essential and enables using the CPU resources.

Modern CPUs have multiple so-called cores, which can independently execute instructions and therefore are able to run multiple parallel threads concurrently. Running multiple tasks in parallel via threads is also called \textbf{task-level parallelism}.
Concurrency between threads that share a common resource introduces another problem. Since there can not be any assumption about the relative speed of those threads, analog interferences are possible. Two threads accessing a resource at nearly the same time can result in a race condition since there is no way to identify the thread that accesses the resource first. To prevent those race conditions, critical sections for safe concurrent access are introduced. At any moment, only one thread is allowed into the critical section shared between the threads. The implementation of a critical section requires atomic read / write behavior.
Such access mechanisms like atomic read/write and critical sections are also crucial in heterogeneous computing, where concurrent memory access of multiple processors can occur.
Frameworks such as OpenMP can help the developer handle multi-threading and safe concurrency with abstractions of those concepts. OpenMP, a shared/memory parallel programming framework for C/C++ and Fortran framework, helps annotate parallel regions like parallelizable loops and creates multiple execution threads for those regions using a run-time.
The thesis implementation uses OpenMP for efficiency and parallelism purposes.

\subsection{Graphics Processing Unit (GPU)}
The GPU handles graphics processing in computer systems. In the beginning, video games were the main driver for technical advancements in developing better GPUs. However, today scientific applications such as machine learning are added to the basic use cases of GPUs and drive the development of the GPU in other ways. Modern GPUs allow programming the GPU as a general-purpose processing unit \cite{nickollsGPUComputingEra2010}, and the design of modern GPUs is significantly different from the first GPUs which had hard-coded pipelines. For computing graphics, the processing of multiple calculations in parallel is essential. Image and video processing primarily consists of doing the same operation on multiple pixels in parallel. Modern recordings and screens can have up to 8 million or more pixels transformed in real-time (i.e., 60 times per second). Such high throughput rates are only possible with a domain-specific processor such as the GPU that can execute thousands of threads concurrently.

Through the general-purpose programming capabilities of modern GPUs, other problems that require processing large data in parallel can be processed using GPUs. Those capabilities enable scientists and high-performance programmers to boost their parallelizable problems on the GPU \cite{cohenSolvingComputationalProblems2009,nickollsScalableParallelProgramming2008,stoneAcceleratingMolecularModeling2007}.
Another benefit of GPUs in comparison to CPUs is their energy efficiency \cite{qasaimehComparingEnergyEfficiency2019}. GPUs are more energy-efficient and time-efficient when executing highly parallel workloads than CPUs and are commonly incorporated in high-performance heterogeneous systems such as supercomputers.
Looking into Nvidia GPU architectures, such as Nvidia Volta \cite{NVIDIATESLAV1002017}, the GPU consists of many Streaming Multiprocessors (SM) that execute multiple thread blocks in parallel. A warp executes a thread block on Nvidia GPUs on the hardware side in the Streaming Multiprocessor. Such warp incorporates many simple cores that are executed in SIMD groups with a single program counter. A warp on Volta or Pascal Nvidia GPU architecture consists of 32 simple cores which share a common memory region.

Programmers of GPUs must be aware of those architecture semantics to use the underlying cache, memory, and computation structure optimally. A thread block should, for example, be sized according to the multiple of the size of one warp, even if programmers can decide on other thread block sizes. Volta has 64 Warps per SM, which means a maximum of 2048 threads scheduled concurrently on that SM \cite{NVIDIATESLAV1002017}.
Frameworks such as OpenCL or CUDA are application programming interfaces (APIs) created to allow software developers to develop general-purpose applications running on the GPU. CUDA is a super-set language of C/C++ that allows the definition of functions that are run on the GPU and CPU \cite{EasyIntroductionCUDA2012,nickollsScalableParallelProgramming2008,sosuthaHeterogeneousParallelComputing2015}. Annotating functions using \texttt{\detokenize{__device__}}, \texttt{\detokenize{__global__}} and/or \texttt{\detokenize{__host__}} tells the compiler on which processing unit the code can be run. Shared CPU-GPU code is possible, which is well suited for executing code on a heterogeneous system. Other advantages, such as unified memory abstractions for heterogeneous systems, are a deciding factor for using this technology in this thesis.

\begin{lstlisting}[language=C++, caption=Simple CUDA kernel example \cite{EasyIntroductionCUDA2012}, label=lst:kernel]
__global__
void add(int n, float *x, float *y)
{
  int index = threadIdx.x;
  int stride = blockDim.x;
  for (int i = index; i < n; i += stride)
      y[i] = x[i] + y[i];
}
\end{lstlisting}
In listing \ref{lst:kernel} a simple "kernel" function that adds an array to another array in parallel on the GPU is shown. Through the \texttt{\detokenize{__global__}} keyword, the compiler creates machine code suitable for an Nvidia GPU. By invoking this kernel, multiple instances of this function are scheduled in thread blocks on the GPU. The thread id of each invocation is accessed through the variable \emph{threadIdx}, and the number of threads in the block is accessed by \emph{blockDim}. When starting multiple blocks, the block id of the kernel invocation can be accessed by \emph{blockIdx}.
Data passed into the function like the arrays x and y have to is created by CUDA run-time API functions, which handle memory allocation on the GPU \cite{MemoryManagement}.

Nvidia GPUs used as accelerator devices have two different memory spaces. Data structures are created and allocated on the host or CPU memory space. After allocating memory on the GPU, the data must be moved to the device from the CPU either by a direct transfer or by relying on a Unified Virtual Memory system accessible from both CPU and GPU. The CUDA Runtime API enables the programmer to do both. In most cases, a single data transfer is done by allocating with \emph{cudaMalloc} and copying the data to the GPU with \emph{cudaMemcpy}. In the heterogeneous computing context, where data needs to be accessed by both the CPU and the GPU simultaneously, a Unified Virtual Memory (UVM) system helps the programmer with the data management. Since CUDA version 6.0, Nvidia introduced \emph{cudaMallocManaged} which handles necessary data transfer in both directions based on pagination \cite{gayatriComparingManagedMemory}. Both the host and device keep track of the pages using a page table. On memory access, missing pages migrate to the correct location.
For performance reasons, the CUDA run-time API allows for prefetching data to a device so that page misses, which can slow down the computation performance a lot, are prevented.

% - Hardware characteristics
% - CUDA Basics
% - Nvidia GPU Basics (Warp, Block, Streaming Multiprocessor)
% - Memory management
% - Unified Memory
% - 

\section{Constraint-Based Causal Structure Learning}
This section introduces necessary terminology in Causal Structure Learning, which is the problem class I look into in this thesis. The Causal Graphical Model and the PC-Algorithm are elaborated based on the definition given by Schmidt et al. \cite{schmidtOrderIndependentConstraintBasedCausal2018}. The order-independent GPU-based PC-Algorithm is the base for the heterogeneous approach of this thesis.

\subsection{Causal Graphical Model}
Given a finite set of $N$ vertices $V = (V_1,...,V_N)$ and a set of edges $E \subseteq V \times V$, let $G = (V,E)$. The vertices each represent a variable. An edge $(V_i, V_j) \in E$ is called directed, i.e., $V_i \rightarrow V_j$, if $(V_i,V_j) \in E$ but $(V_j, V_i) \notin E$. An edge is called undirected, if both $(V_i,V_j) \in E$ and $(V_j, V_i) \in E$, i.e. $V_i - V_j$. Two vertices $V_i$, $V_j$ are called adjacent if there is an undirected edge $V_i - V_j$. The adjacency set $adj(G, V_i)$ of the vertex $V_i \in V$ in $G$ are all vertices $V_j \in V$ that are connected to $V_i$ by an undirected edge.

Some graph $G$ where all edges are directed, and $G$ does not contain any cycle is a Directed Acyclic Graph (DAG). In the context of Causal Structure Learning, a directed edge $E$ in such a DAG represents a direct causal relationship between the connected vertices.
The d-separation criterion used in the DAG enables information of conditional independence between variables. Two variables $V_i, V_j \in V$ are conditionally independent given a set $S \subset V \backslash \{V_i, V_j\}$ if the vertices $V_i$ and $V_j$ are d-separated by the set $S$, which is called separation set.
A distribution $P$ of the variable set $V_1, ..., V_N$ that satisfies the above condition is called faithful.
Several different DAGs that describe the same conditional independence information form a Markov equivalence class. \cite{anderssonCharacterizationMarkovEquivalence1997}. If two DAGs are Markov equivalent, they share the same skeleton $C$ or the underlying undirected graph and the same v-structures.
These v-structures are triples $V_i,V_j,V_k$ with $(V_i,V_k) \notin E$ and $(V_k,V_i) \notin E$ and directed edges $V_i \rightarrow V_j$ and $V_k \rightarrow V_j$. Moreover, it is possible to uniquely describe the corresponding Markov equivalent class by a Complete Partially Directed Acyclic Graph (CPDAG) \cite{chickeringOptimalStructureIdentification2003}.
A CPDAG is a partially directed acyclic graph where all DAGs in the Markov equivalence class incorporate the same directed edges, and there exist two DAGs that incorporate the two directed versions of every undirected edge $V_i - V_j$ in the Markov equivalence class.
Hence, the focus lies on estimating the equivalence class of the DAG G based on the corresponding probability distribution $P$ of the involved variables $V_1,...,V_N$. In particular, under the assumption that the distribution $P$ is generated from the true causal DAG $G$, there is an edge $V_i - V_j$ in the skeleton of $G$ if and only if $V_i,V_j$ are dependent given all $S \subseteq V\backslash \{V_i,V_j\}$ \cite{spirtesCausationPredictionSearch1993}.
Hence, the examination of the conditional independence information of the observed variables $V_1,...,V_N$ allows for estimating the undirected skeleton $C$ of the corresponding DAG $G$. The extension of the skeleton $C$ to the equivalence class of the DAG $G$ can be done by the repeated application of deterministic edge orientation rules on the skeleton \cite{colomboOrderIndependentConstraintBasedCausal,kalischEstimatingHighDimensionalDirected2007,pearlIntroductionCausalInference2010}.

\subsection{Constraint-Based Methods}
In Causal Structure Learning, two main approaches have been proposed: the constraint-based method and the score-based method. While in the constraint-based method, CI-Tests are used to recover the CPDAG, score-based algorithms search for a high score rank to find the best fitting CPDAG. Both approaches have their advantages and disadvantages. Choosing the best approach is mostly dependent on the type of data. Constraint-based methods, for example, are more accurate on datasets with a small sample size \cite{scutariBayesianNetworkConstraintBased2017}. Example constraint-based approaches are PC \cite{spirtesCausationPredictionSearch1993}, Rank PC \cite{harrisPCAlgorithmNonparanormal}, PC-stable \cite{colomboOrderIndependentConstraintBasedCausal}, IC \cite{vermaEquivalenceSynthesisCausal1990}, and FCI \cite{spirtesCausationPredictionSearch1993}.
The constraint-based PC-stable algorithm by Colombo et al. \cite{colomboOrderIndependentConstraintBasedCausal} lays the basis for this thesis work because of the order independence in between different CI-tests and the possibility to parallelize the algorithm easily. For the same reason, different parallelized GPU and CPU implementations of the PC-stable algorithm exist \cite{schmidtLoadBalancedParallelConstraintBased2019,schmidtOrderIndependentConstraintBasedCausal2018,zarebavaniCuPCCUDAbasedParallel2018}, which also influenced the approach described in this thesis.
The PC algorithm avoids doing conditional independence checks for each pair of vertices $V_i, V_j \in V$ given all possible subsets $S \subseteq V \backslash \{V_i, V_j\}$ \cite{pearlTheoryInferredCausation1995} since this would become infeasible for a large number of vertices. The PC algorithm independence is tested incrementally in multiple levels by increasing the separation set size in each level. Edges where the given independence test successfully breaks the threshold are deleted. Therefore fewer edges have to be considered in higher levels, and the test count is reduced. Reducing the test count with this method applied to sparse matrices can decrease the complexity from exponential to polynomial with respect to the number of vertices \cite{kalischEstimatingHighDimensionalDirected2007}.
The PC-stable variant of the PC algorithm removes the dependency on the order of the variable set $V_1, ...,V_N$ \cite{colomboOrderIndependentConstraintBasedCausal,schmidtOrderIndependentConstraintBasedCausal2018}.

\subsection{PC Algorithm}
The PC Algorithm (Spirtes et al. \cite{spirtesCausationPredictionSearch1993}) was designed for learning DAGs. It assumes causal sufficiency, which means there are no unmeasured common causes. The PC Algorithm is mostly used in high-dimensional data settings and aims to be performant for sparse graphs that contain thousands of variables. \cite{kalischUnderstandingHumanFunctioning2010}
The version of the PC Algorithm described in Colombo et al. \cite{colomboOrderIndependentConstraintBasedCausal} is also called PC-stable because of its order independence regarding the different calculations. Order independence of the computations helps to parallelize an algorithm by not relying on synchronization. The order-independent PC-stable algorithm is the algorithm chosen for later parallelizing efforts in this thesis. The PC-stable algorithm is the algorithm the thesis is based on; due to that, the term PC algorithm is used as a synonym for the PC-stable variant in the following.

% TODO CI test dependency => change to algorithm of colombo
\begin{algorithm}
    \caption{Adjacency search of PC-stable algorithm \cite{colomboOrderIndependentConstraintBasedCausal}}
    \label{alg:pcstable}
    \begin{algorithmic}[1]
    \Require Vertex set $V$, Additional data necessary for the CI-Test
    \Ensure Estimated skeleton $C$, separation sets \textbf{Sepset}
    \State with fully connected skeleton $C$ and $l = -1$
    \Repeat 
        \State $l=l+1$
        \For{all Variables $V_i$ in $C$}
            \State Let $a(V_i) = adj(C,V_i)$
        \EndFor
        \Repeat
            \State Select pairs $(V_i,V_j)$ adjacent in $C$ with $|a(V_i)\backslash\{V_j\}| \geq l$
            \Repeat
                \State Choose separation set $S \subseteq a(V_i ) \ {V_j }$ with $| S | = l$
                \If{$p(V_i,V_j|S) \leq \alpha$}
                    \State Delete edge $V_i - V_j$ from $C$
                    \State Set Sepset(i,j) = Sepset(j,i) = S
                \EndIf
            \Until{edge $V_i - V_j$ is deleted in $C$ or all $S \subseteq a(V_i) \backslash \{V_j\}$ with $|S| = l$ have been chosen}
        \Until{all adjacent vertices $V_i$, and $V_j$ in $C$ such that $|a(V_i)\backslash\{V_j\}| \geq l$ have been considered}
    \Until{each adjacent pair $V_i$, $V_j$ in $C$ satisfy $|a(V_i)\backslash\{V_j\}| \geq l$}
    \State \textbf{return} C, Sepset
    \end{algorithmic}
\end{algorithm}

As can be seen in \ref{alg:pcstable} the starting point of the algorithm is a set of vertices that represent the given variables. The PC algorithm incrementally learns a skeleton at each level. Starting with a separation set of size 0 $S = \emptyset$ and a complete undirected skeleton $C$ in level 0, independence tests for every edge pair are done. Every edge between any vertice combination exist in level 0, therefore the independence test count can be calculated as $|P_0| = \frac{(|V| - 1) * |V|}{2}$. In subsequent level iterations, separation sets are non-empty, and the existence of an edge cannot be assumed. The test result is compared to a tuning parameter $\alpha$ and an edge $V_i - V_j$ is queued up for deletion in C if $p(V_i,V_j|S) \leq \alpha$. The real deletion is done in between levels, else deleted edges had to be considered by other CI tests. For each level $l$ the separation set $S$ with $|S| = l$ is built from the adjacent vertices $S \subset adj(C, V_i)\backslash V_j$. After testing every edge with its possible separation sets $S$ for that level, the queued edges are deleted, and the next level $l=l+1$ starts. If there is no separation set $S$ with $|S| = l$ for any vertex combination, the maximum level is reached, and the skeleton $C$ is estimated.
This skeleton is then used to apply deterministic orientation rules so that the corresponding CPDAG is found. \cite{colomboOrderIndependentConstraintBasedCausal,kalischEstimatingHighDimensionalDirected2007,pearlIntroductionCausalInference2010,schmidtOrderIndependentConstraintBasedCausal2018}
% - Explain PC Algorithm
% - Pseudocode listing
% - Loops