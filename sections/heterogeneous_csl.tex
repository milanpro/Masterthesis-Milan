%!TEX root = ../thesis.tex
\chapter{Heterogeneous Constraint-Based Causal Structure Learning}
Parallelizing the PC algorithm on heterogeneous systems starts with choosing a task size that is flexible enough for the different processing units executing the workload. As a basis, the implementation of the PC algorithm from the paper \cite{schmidt_order-independent_2018} is used since efforts for parallelizing the PC algorithm have already been done to utilize the throughput of the GPU. After that, the chosen task must be balanced on the CPU and the GPU using a balancing algorithm. The balancing algorithm has to be adaptable to the environmental parameters of the system it runs on.

\section{Parallelize the PC Algorithm}
Parallelizing workloads is the basis of parallel programming. A well-known design methodology used to architect a workload such that it can be run in parallel efficiently is Foster's Methodology \cite{foster_designing_1995}. Foster's Methodology is also often used for distributed-memory systems. Since GPU-CPU heterogeneous systems are distributed-memory systems, I will apply this method to parallelize the workload. Foster's Methodology uses four distinct stages, which are meant to be applied to the workload one after another:

\begin{enumerate}
    \item Partitioning
    \item Communication
    \item Agglomeration 
    \item Mapping
\end{enumerate}

The workload is decomposed into the smallest possible tasks in the Partitioning stage. For a correct execution, the Communication stage defines the necessary coordination of task execution. Both the first and the second stage are essential for the search of concurrency, scalability, and algorithm correctness. In the Agglomeration stage, multiple tasks are agglomerated, better to align the computation and data to the underlying system. The Mapping stage can be seen as analog to scheduling and maps the agglomerated tasks onto the different processing units.

\subsection{Partitioning and Communication}
In the Partitioning stage, opportunities for parallel execution are exposed. A good partition keeps computational parts and their respective data together to minimize the later communication efforts. Using fine-grained decomposition of the workload can be done in two different ways. Domain decomposition defines small data fragments first and specifies the computation later. Functional decomposition splits the computation into disjoint tasks and handles the data after. Domain decomposition is more appropriate than functional decomposition if there is a significant data overlap.
More generalized functional decomposition can also be seen as splitting a multivariate function $f(x_1,x_2,...,x_n)$ into multiple functions ${g_1,g_2,...g_m}$ such that $f(x_1,x_2,...,x_n) = \theta(g_1(x_1,x_2,...,x_n),g_2(x_1,x_2,...,x_n),...g_m(x_1,x_2,...,x_n))$ where $\theta$ is another function combining the results. Every function $g_n$ gets the same input as $f$. If the functional approach is applied to some algorithmic problem, loops with independent iterations can be mapped to this mathematical model and make parallelizing single iterations possible. If iterations of such loops mapped to a task do have side effect communication or synchronization must be done.
The Communication stage specifies links between data consumers and producers. Domain decomposition problems can have more complex communication infrastructures due to data dependencies. Distributing computation and communication is more efficient than centralizing the algorithm through some central manager, which communicates with every task. Independent and balanced tasks communication-wise are more capable of performant execution.

Using the PC-stable GPU-accelerated Algorithm proposed in the paper by Schmidt et al. \cite{schmidt_order-independent_2018} makes handling the Partitioning and Communication step straightforward. The algorithm is based on the gaussian distribution model and handles only the CI-Test for normal distributed data. For simplicity purposes the thesis implementation incorporates the same approach and looks into other CI-Tests in the later discussion.

\begin{algorithm}
    \caption{Adjacency search of PC-stable algorithm with gaussian distribution model \cite{schmidt_order-independent_2018, colombo_order-independent_nodate}}
    \label{alg:pcstable_gaussian}
    \begin{algorithmic}[1]
    \Require Vertex set $V$, correlation matrix $Cor$
    \Ensure Estimated skeleton $C$, separation sets \textbf{Sepset}
    \State with fully connected skeleton $C$ and $l = -1$
    \Repeat \label{alg:pcstable_gaussian:level_loop}
        \State $l=l+1$
        \For{all Variables $V_i$ in $C$}
            \State Let $a(V_i) = adj(C,V_i)$
        \EndFor
        \Repeat \label{alg:pcstable_gaussian:edge_loop}
            \State Select pairs $(V_i,V_j)$ adjacent in $C$ with $|a(V_i)\backslash\{V_j\}| \geq l$
            \Repeat \label{alg:pcstable_gaussian:sepset_loop}
                \State Choose separation set $S \subseteq a(V_i ) \backslash \{V_j\}|$ with $| S | = l$
                \If{$p(V_i,V_j|S) \leq \alpha$}
                    \State Delete edge $V_i - V_j$ from $C$
                    \State Set Sepset(i,j) = Sepset(j,i) = S
                \EndIf
            \Until{edge $V_i - V_j$ is deleted in $C$ or all $S \subseteq a(V_i) \backslash \{V_j\}$ with $|S| = l$ have been chosen}
        \Until{all adjacent vertices $V_i$, and $V_j$ in $C$ such that $|a(V_i)\backslash\{V_j\}| \geq l$ have been considered}
    \Until{each adjacent pair $V_i$, $V_j$ in $C$ satisfy $|a(V_i)\backslash\{V_j\}| \geq l$}
    \State \textbf{return} C, Sepset
    \end{algorithmic}
\end{algorithm}

As can be seen in algorithm \ref{alg:pcstable_gaussian} the main input next to the vertices is the correlation matrix for those vertices. The correlation matrix is necessary for the gaussian-based CI-Test. The algorithm \ref{alg:pcstable} is a CI-Test generalized version of the algorithm \ref{alg:pcstable_gaussian}.
As already stated, loops can be split into tasks if their iteration execution flow is independent from the input of other iterations. This statement does not apply to the repetition in line \ref{alg:pcstable_gaussian:level_loop} because every level depends on the results of its predecessor. Synchronization must occur between each level.
Only two loops are independent iteration-wise. The proposed algorithm can be split between different edge as in line \ref{alg:pcstable_gaussian:edge_loop} or between different separation sets as in line \ref{alg:pcstable_gaussian:sepset_loop}. The smaller and, therefore, the correct task for the Partitioning phase is a separation set split.

In the proposed GPU-accelerated algorithm variant of Schmidt et al. \cite{schmidt_order-independent_2018} level 0 is split into edges, because there is only one separation set $S \subseteq a(V_i ) \backslash \{V_j\}$ with $| S | = l$ which is the empty set ${}$. Therefore only one CI-Test has to be done per edge.
Level $1,2 ... m$ all split into per separation set tasks in the Partitioning stage. Performance improvements can be done by having a early stop criterion such as a deleted edge, but this requires additional communication between tasks. Data that is shared can not be properly aligned to the tasks. Task data access is nearly randomized on the input correlation in level 1 and following because the separation sets can be build from all existing vertices. 

%- Partitioning und Communication genauer erklären
%- GPU paper basis
%    - partitioning schon getan
%    - Task ist eine edge
%    - Wiederverwenden
%    - Durch parallelisierung auf processing units weiterhin diese partitionierung
    
\subsection{Agglomeration}
The Agglomeration stage uses the correct algorithm produced by the Partitioning and Communication stages and specializes that for the execution environment. Tasks should be agglomerated to be executed efficiently. In the case of this thesis the execution environment is a heterogeneous system containing CPUs and GPUs as processing units. The resulting number of tasks of the Agglomeration stage can be greater than the number of processing units. To guide through the Agglomeration stage, conflicting decisions have to be made:

\begin{enumerate}
    \item Reduce communication cost through agglomerated computation
    \item Preserve flexibility with respect to later Mapping stage and maintain parallelism
\end{enumerate}

With regards to the GPU implementation of the paper by Schmidt et al. \cite{schmidt_order-independent_2018} which splits the algorithm into separation set tasks and parallelizes those on the GPU, agglomerating those tasks into larger task sets can be done by 
taking the next bigger loop into consideration. The per edge iteration in line \ref{alg:pcstable_gaussian:edge_loop} is suitable for 
% - Agglomeration genauer erklären
% - Durch adjazenz matrix basis bietet sich row basiert an
% - row bundelt mehrere edges und ist damit etwas geeigneter als Task
% - einzelne edge können weiterhin parallel bearbeitet werden auf den PUs
% - genauer die ausführung der GPU anschauen
%  - GPU als basis -> wie geschwindigkeit der GPU erhöhen
%  - serieller/paralleler anteil
%  - GPU Block besteht meist aus 64 Threads
%  - (Beispiel code für ausführung)
%  - Iterationsanzahl bestimmt längste thread ausführung
%  - iterationsanzahl ist abhängig von row length
%  - Testanzahl berechnen aus und daraus iteration (wie)
%  - Diagram Iterations länge
%  - erklären wie dadurch der serielle anteil gekürzt werden kann
 
\subsubsection{Compact Adjacency Lists}
% - durch compact step werden adjacency lists zwischen leveln berechnet
% - länge der liste ist anzahl an edges in einer row
% - letztes element genutzt als längenspeicher
% - grafik zur verbesserten darstellung
% - wird auch in \cite{zarebavani_cupc_2018} verwendet

\subsection{Mapping}
% - Mapping genauer erklären
% - Scheduling spezifizieren
%     - Static/Dynamic scheduling
%     - static = prebalanced
%         vor nachteile
%     - dynamic 
%         vor nachteile
% - Two approaches pre balanced and workstealing

\section{Heterogeneous PC Algorithm Approaches}
\subsection{Pre-Balanced Approach}
% - Static scheduling ansatz
% - Load balancer schiebt längste rows auf CPU
% ...
% - Viel Memory/ communication -> nvprof?
% - Limitieren durch Migrierungsschritt der CPU

\subsection{Workstealing Approach}
% - Dynamic scheduling
% - Workstealing erklären (herkunft etc)
% - GPU läuft normal durch
% - CPU nimmt sich längste rows als erstes vor und arbeitet von hinten ab
% - regelmäßiges überprüfen ob schon fertig
% - vermehrte kommunikation