%!TEX root = ../thesis.tex
\chapter{Introduction}
% Determining the causal relationships in systems helps human to understand underlying mechanisms. Commonly experiments are designed and executed to derive causal relationships. In complex systems with thousands of variables, experiments may become impracticle due to cost or ethical reasons (medicine â€¦).
% [Furthermore, human cannot grasp] 
% In these settings, algorithms support to derive observational data
Determining the causal relationships in systems helps scientists to understand the underlying mechanisms. Commonly randomized controlled experiments are explicitly designed to derive causal relationships, but in complex systems with thousands of variables, experiments may become impractical due to costs, effort, or ethical reasons. In these settings, scientists rely on algorithms to derive causal relationships from observational data \cite{pearlCausality2009,spirtesCausationPredictionSearch1993,neapolitanLearningBayesianNetworks2003,pearlProbabilisticReasoningIntelligent1988}. 
These causal structure learning algorithms take observational data as input and output the causal relationships between variables in the form of a graphical model.
% Causality in a complex system is often hard to grasp for humans, where thousands of variables influence each other, and the cause for an observation is not trivially found. Due to the systems' complexity, a more rational way of deriving causality information is using automated routines such as algorithms to gather causality information.  Those algorithms can be executed by computational systems that are way faster at processing observational data than a human could be.
% Causal structure learning is a field in computer science that deals with learning causal relationships between variables to infer a causal structure in the form of a graphical model. The causal structure enables predicting events that share a common cause and might lead to the prevention or acceleration of such events.
%In data mining and statistics deriving knowledge from observational data is used to find correlations between variables in the data. However, while correlation does imply a relationship between variables, it does not inform us about the creation of that relationship. This information can only be gathered through causation. Nevertheless, correlation does not imply causation, which means that information about causality must be gathered to understand the datasets' relationships thoroughly. Based on correlational relationship information, causation information can be inferred by applying algorithms that search for causal structure. The search for causal structure is also called causal structure learning \cite{spirtesCausationPredictionSearch1993}.

There are many fields where causal relationships are of particular interest. Causal structure learning is, for example, used to derive gene regulatory networks. The inference of gene regulatory networks in medicine or, more specifically, genetic research can help to estimate causal effects among genes \cite{rauJointEstimationCausal2013}.
In these fields, datasets are often large-scale and high-dimensional resulting in long execution times, which hinders the application of causal structure learning in real-world settings.
% motivation, Relevanz (Anwendungsfall, Daten etc warum Effizienz?)

Moore's Law \cite{mooreCrammingMoreComponents2006} says that transistors placed on an integrated circuit double approximately every two years. This law has been self-fulfilling, and this effect can be seen in the real world, indicating a steady processing performance gain. Still, modern processor development is getting to a physical limit, called the "Power Wall". More transistors that are more densely placed, with lower voltage and higher frequencies, produce more heat. While cooling performance has its physical limits, the "Power Wall" is hit at some point.

To work against this limit, modern processors incorporate multiple cores that work in parallel. Studies show that introducing additional cores on the same chip is also limited physically because of the growing density combined with heat and voltage limitations. This limitation is also called "dark silicon" because parts of the chip are disabled to counteract the overheating problem \cite{esmaeilzadehDarkSiliconEnd2011}.

Multi-core processors can speed up execution by splitting work and processing that work in parallel. However, parallelization has its limits and converges to an upper bound for any work and can be calculated by applying Amdahl's law\cite{amdahlValiditySingleProcessor1967}. Even for highly parallel algorithms that are designed for multi-core systems, the upper bound still exists.

One of the next logical steps for improving performance and optimizing execution speed despite the physical and algorithmic limitations is adding specialized processors to the system tailored for the given use case. Heterogeneous systems emerged. The processor best fitting for the work to be done is used to achieve optimal performance. Nevertheless, while one processor handles the work, the remaining processors may not be used, and additional computing power is wasted.

Heterogeneous computing tries to minimize the wasted computational power in heterogeneous systems by using as many resources as possible. Algorithms and applications are inspected, split into parts given their properties, such as computation or data dependency, and those parts are then placed onto the different processors. The execution time is reduced by using multiple specialized processors for the best fitting part of the executed work.

A challenge in using multiple specialized processors is that the relative execution time of work done on heterogeneous processors is more complicated to estimate than on homogeneous processors. The estimation of execution time is an essential parameter for scheduling different parts of the work on each processor. On homogeneous systems, work is placed on any processor because deciding the execution unit does not influence the execution time. In contrast to that, heterogeneous systems use a scheduling algorithm that estimates resource utilization and handles placing the work on each processor. Such a scheduling algorithm is also called a load-balancer. Load-balancing is one of the primary focuses of this thesis' algorithm design.

Another challenge of heterogeneous systems is the importance of minimizing communication between the parts executed on different processors. The communication time spent is directly related to the interconnection speed between those processors. Modern heterogeneous system interconnects between processors are getting faster. Still, the interconnect between processors can be one of the slowest connections in a computer. For most heterogeneous computing solutions, the interconnect becomes a bottleneck. This bottleneck is an essential restriction to heterogeneous computing so that many solutions for heterogeneous systems are even slower while using all of the resources because of the communication overhead. Due to the importance of efficient communication designing heterogeneous algorithms essentially means solving the communication problem.

In this thesis, both the PC algorithm and heterogeneous computing are combined to an efficient PC algorithm variant that can be executed on a heterogeneous system to use its resources and get an additional speedup compared to the non-heterogeneous variant. Thereby we tackle the long execution times of many real-world PC algorithm use-cases. The introduced variant of the PC algorithm incorporates load-balancing solutions to handle optimal resource utilization and optimizations regarding possible communication overhead. Experiments are done using the heterogeneously executed PC algorithm to analyze bottlenecks and the efficiency of the solution.

This thesis is structured starting with the theoretical background in Chapter \ref{chap:background}, followed by the problem statement (cf. Chapter \ref{chap:problem_statement}). A solution for the problem statement is proposed in Chapter \ref{chap:hetcsl}, and this solution is evaluated through experiments in Chapter \ref{chap:experiments}. Related work associated with the found solution is elaborated in Chapter \ref{chap:relwork}, which then leads to discussing the experiments done in the context of the related work in Chapter \ref{chap:discussion}. In the end, future work that could follow this thesis is composed (cf. Chapter \ref{chap:fuwork}). In the following, a more in-depth outline of this structure is given.

Basic terminology is introduced with the theoretical background. While parallel programming is essential for parallelizing algorithms on heterogeneous systems, this topic is covered first. The basics of heterogeneous computing and constraint-based causal structure learning are elaborated in the second part of the background chapter.

After that, in the problem statement chapter, the research question is developed using the acquired terminology and explaining the problem state which is aimed to be solved. In this chapter, the basis for the proposed heterogeneous parallelization of the PC algorithm is laid. Following the problem statement, the heterogeneous PC algorithm is developed in the heterogeneous causal structure learning chapter using Fosters' Methodology \cite{fosterDesigningBuildingParallel1995}. Fosters' Methodology is one of the standard methods to parallelize algorithms, even in heterogeneous system contexts. Distributing multiple parts of the work to be executed on multiple processors is called scheduling; doing that with resource utilization in mind is also called load-balancing. By inspecting an optimized load-balancing for the parallelized PC algorithm, two approaches are worked out and explained.

In the experiments chapter, those two approaches are tested using benchmarks with different parameters on multiple systems. Then the work on the heterogeneous PC algorithm is connected to related work. A discussion on the viability of both approaches based on the experimental results is started in the discussion chapter. The problem statement is linked to the experiments' results, and the research question is answered.

At the end of the thesis, future work is elaborated that could help solidify the results or produce even better results.

% Trend hardware? heterogene (vllt dark silicon)
% moores law
% processing speed i limited an does not grow any longer (physics, get too hot when more dense) => dynamic power, power wall
% ILP wall? maybe too detailed
% memory wall (memory speed, and latency limited)
% Dark silicon = next step / power wall 2.0
% - multi-core scaling limited as well
% 
% - specialized hardware has potential
% - speedup through using the right processor for the correct task
% - Needs alot more software optimizations
% speedup for what?
% - medical causations => The faster, the better
% - when doing multiple experiments/iterations on data, small speedup can mean alot
% - energy efficiency, by optimal resource usage
% - some processing units are more efficient

% Outline, structure

% Teaser auf problem statement

% Womit muss man sich auseinandersetzen (load balanacing)