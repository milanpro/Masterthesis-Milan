%!TEX root = ../thesis.tex
\chapter{Introduction}
In data mining and statistics deriving knowledge from observational data is used to find correlations between variables in the data. Through those correlations, data distribution characteristics are found. Correlation does not imply causation, which means that causality information must be gathered to understand the data and found correlation thoroughly. 

There are many fields where causal relationships of particular interest. The inference of gene regulatory networks in medicine or, more specifically, genetic research can help to estimate causal effects among genes. \cite{rauJointEstimationCausal2013}
% motivation, Relevanz (Anwendungsfall, Daten etc warum Effizienz?)

Moore's Law \cite{mooreCrammingMoreComponents1965} says that transistors placed on an integrated circuit double approximately every two years. This law has been self-fulfilling and can be seen in the real world. Still, modern processor development is getting to a physical limit, called the "Power Wall". More transistors that are more densely placed, with lower voltage and higher frequencies, produce more heat. While cooling performance has its physical limits, the "Power Wall" is hit at some point. To work against this limit, modern processors incorporate multiple cores that work in parallel.

Multi-core processors can speed up execution by splitting work and processing that work in parallel. However, parallelization has its limits and converges to an upper bound for any work \cite{amdahlValiditySingleProcessor1967}. While this is specific for parallelized work, and there are algorithms and applications specifically designed to be executed on multiple cores, the upper bound still exists. One of the next logical steps for improving performance and optimizing execution speed is adding specialized processors tailored for the use case. With adding additional specialized processors, heterogeneous systems with heterogeneous processors emerged. The processor best fitting for the work to be done is used, and with that, the best possible performance is to be expected. Nevertheless, while one processor handles the work, the remaining processors may not be used, and additional computing power is lost.

Heterogeneous computing tries to solve the problem of lost computing power in heterogeneous systems by using as many resources as possible. Algorithms and applications are inspected, split into parts given their properties, such as parallelizability, and those parts are then placed onto the different processors. The execution time is reduced by using multiple specialized processors for the best fitting part of the executed work. A downside of using multiple specialized processors is that parallelized work almost always needs orchestration and communication between each executing unit. The communication time spent is directly related to the interconnection speed between those executing units.

Modern heterogeneous system interconnects between processors are getting faster. Still, the interconnect between processors is one of the slowest connections in a computer. Therefore most heterogeneous computing solutions are limited by their interconnect bottleneck. This bottleneck is an essential part of heterogeneous computing that many solutions for heterogeneous systems are even slower while using all of the resources because of the communication overhead. Due to the importance of efficient communication designing heterogeneous algorithms essentially means solving the communication problem.

In this thesis, both the PC algorithm and heterogeneous computing are combined to an efficient PC algorithm variant that can be executed on a heterogeneous system to use its resources and get an additional speedup compared to the non-heterogeneous variant. Experiments are done using the heterogeneously executed PC algorithm to analyze bottlenecks and the efficiency of the solution.

Since many use cases of causal structure learning profit from faster results and datasets can be large-scale, leading to long execution times, a speedup in execution time is beneficial.

Basic terminology is introduced with the theoretical background chapter. While parallel programming is essential for parallelizing algorithms on heterogeneous systems, this topic is covered first. The basics of heterogeneous computing and constraint-based causal structure learning are elaborated in the second part of the background chapter.

After that, in the problem statement chapter, the research question is developed using the acquired terminology and explaining the problem state which is aimed to be solved. In this chapter, the basis for the proposed heterogeneous parallelization of the PC algorithm is laid. Following the problem statement, the heterogeneous PC algorithm is developed in the heterogeneous causal structure learning chapter using Fosters' Methodology. Fosters' Methodology is one of the standard methods to parallelize algorithms, even in heterogeneous system contexts. Distributing multiple parts of the work to be executed on multiple processors is called scheduling; doing that with resource utilization in mind is also called load-balancing. By inspecting an optimized load-balancing for the parallelized PC algorithm, two approaches are worked out and explained.

In the Experiments chapter, those two approaches are tested using benchmarks with different parameters on multiple systems. Then the work on the heterogeneous PC algorithm is connected to related work. With the results of the experiments, a discussion on the viability of both approaches is started in the discussion chapter. The problem statement is linked to the experiments' results, and the research question is answered.

At the end of the thesis, future work is elaborated that could help solidify the results or produce even better results.

% Trend hardware? heterogene (vllt dark silicon)
% moores law
% processing speed i limited an does not grow any longer (physics, get too hot when more dense) => dynamic power, power wall
% ILP wall? maybe too detailed
% memory wall (memory speed, and latency limited)
% Dark silicon = next step / power wall 2.0
% - multicore scaling limited as well
% \cite{esmaeilzadehDarkSiliconEnd2011}
% - specialized hardware has potential
% - speedup through using the right processor for the correct task
% - Needs alot more software optimizations
% speedup for what?
% - medical causations => The faster, the better
% - when doing multiple experiments/iterations on data, small speedup can mean alot
% - energy efficiency, by optimal resource usage
% - some processing units are more efficient

% Outline, structure

% Teaser auf problem statement

% Womit muss man sich auseinandersetzen (load balanacing)