%!TEX root = ../thesis.tex
\chapter{Future Work}
\label{chap:fuwork}
The experiments in this thesis rely on the same dataset with differently sized variable samples. Although this helps to get a first good impression of the effectiveness of the proposed approaches, there is still a bias in the experiment results regarding the dataset structure and edge deletion behavior. A different edge deletion state of the graph will lead to a different efficiency of the heterogeneous PC algorithm. While a more significant difference in task execution time outliers and non-outliers could result in more effective outlier scheduling on the CPU, less to no outlier occurrence should make the GPU-only variant more effective.

This dataset behavior could be tested by executing the benchmarks with a different alpha value, directly influencing the amount with which edges are deleted. Another way of testing is generating datasets that show one of the said behaviors.

For better comparison, other real-world dataset experiments could be performed. Multiple gene expression datasets are often used in the context of benchmarking the PC algorithm, such as NCI-60, MCC, or BR51 \cite{leFastPCAlgorithm2019}. By executing benchmarks with said datasets, direct comparisons to state-of-the-art methods could be made to better understand the performance implications of both approaches. Additionally, insights about the dependency of the approaches on the dataset and the effectiveness in other real-world cases can be derived.

The approaches shown in this thesis are not dependent on some specific CI-test. For simplicity, every benchmark was executed using a CI-test that fits the multivariate normal distribution. For other distributions such as categorical data, other tests are used \cite{scutariLearningBayesianNetworks2010}. Those other tests require other data structures and can change the data access patterns significantly. With other data access patterns, the data dependencies between tasks change, and therefore, the interconnect usage between CPU and GPU has to be considered for the algorithm design. Testing other CI-test could help to get insights on how those tests fit the heterogeneous computing setting.

Real-world datasets are not always composed of variables distributed in the same way. Mixed distribution datasets, also called heterogeneous datasets, require more complex conditional independence tests. Different CI-tests are possibly better executed on different specialized processing units. A more complex load-balancing could be introduced in heterogeneous datasets that balance tasks based on the CI-test used in the task.

The two used heterogeneous systems called Delos and AC922, the experiments are based on, show that the underlying system/hardware specifications are essential for the execution of heterogeneous computing. Other systems could be added to the experiments to understand the influences of the underlying hardware. Those systems could feature different performance disparities between CPU and GPU to get further insights into the effects of the hardware characteristics besides to the interconnect. Adding multiple GPU benchmarks could also be interesting to test and profile execution times. Sometimes two different system GPUs are directly connected to two different system CPUs. A more intelligent scheduling algorithm could be beneficial in such a system, identifying which task to steal from which GPU for best performance.

Heterogeneous systems do not only consist of CPUs and GPUs. Many different kinds of processing units could also support the algorithm execution. One of the also widely known processing units is called Field Programmable Gate Array (FPGA). FPGAs can be programmed to mimic an application-specific integrated circuit. The developer is in charge of deciding which gate is connected to which and designs hardware via software. An FPGA can be more efficient in comparison to CPU and GPU when used tailored for a use-case, \cite{qasaimehComparingEnergyEfficiency2019} and could be added to the heterogeneous computing solution proposed in this thesis to accelerate further.
% Test with different dataset charateristics
% Test with Stronger CPU/GPU disparity
% Add FPGA
% 